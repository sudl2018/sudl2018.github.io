<!DOCTYPE HTML>
<html lang="zh-CN">
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!--Setting-->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta http-equiv="Cache-Control" content="no-siteapp">
    <meta http-equiv="Cache-Control" content="no-transform">
    <meta name="renderer" content="webkit|ie-comp|ie-stand">
    <meta name="apple-mobile-web-app-capable" content="sudoli&#39;s blog">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="format-detection" content="telephone=no,email=no,adress=no">
    <meta name="browsermode" content="application">
    <meta name="screen-orientation" content="portrait">
    <link rel="dns-prefetch" href="http://yoursite.com">
    <!--SEO-->





<meta name="robots" content="all" />
<meta name="google" content="all" />
<meta name="googlebot" content="all" />
<meta name="verify" content="all" />
    <!--Title-->


<title>使用正则化去优化深度学习模型 | sudoli&#39;s blog</title>


    <link rel="alternate" href="/atom.xml" title="sudoli&#39;s blog" type="application/atom+xml">


    <link rel="icon" href="/favicon.ico">

    



<link rel="stylesheet" href="/css/bootstrap.min.css?rev=3.3.7">
<link rel="stylesheet" href="/css/font-awesome.min.css?rev=4.5.0">
<link rel="stylesheet" href="/css/style.css?rev=@@hash">




    
	<div class="hide">
		<script type="text/javascript">
			var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan class='cnzz_stat_icon_1263868967 hide' %3E%3Cscript%20src%3D%22https%3A%2F%2Fs95.cnzz.com%2Fz_stat.php%3Fweb_id%3D1272564536%22%3E%3C%2Fscript%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s19.cnzz.com/z_stat.php%3Fid%3D1263868967%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
		</script>
	</div><!-- hexo-inject:begin --><!-- hexo-inject:end -->






    

</head>


<!--[if lte IE 8]>
<style>
    html{ font-size: 1em }
</style>
<![endif]-->
<!--[if lte IE 9]>
<div style="ie">你使用的浏览器版本过低，为了你更好的阅读体验，请更新浏览器的版本或者使用其他现代浏览器，比如Chrome、Firefox、Safari等。</div>
<![endif]-->

<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="main-header"  style="background-image:url(http://onq81n53u.bkt.clouddn.com/neweeess_%E5%89%AF%E6%9C%AC.png)"  >
    <div class="main-header-box">
        <a class="header-avatar" href="/" title='sudoli'>
            <img src="/img/avatar.jpg" alt="logo头像" class="img-responsive center-block">
        </a>
        <div class="branding">
        	<!--<h2 class="text-hide">Snippet主题,从未如此简单有趣</h2>-->
            
                <h2> best </h2>
            
    	</div>
    </div>
</header>
    <nav class="main-navigation">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="navbar-header"><span class="nav-toggle-button collapsed pull-right" data-toggle="collapse" data-target="#main-menu" id="mnav">
                    <span class="sr-only"></span>
                        <i class="fa fa-bars"></i>
                    </span>
                    <a class="navbar-brand" href="http://yoursite.com">sudoli&#39;s blog</a>
                </div>
                <div class="collapse navbar-collapse" id="main-menu">
                    <ul class="menu">
                        
                            <li role="presentation" class="text-center">
                                <a href="/"><i class="fa "></i>主页</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/机器学习/"><i class="fa "></i>机器学习</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/深度学习/"><i class="fa "></i>深度学习</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/SpringBoot/"><i class="fa "></i>SpringBoots</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/archives/"><i class="fa "></i>时间轴</a>
                            </li>
                        
                    </ul>
                </div>
            </div>
        </div>
    </div>
</nav>
    <section class="content-wrap">
        <div class="container">
            <div class="row">
                <main class="col-md-8 main-content m-post">
                    <p id="process"></p>
<article class="post">
    <div class="post-head">
        <h1 id="使用正则化去优化深度学习模型">
            
	            使用正则化去优化深度学习模型
            
        </h1>
        <div class="post-meta">
    
    
    <span class="categories-meta fa-wrap">
        <i class="fa fa-folder-open-o"></i>
        <a href="/categories/深度学习">
            深度学习
        </a>
    </span>
    
    
    <span class="fa-wrap">
        <i class="fa fa-tags"></i>
        <span class="tags-meta">
            
                
                    <a href="/tags/深度学习" title='深度学习'>
                        深度学习
                    </a>
                
                    <a href="/tags/TensorFlow" title='TensorFlow'>
                        TensorFlow
                    </a>
                
                    <a href="/tags/L2" title='L2'>
                        L2
                    </a>
                
            
        </span>
    </span>
    

    
        
        <span class="fa-wrap">
            <i class="fa fa-clock-o"></i>
            <span class="date-meta">2018/05/01</span>
        </span>
    
</div>

            
            
    </div>
    
    <div class="post-body post-content">
        <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<h1 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h1><ul>
<li><p>了解正则化对模型的影响</p>
</li>
<li><p>训练数据采用notMNIST</p>
</li>
</ul>
<ul>
<li>采用L2 正则化</li>
</ul>
<ul>
<li><p>采用Dropout随机失活</p>
</li>
<li><p>采用学习速率衰减</p>
</li>
<li><p>我在增加神经网络层时遇到了梯度消失问题，就是loss 值算出来是nan;我有一种实用但是不能彻底解决梯度消失问题的策略；</p>
<ul>
<li><p>首先RELU 可以帮助防止梯度消失</p>
</li>
<li><p>其次就是权重的初始化很重要，梯度消失是多个小于1的数相乘，所以如何避免呢，就是初始化时，假设前面一层有n个节点，那么我会这样初始化：</p>
</li>
<li><script type="math/tex; mode=display">
relu : stddev=np.sqrt(\frac{2.0}{n} ) \\
sigmod : stddev=np.sqrt(\frac{1.0}{n} ) \\
tanh:  stddev=np.sqrt(\frac{1.0}{n} ）</script></li>
</ul>
</li>
</ul>
<h1 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h1><ul>
<li><h3 id="tf-nn-dropout"><a href="#tf-nn-dropout" class="headerlink" title="tf.nn.dropout"></a>tf.nn.dropout</h3></li>
<li><p>按照keep_prob失活神经元；</p>
</li>
<li><p>这种操作类似于缩放，但是记住缩放后，<strong>元素的和是不变的</strong>，所以不需要再手动乘上倍数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.dropout(</span><br><span class="line">    x,</span><br><span class="line">    keep_prob,</span><br><span class="line">    noise_shape=<span class="keyword">None</span>,</span><br><span class="line">    seed=<span class="keyword">None</span>,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">Args:</span><br><span class="line">x: A floating point tensor.</span><br><span class="line">keep_prob: A scalar Tensor <span class="keyword">with</span> the same type <span class="keyword">as</span> x. The probability that each element <span class="keyword">is</span> kept.</span><br><span class="line">noise_shape: A <span class="number">1</span>-D Tensor of type int32, representing the shape <span class="keyword">for</span> randomly generated keep/drop flags.</span><br><span class="line">seed: A Python integer. Used to create random seeds. See tf.set_random_seed <span class="keyword">for</span> behavior.</span><br><span class="line">name: A name <span class="keyword">for</span> this operation (optional).</span><br><span class="line">Returns:</span><br><span class="line">A Tensor of the same shape of x.</span><br></pre></td></tr></table></figure>
</li>
<li><h3 id="tf-train-exponential-decay"><a href="#tf-train-exponential-decay" class="headerlink" title="tf.train.exponential_decay"></a>tf.train.exponential_decay</h3></li>
<li><p>这个就是专门用来对学习速率进行衰减的</p>
</li>
<li><p>一句话概括这个函数，每decay_steps（步）就以decay_rate（率）乘以learning_rate</p>
</li>
<li><p>计算公式如下</p>
</li>
<li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">decayed_learning_rate = learning_rate *</span><br><span class="line">                        decay_rate ^ (global_step / decay_steps)</span><br></pre></td></tr></table></figure>
</li>
<li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">tf.train.exponential_decay(</span><br><span class="line">    learning_rate,</span><br><span class="line">    global_step,</span><br><span class="line">    decay_steps,</span><br><span class="line">    decay_rate,</span><br><span class="line">    staircase=<span class="keyword">False</span>,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">Args:</span><br><span class="line">learning_rate: A scalar float32 <span class="keyword">or</span> float64 Tensor <span class="keyword">or</span> a Python number. The initial learning rate.</span><br><span class="line">global_step: A scalar int32 <span class="keyword">or</span> int64 Tensor <span class="keyword">or</span> a Python number. Global step to use <span class="keyword">for</span> the decay computation. Must <span class="keyword">not</span> be negative.</span><br><span class="line">decay_steps: A scalar int32 <span class="keyword">or</span> int64 Tensor <span class="keyword">or</span> a Python number. Must be positive. See the decay computation above.</span><br><span class="line">decay_rate: A scalar float32 <span class="keyword">or</span> float64 Tensor <span class="keyword">or</span> a Python number. The decay rate.</span><br><span class="line">staircase: Boolean. If <span class="keyword">True</span> decay the learning rate at discrete intervals</span><br><span class="line">name: String. Optional name of the operation. Defaults to <span class="string">'ExponentialDecay'</span>.</span><br><span class="line">Returns:</span><br><span class="line">A scalar Tensor of the same type <span class="keyword">as</span> learning_rate. The decayed learning rate.</span><br><span class="line"></span><br><span class="line">Raises:</span><br><span class="line">ValueError: <span class="keyword">if</span> global_step <span class="keyword">is</span> <span class="keyword">not</span> supplied.</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># These are all the modules we'll be using later. Make sure you can import them</span></span><br><span class="line"><span class="comment"># before proceeding further.</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> cPickle <span class="keyword">as</span> pickle</span><br></pre></td></tr></table></figure>
<pre><code>C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">pickle_file = <span class="string">'notMNIST.pickle'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(pickle_file, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">  save = pickle.load(f)</span><br><span class="line">  train_dataset = save[<span class="string">'train_dataset'</span>]</span><br><span class="line">  train_labels = save[<span class="string">'train_labels'</span>]</span><br><span class="line">  valid_dataset = save[<span class="string">'valid_dataset'</span>]</span><br><span class="line">  valid_labels = save[<span class="string">'valid_labels'</span>]</span><br><span class="line">  test_dataset = save[<span class="string">'test_dataset'</span>]</span><br><span class="line">  test_labels = save[<span class="string">'test_labels'</span>]</span><br><span class="line">  <span class="keyword">del</span> save  <span class="comment"># hint to help gc free up memory</span></span><br><span class="line">  print(<span class="string">'Training set'</span>, train_dataset.shape, train_labels.shape)</span><br><span class="line">  print(<span class="string">'Validation set'</span>, valid_dataset.shape, valid_labels.shape)</span><br><span class="line">  print(<span class="string">'Test set'</span>, test_dataset.shape, test_labels.shape)</span><br></pre></td></tr></table></figure>
<pre><code>Training set (20000, 28, 28) (20000,)
Validation set (1000, 28, 28) (1000,)
Test set (1000, 28, 28) (1000,)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把3维的训练数据转化为2维，即把28*28的图像像素矩阵转化成一维向量的转置 </span></span><br><span class="line"><span class="comment"># 把标签转化为one-hot 向量</span></span><br><span class="line">image_size = <span class="number">28</span></span><br><span class="line">num_labels = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reformat</span><span class="params">(dataset, labels)</span>:</span></span><br><span class="line">  dataset = dataset.reshape((<span class="number">-1</span>, image_size * image_size)).astype(np.float32)</span><br><span class="line">  <span class="comment"># Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]</span></span><br><span class="line">  labels = (np.arange(num_labels) == labels[:,<span class="keyword">None</span>]).astype(np.float32)</span><br><span class="line">  <span class="keyword">return</span> dataset, labels</span><br><span class="line">train_dataset, train_labels = reformat(train_dataset, train_labels)</span><br><span class="line">valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)</span><br><span class="line">test_dataset, test_labels = reformat(test_dataset, test_labels)</span><br><span class="line">print(<span class="string">'Training set'</span>, train_dataset.shape, train_labels.shape)</span><br><span class="line">print(<span class="string">'Validation set'</span>, valid_dataset.shape, valid_labels.shape)</span><br><span class="line">print(<span class="string">'Test set'</span>, test_dataset.shape, test_labels.shape)</span><br></pre></td></tr></table></figure>
<pre><code>Training set (20000, 784) (20000, 10)
Validation set (1000, 784) (1000, 10)
Test set (1000, 784) (1000, 10)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(predictions, labels)</span>:</span></span><br><span class="line">  <span class="comment"># argmax  axis = 0的时候返回每一列最大值的位置索引</span></span><br><span class="line">  <span class="comment"># axis = 1的时候返回每一行最大值的位置索引</span></span><br><span class="line">  <span class="comment"># axis = 2、3、4...，即为多维张量时，同理推断</span></span><br><span class="line">  <span class="comment"># 得到true，fasle 的向量，然后统计1的个数，就是正确率</span></span><br><span class="line">  <span class="keyword">return</span> (<span class="number">100.0</span> * np.sum(np.argmax(predictions, <span class="number">1</span>) == np.argmax(labels, <span class="number">1</span>))</span><br><span class="line">          / predictions.shape[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line">localtime = time.asctime( time.localtime(time.time()) )</span><br><span class="line">print(localtime)</span><br></pre></td></tr></table></figure>
<pre><code>Tue Sep  4 09:08:04 2018
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 任务1，利用nn.l2_loss(t)来训练模型，包含了L2正则化</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span>将全连接层转化成1024节点，并且有relu的隐藏层</span></span><br><span class="line"><span class="comment"># 图的初始化操作</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 输入数据，对于训练数据，我们使用一个占位符</span></span><br><span class="line">  <span class="comment"># 在运行时，用一个批次</span></span><br><span class="line">  tf_train_dataset = tf.placeholder(tf.float32,</span><br><span class="line">                                    shape=(batch_size, image_size * image_size))</span><br><span class="line">  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))</span><br><span class="line">  tf_valid_dataset = tf.constant(valid_dataset)</span><br><span class="line">  tf_test_dataset = tf.constant(test_dataset)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 初始化变量</span></span><br><span class="line">  weights = tf.Variable(</span><br><span class="line">    tf.truncated_normal([image_size * image_size, <span class="number">1024</span>]))</span><br><span class="line">  biases = tf.Variable(tf.zeros([<span class="number">1024</span>]))</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">  <span class="comment"># 训练计算</span></span><br><span class="line">  hidden1 = tf.matmul(tf_train_dataset, weights) + biases</span><br><span class="line">  hidden1 = tf.nn.relu(hidden1)</span><br><span class="line"></span><br><span class="line">  weights1 = tf.Variable(</span><br><span class="line">    tf.truncated_normal([<span class="number">1024</span>, num_labels]))</span><br><span class="line">  biases1 = tf.Variable(tf.zeros([num_labels]))</span><br><span class="line">  </span><br><span class="line">  logits = tf.matmul(hidden1, weights1) + biases1</span><br><span class="line">  </span><br><span class="line">  loss = tf.reduce_mean(</span><br><span class="line">    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))+\</span><br><span class="line">         <span class="number">1e-3</span> *(tf.nn.l2_loss(weights)+tf.nn.l2_loss(weights1))  </span><br><span class="line">  <span class="comment"># 优化器还是</span></span><br><span class="line">  optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(loss)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 计算预测值,这里的定义直接可以通过.evl()来调用</span></span><br><span class="line">  train_prediction = tf.nn.softmax(logits)</span><br><span class="line">  valid_prediction = tf.nn.softmax(</span><br><span class="line">      tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases),weights1) + biases1)</span><br><span class="line">  test_prediction = tf.nn.softmax(</span><br><span class="line">        tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases),weights1) + biases1)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">num_steps = <span class="number">3001</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> session:</span><br><span class="line">  tf.global_variables_initializer().run()</span><br><span class="line">  print(<span class="string">"Initialized"</span>)</span><br><span class="line">  starttime = time.clock()</span><br><span class="line">  print(<span class="string">'start time:'</span>,time.asctime( time.localtime(time.time()) ))</span><br><span class="line">  <span class="keyword">for</span> step <span class="keyword">in</span> range(num_steps):</span><br><span class="line">    <span class="comment"># 用偏移量来选择数据，10一批次， 101个数据，当取到12批次，offset = 110%101 = 9,可以保证批次有效</span></span><br><span class="line">    <span class="comment"># <span class="doctag">Note:</span>我们可以用更好的随机策略选择批次</span></span><br><span class="line">    offset = (step * batch_size) % (train_labels.shape[<span class="number">0</span>] - batch_size)</span><br><span class="line">    <span class="comment"># 创建一个批次.</span></span><br><span class="line">    batch_data = train_dataset[offset:(offset + batch_size), :]</span><br><span class="line">    batch_labels = train_labels[offset:(offset + batch_size), :]</span><br><span class="line">    <span class="comment"># 准备一个字典，来存放批数据，以便于 placeholder 来放数据</span></span><br><span class="line">    <span class="comment"># 数据类型 numpy 的数组</span></span><br><span class="line">    feed_dict = &#123;tf_train_dataset : batch_data, tf_train_labels : batch_labels&#125;</span><br><span class="line">    _, l, predictions = session.run(</span><br><span class="line">      [optimizer, loss, train_prediction], feed_dict=feed_dict)</span><br><span class="line">    <span class="keyword">if</span> (step % <span class="number">500</span> == <span class="number">0</span>):</span><br><span class="line">      print(<span class="string">"Minibatch loss at step %d: %f"</span> % (step, l))</span><br><span class="line">      print(<span class="string">"Minibatch accuracy: %.1f%%"</span> % accuracy(predictions, batch_labels))</span><br><span class="line">      print(<span class="string">"Validation accuracy: %.1f%%"</span> % accuracy(</span><br><span class="line">        valid_prediction.eval(), valid_labels))</span><br><span class="line">      print(<span class="string">"Test accuracy: %.1f%%"</span> % accuracy(test_prediction.eval(), test_labels))</span><br><span class="line">  print(<span class="string">'end time:'</span>,time.asctime( time.localtime(time.time()) ))</span><br><span class="line">  elapsed = (time.clock() - starttime)</span><br><span class="line">  print(<span class="string">"Time used:"</span>,elapsed)</span><br></pre></td></tr></table></figure>
<pre><code>Initialized
start time: Tue Sep  4 09:45:29 2018
Minibatch loss at step 0: 677.508362
Minibatch accuracy: 6.2%
Validation accuracy: 10.6%
Test accuracy: 11.5%
Minibatch loss at step 500: 342.153320
Minibatch accuracy: 76.6%
Validation accuracy: 74.5%
Test accuracy: 82.2%
Minibatch loss at step 1000: 336.932465
Minibatch accuracy: 75.0%
Validation accuracy: 74.6%
Test accuracy: 83.3%
Minibatch loss at step 1500: 315.919678
Minibatch accuracy: 84.4%
Validation accuracy: 75.6%
Test accuracy: 84.2%
Minibatch loss at step 2000: 310.289490
Minibatch accuracy: 85.9%
Validation accuracy: 75.8%
Test accuracy: 84.5%
Minibatch loss at step 2500: 308.458069
Minibatch accuracy: 85.2%
Validation accuracy: 76.7%
Test accuracy: 84.8%
Minibatch loss at step 3000: 304.995117
Minibatch accuracy: 84.4%
Validation accuracy: 76.5%
Test accuracy: 84.7%
end time: Tue Sep  4 09:46:01 2018
Time used: 31.499022337961833
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span>制作过拟合情况,通过减少训练批次的大小</span></span><br><span class="line"><span class="comment"># 下面训练随机梯度下降</span></span><br><span class="line"><span class="comment"># 将数据保存到常量节点，创建一个占位节点,每次用数据代入占位节点</span></span><br><span class="line"><span class="comment"># 图的初始化操作</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">50</span></span><br><span class="line"></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 输入数据，对于训练数据，我们使用一个占位符</span></span><br><span class="line">  <span class="comment"># 在运行时，用一个批次</span></span><br><span class="line">  tf_train_dataset = tf.placeholder(tf.float32,</span><br><span class="line">                                    shape=(batch_size, image_size * image_size))</span><br><span class="line">  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))</span><br><span class="line">  tf_valid_dataset = tf.constant(valid_dataset)</span><br><span class="line">  tf_test_dataset = tf.constant(test_dataset)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 初始化变量</span></span><br><span class="line">  weights = tf.Variable(</span><br><span class="line">    tf.truncated_normal([image_size * image_size, <span class="number">1024</span>]))</span><br><span class="line">  biases = tf.Variable(tf.zeros([<span class="number">1024</span>]))</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">  <span class="comment"># 训练计算</span></span><br><span class="line">  hidden1 = tf.matmul(tf_train_dataset, weights) + biases</span><br><span class="line">  hidden1 = tf.nn.relu(hidden1)</span><br><span class="line"></span><br><span class="line">  weights1 = tf.Variable(</span><br><span class="line">    tf.truncated_normal([<span class="number">1024</span>, num_labels]))</span><br><span class="line">  biases1 = tf.Variable(tf.zeros([num_labels]))</span><br><span class="line">  </span><br><span class="line">  logits = tf.matmul(hidden1, weights1) + biases1</span><br><span class="line">  </span><br><span class="line">  loss = tf.reduce_mean(</span><br><span class="line">    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))+\</span><br><span class="line">         <span class="number">0</span> *(tf.nn.l2_loss(weights)+tf.nn.l2_loss(weights1))  </span><br><span class="line">  <span class="comment"># 优化器还是</span></span><br><span class="line">  optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(loss)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 计算预测值,这里的定义直接可以通过.evl()来调用</span></span><br><span class="line">  train_prediction = tf.nn.softmax(logits)</span><br><span class="line">  valid_prediction = tf.nn.softmax(</span><br><span class="line">      tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases),weights1) + biases1)</span><br><span class="line">  test_prediction = tf.nn.softmax(</span><br><span class="line">        tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases),weights1) + biases1)</span><br><span class="line"></span><br><span class="line">num_steps = <span class="number">300</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> session:</span><br><span class="line">  tf.global_variables_initializer().run()</span><br><span class="line">  print(<span class="string">"Initialized"</span>)</span><br><span class="line">  starttime = time.clock()</span><br><span class="line">  print(<span class="string">'start time:'</span>,time.asctime( time.localtime(time.time()) ))</span><br><span class="line">  <span class="keyword">for</span> step <span class="keyword">in</span> range(num_steps):</span><br><span class="line">    <span class="comment"># 用偏移量来选择数据，10一批次， 101个数据，当取到12批次，offset = 110%101 = 9,可以保证批次有效</span></span><br><span class="line">    <span class="comment"># <span class="doctag">Note:</span>我们可以用更好的随机策略选择批次</span></span><br><span class="line">    offset = (step * batch_size) % (train_labels.shape[<span class="number">0</span>] - batch_size)</span><br><span class="line">    <span class="comment"># 创建一个批次.</span></span><br><span class="line">    batch_data = train_dataset[offset:(offset + batch_size), :]</span><br><span class="line">    batch_labels = train_labels[offset:(offset + batch_size), :]</span><br><span class="line">    <span class="comment"># 准备一个字典，来存放批数据，以便于 placeholder 来放数据</span></span><br><span class="line">    <span class="comment"># 数据类型 numpy 的数组</span></span><br><span class="line">    feed_dict = &#123;tf_train_dataset : batch_data, tf_train_labels : batch_labels&#125;</span><br><span class="line">    _, l, predictions = session.run(</span><br><span class="line">      [optimizer, loss, train_prediction], feed_dict=feed_dict)</span><br><span class="line">    <span class="keyword">if</span> (step % <span class="number">30</span> == <span class="number">0</span>):</span><br><span class="line">      print(<span class="string">"Minibatch loss at step %d: %f"</span> % (step, l))</span><br><span class="line">      print(<span class="string">"Minibatch accuracy: %.1f%%"</span> % accuracy(predictions, batch_labels))</span><br><span class="line">      print(<span class="string">"Validation accuracy: %.1f%%"</span> % accuracy(</span><br><span class="line">        valid_prediction.eval(), valid_labels))</span><br><span class="line">      print(<span class="string">"Test accuracy: %.1f%%"</span> % accuracy(test_prediction.eval(), test_labels))</span><br><span class="line">  print(<span class="string">'end time:'</span>,time.asctime( time.localtime(time.time()) ))</span><br><span class="line">  elapsed = (time.clock() - starttime)</span><br><span class="line">  print(<span class="string">"Time used:"</span>,elapsed)</span><br></pre></td></tr></table></figure>
<pre><code>Initialized
start time: Tue Sep  4 09:51:58 2018
Minibatch loss at step 0: 301.258759
Minibatch accuracy: 4.0%
Validation accuracy: 21.8%
Test accuracy: 22.0%
Minibatch loss at step 30: 135.621140
Minibatch accuracy: 68.0%
Validation accuracy: 75.7%
Test accuracy: 80.1%
Minibatch loss at step 60: 62.916607
Minibatch accuracy: 80.0%
Validation accuracy: 75.6%
Test accuracy: 80.8%
Minibatch loss at step 90: 75.922493
Minibatch accuracy: 68.0%
Validation accuracy: 72.6%
Test accuracy: 77.4%
Minibatch loss at step 120: 126.814140
Minibatch accuracy: 70.0%
Validation accuracy: 72.4%
Test accuracy: 80.6%
Minibatch loss at step 150: 41.706875
Minibatch accuracy: 72.0%
Validation accuracy: 76.6%
Test accuracy: 83.5%
Minibatch loss at step 180: 46.694637
Minibatch accuracy: 78.0%
Validation accuracy: 75.7%
Test accuracy: 81.1%
Minibatch loss at step 210: 101.218369
Minibatch accuracy: 80.0%
Validation accuracy: 75.6%
Test accuracy: 82.2%
Minibatch loss at step 240: 22.329721
Minibatch accuracy: 76.0%
Validation accuracy: 78.1%
Test accuracy: 83.4%
Minibatch loss at step 270: 29.404995
Minibatch accuracy: 70.0%
Validation accuracy: 79.2%
Test accuracy: 83.8%
end time: Tue Sep  4 09:52:01 2018
Time used: 2.9922106035760407
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># task 3,加入Dropout 随机失活</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> 使用nn.dropout来处理，但是只是用于训练时，评估时不需要用</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 输入数据，对于训练数据，我们使用一个占位符</span></span><br><span class="line">  <span class="comment"># 在运行时，用一个批次</span></span><br><span class="line">  tf_train_dataset = tf.placeholder(tf.float32,</span><br><span class="line">                                    shape=(batch_size, image_size * image_size))</span><br><span class="line">  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))</span><br><span class="line">  tf_valid_dataset = tf.constant(valid_dataset)</span><br><span class="line">  tf_test_dataset = tf.constant(test_dataset)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 初始化变量</span></span><br><span class="line">  weights = tf.Variable(</span><br><span class="line">    tf.truncated_normal([image_size * image_size, <span class="number">1024</span>]))</span><br><span class="line">  biases = tf.Variable(tf.zeros([<span class="number">1024</span>]))</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">  <span class="comment"># 训练计算</span></span><br><span class="line">  hidden1 = tf.matmul(tf_train_dataset, weights) + biases</span><br><span class="line">  hidden1 = tf.nn.relu(hidden1)</span><br><span class="line"></span><br><span class="line">  weights1 = tf.Variable(</span><br><span class="line">    tf.truncated_normal([<span class="number">1024</span>, num_labels]))</span><br><span class="line">  biases1 = tf.Variable(tf.zeros([num_labels]))</span><br><span class="line">  <span class="comment"># 采用dropout 随机失活</span></span><br><span class="line">  logits = tf.matmul(tf.nn.dropout(hidden1,<span class="number">0.5</span>), weights1) + biases1</span><br><span class="line">  </span><br><span class="line">  loss = tf.reduce_mean(</span><br><span class="line">    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))</span><br><span class="line">  <span class="comment"># 优化器还是</span></span><br><span class="line">  optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(loss)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 计算预测值,这里的定义直接可以通过.evl()来调用</span></span><br><span class="line">  train_prediction = tf.nn.softmax(logits)</span><br><span class="line">  valid_prediction = tf.nn.softmax(</span><br><span class="line">      tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases),weights1) + biases1)</span><br><span class="line">  test_prediction = tf.nn.softmax(</span><br><span class="line">        tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases),weights1) + biases1)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">num_steps = <span class="number">3001</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(predictions, labels)</span>:</span></span><br><span class="line">  <span class="comment"># argmax  axis = 0的时候返回每一列最大值的位置索引</span></span><br><span class="line">  <span class="comment"># axis = 1的时候返回每一行最大值的位置索引</span></span><br><span class="line">  <span class="comment"># axis = 2、3、4...，即为多维张量时，同理推断</span></span><br><span class="line">  <span class="comment"># 得到true，fasle 的向量，然后统计1的个数，就是正确率</span></span><br><span class="line">  <span class="keyword">return</span> (<span class="number">100.0</span> * np.sum(np.argmax(predictions, <span class="number">1</span>) == np.argmax(labels, <span class="number">1</span>))</span><br><span class="line">          / predictions.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> session:</span><br><span class="line">  tf.global_variables_initializer().run()</span><br><span class="line">  print(<span class="string">"Initialized"</span>)</span><br><span class="line">  starttime = time.clock()</span><br><span class="line">  print(<span class="string">'start time:'</span>,time.asctime( time.localtime(time.time()) ))</span><br><span class="line">  <span class="keyword">for</span> step <span class="keyword">in</span> range(num_steps):</span><br><span class="line">    <span class="comment"># 用偏移量来选择数据，10一批次， 101个数据，当取到12批次，offset = 110%101 = 9,可以保证批次有效</span></span><br><span class="line">    <span class="comment"># <span class="doctag">Note:</span>我们可以用更好的随机策略选择批次</span></span><br><span class="line">    offset = (step * batch_size) % (train_labels.shape[<span class="number">0</span>] - batch_size)</span><br><span class="line">    <span class="comment"># 创建一个批次.</span></span><br><span class="line">    batch_data = train_dataset[offset:(offset + batch_size), :]</span><br><span class="line">    batch_labels = train_labels[offset:(offset + batch_size), :]</span><br><span class="line">    <span class="comment"># 准备一个字典，来存放批数据，以便于 placeholder 来放数据</span></span><br><span class="line">    <span class="comment"># 数据类型 numpy 的数组</span></span><br><span class="line">    feed_dict = &#123;tf_train_dataset : batch_data, tf_train_labels : batch_labels&#125;</span><br><span class="line">    _, l, predictions = session.run(</span><br><span class="line">      [optimizer, loss, train_prediction], feed_dict=feed_dict)</span><br><span class="line">    <span class="keyword">if</span> (step % <span class="number">500</span> == <span class="number">0</span>):</span><br><span class="line">      print(<span class="string">"Minibatch loss at step %d: %f"</span> % (step, l))</span><br><span class="line">      print(<span class="string">"Minibatch accuracy: %.1f%%"</span> % accuracy(predictions, batch_labels))</span><br><span class="line">      print(<span class="string">"Validation accuracy: %.1f%%"</span> % accuracy(</span><br><span class="line">        valid_prediction.eval(), valid_labels))</span><br><span class="line">      print(<span class="string">"Test accuracy: %.1f%%"</span> % accuracy(test_prediction.eval(), test_labels))</span><br><span class="line">  print(<span class="string">'end time:'</span>,time.asctime( time.localtime(time.time()) ))</span><br><span class="line">  elapsed = (time.clock() - starttime)</span><br><span class="line">  print(<span class="string">"Time used:"</span>,elapsed)</span><br></pre></td></tr></table></figure>
<pre><code>Initialized
start time: Tue Sep  4 12:01:16 2018
Minibatch loss at step 0: 431.407959
Minibatch accuracy: 14.8%
Validation accuracy: 25.6%
Test accuracy: 28.3%
Minibatch loss at step 500: 13.565708
Minibatch accuracy: 79.7%
Validation accuracy: 82.1%
Test accuracy: 86.1%
Minibatch loss at step 1000: 12.175354
Minibatch accuracy: 75.0%
Validation accuracy: 82.2%
Test accuracy: 88.3%
Minibatch loss at step 1500: 4.373288
Minibatch accuracy: 78.1%
Validation accuracy: 81.5%
Test accuracy: 87.0%
Minibatch loss at step 2000: 4.806540
Minibatch accuracy: 83.6%
Validation accuracy: 81.6%
Test accuracy: 87.6%
Minibatch loss at step 2500: 7.298935
Minibatch accuracy: 79.7%
Validation accuracy: 81.0%
Test accuracy: 89.0%
Minibatch loss at step 3000: 1.783429
Minibatch accuracy: 85.2%
Validation accuracy: 82.0%
Test accuracy: 89.3%
end time: Tue Sep  4 12:01:46 2018
Time used: 30.266830585829666
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># task4 ,调整训练精确度，最高据说达到了97%</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span>两种思路，一种是学习速率衰减</span></span><br><span class="line"><span class="comment"># 另一种是增加多层</span></span><br><span class="line"><span class="comment"># 我先来试试学习速率衰减+dropout +正则化：准确率89%</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 输入数据，对于训练数据，我们使用一个占位符</span></span><br><span class="line">  <span class="comment"># 在运行时，用一个批次</span></span><br><span class="line">  tf_train_dataset = tf.placeholder(tf.float32,</span><br><span class="line">                                    shape=(batch_size, image_size * image_size))</span><br><span class="line">  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))</span><br><span class="line">  tf_valid_dataset = tf.constant(valid_dataset)</span><br><span class="line">  tf_test_dataset = tf.constant(test_dataset)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 第一层,层数多了会出现梯度消失，所有初始化时很有讲究，就是初始化为前一层所有节点的个数</span></span><br><span class="line">  w1 = tf.Variable(tf.truncated_normal([image_size * image_size, <span class="number">1024</span>],stddev=np.sqrt(<span class="number">2.0</span>/(image_size * image_size) )))</span><br><span class="line">  b1 = tf.Variable(tf.zeros([<span class="number">1024</span>]))</span><br><span class="line">  a1 = tf.nn.relu(tf.matmul(tf_train_dataset, w1) + b1)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 第二层</span></span><br><span class="line">  w2 = tf.Variable(tf.truncated_normal([<span class="number">1024</span>, <span class="number">512</span>],stddev=np.sqrt(<span class="number">2.0</span>/<span class="number">1024</span> )))</span><br><span class="line">  b2 = tf.Variable(tf.zeros([<span class="number">512</span>]))</span><br><span class="line">  a2 = tf.nn.tanh(tf.matmul(a1, w2) + b2)</span><br><span class="line">  <span class="comment"># </span></span><br><span class="line">  <span class="comment"># 第三层</span></span><br><span class="line">  w3 = tf.Variable(tf.truncated_normal([<span class="number">512</span>, <span class="number">256</span>],stddev=np.sqrt(<span class="number">2.0</span>/<span class="number">512</span> )))</span><br><span class="line">  b3 = tf.Variable(tf.zeros([<span class="number">256</span>]))</span><br><span class="line">  a3 = tf.nn.relu(tf.matmul(a2, w3) + b3)</span><br><span class="line">  <span class="comment"># </span></span><br><span class="line">  <span class="comment"># 第四层</span></span><br><span class="line">  w4 = tf.Variable(tf.truncated_normal([<span class="number">256</span>, <span class="number">128</span>],stddev=np.sqrt(<span class="number">2.0</span>/<span class="number">256</span> )))</span><br><span class="line">  b4 = tf.Variable(tf.zeros([<span class="number">128</span>]))</span><br><span class="line">  a4 = tf.nn.relu(tf.matmul(a3, w4) + b4)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># # 第五层</span></span><br><span class="line">  w5 = tf.Variable(tf.truncated_normal([<span class="number">128</span>, num_labels],stddev=np.sqrt(<span class="number">2.0</span>/<span class="number">128</span> )))</span><br><span class="line">  b5 = tf.Variable(tf.zeros([num_labels]))</span><br><span class="line">  <span class="comment"># 特别注意，最后一层不要用激活函数，会梯度消失的，最后都是全连接层</span></span><br><span class="line">  logits = tf.matmul(a4, w5) + b5</span><br><span class="line">            </span><br><span class="line">  </span><br><span class="line">  loss = tf.reduce_mean(</span><br><span class="line">    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))</span><br><span class="line">  <span class="comment"># 优化器还是GD+学习速率衰减</span></span><br><span class="line">  global_step = tf.Variable(<span class="number">0</span>)  <span class="comment"># count the number of steps taken.</span></span><br><span class="line">  learning_rate = tf.train.exponential_decay(<span class="number">0.5</span>, global_step,decay_steps=<span class="number">500</span>,decay_rate=<span class="number">0.96</span>)</span><br><span class="line">  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)</span><br><span class="line">  <span class="comment"># optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 计算预测值,这里的定义直接可以通过.evl()来调用</span></span><br><span class="line">  train_prediction = tf.nn.softmax(logits)</span><br><span class="line">  </span><br><span class="line">  p1 = tf.nn.relu(tf.matmul(tf_valid_dataset, w1) + b1)</span><br><span class="line">  p1 = tf.nn.relu(tf.matmul(p1, w2) + b2)</span><br><span class="line">  p1 = tf.nn.relu(tf.matmul(p1, w3) + b3)</span><br><span class="line">  p1 = tf.nn.relu(tf.matmul(p1, w4) + b4)</span><br><span class="line">  p1 = tf.nn.relu(tf.matmul(p1, w5) + b5)</span><br><span class="line">  valid_prediction = tf.nn.softmax(p1)</span><br><span class="line">  </span><br><span class="line">  t1 = tf.nn.relu(tf.matmul(tf_test_dataset, w1) + b1)</span><br><span class="line">  t1 = tf.nn.relu(tf.matmul(t1, w2) + b2)</span><br><span class="line">  t1 = tf.nn.relu(tf.matmul(t1, w3) + b3)</span><br><span class="line">  t1 = tf.nn.relu(tf.matmul(t1, w4) + b4)</span><br><span class="line">  t1 = tf.nn.relu(tf.matmul(t1, w5) + b5)</span><br><span class="line">  test_prediction = tf.nn.softmax(t1)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">num_steps = <span class="number">6000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> session:</span><br><span class="line">  tf.global_variables_initializer().run()</span><br><span class="line">  print(<span class="string">"Initialized"</span>)</span><br><span class="line">  starttime = time.clock()</span><br><span class="line">  print(<span class="string">'start time:'</span>,time.asctime( time.localtime(time.time()) ))</span><br><span class="line">  <span class="keyword">for</span> step <span class="keyword">in</span> range(num_steps):</span><br><span class="line">    <span class="comment"># 用偏移量来选择数据，10一批次， 101个数据，当取到12批次，offset = 110%101 = 9,可以保证批次有效</span></span><br><span class="line">    <span class="comment"># <span class="doctag">Note:</span>我们可以用更好的随机策略选择批次</span></span><br><span class="line">    offset = (step * batch_size) % (train_labels.shape[<span class="number">0</span>] - batch_size)</span><br><span class="line">    <span class="comment"># 创建一个批次.</span></span><br><span class="line">    batch_data = train_dataset[offset:(offset + batch_size), :]</span><br><span class="line">    batch_labels = train_labels[offset:(offset + batch_size), :]</span><br><span class="line">    <span class="comment"># 准备一个字典，来存放批数据，以便于 placeholder 来放数据</span></span><br><span class="line">    <span class="comment"># 数据类型 numpy 的数组</span></span><br><span class="line">    feed_dict = &#123;tf_train_dataset : batch_data, tf_train_labels : batch_labels&#125;</span><br><span class="line">    _, l, predictions = session.run(</span><br><span class="line">      [optimizer, loss, train_prediction], feed_dict=feed_dict)</span><br><span class="line">    <span class="keyword">if</span> (step % <span class="number">500</span> == <span class="number">0</span>):</span><br><span class="line">      print(<span class="string">"Minibatch loss at step %d: %f"</span> % (step, l))</span><br><span class="line">      print(<span class="string">"Minibatch accuracy: %.1f%%"</span> % accuracy(predictions, batch_labels))</span><br><span class="line">      print(<span class="string">"Validation accuracy: %.1f%%"</span> % accuracy(</span><br><span class="line">        valid_prediction.eval(), valid_labels))</span><br><span class="line">      print(<span class="string">"Test accuracy: %.1f%%"</span> % accuracy(test_prediction.eval(), test_labels))</span><br><span class="line">  print(<span class="string">'end time:'</span>,time.asctime( time.localtime(time.time()) ))</span><br><span class="line">  elapsed = (time.clock() - starttime)</span><br><span class="line">  print(<span class="string">"Time used:"</span>,elapsed)</span><br></pre></td></tr></table></figure>
<pre><code>Initialized
start time: Tue Sep  4 14:47:32 2018
Minibatch loss at step 0: 2.305960
Minibatch accuracy: 14.8%
Validation accuracy: 39.5%
Test accuracy: 39.8%
Minibatch loss at step 500: 0.361337
Minibatch accuracy: 89.8%
Validation accuracy: 84.6%
Test accuracy: 90.0%
Minibatch loss at step 1000: 0.176127
Minibatch accuracy: 92.2%
Validation accuracy: 84.7%
Test accuracy: 89.9%
Minibatch loss at step 1500: 0.134203
Minibatch accuracy: 95.3%
Validation accuracy: 83.9%
Test accuracy: 89.3%
Minibatch loss at step 2000: 0.032851
Minibatch accuracy: 99.2%
Validation accuracy: 86.0%
Test accuracy: 91.0%
Minibatch loss at step 2500: 0.023428
Minibatch accuracy: 100.0%
Validation accuracy: 86.1%
Test accuracy: 90.8%
Minibatch loss at step 3000: 0.051366
Minibatch accuracy: 98.4%
Validation accuracy: 86.1%
Test accuracy: 91.7%
Minibatch loss at step 3500: 0.026836
Minibatch accuracy: 99.2%
Validation accuracy: 86.1%
Test accuracy: 91.9%
Minibatch loss at step 4000: 0.002356
Minibatch accuracy: 100.0%
Validation accuracy: 86.1%
Test accuracy: 91.4%
Minibatch loss at step 4500: 0.002357
Minibatch accuracy: 100.0%
Validation accuracy: 86.3%
Test accuracy: 91.6%
Minibatch loss at step 5000: 0.000582
Minibatch accuracy: 100.0%
Validation accuracy: 86.7%
Test accuracy: 91.1%
Minibatch loss at step 5500: 0.024401
Minibatch accuracy: 99.2%
Validation accuracy: 86.4%
Test accuracy: 90.7%
end time: Tue Sep  4 14:49:17 2018
Time used: 105.25818238910142
</code></pre>
    </div>

    <div class="post-footer">
        <div>
            
                转载声明：商业转载请联系作者获得授权,非商业转载请注明出处 © <a href="" target="_blank">sudl</a>
            
        </div>
        <div>
            
        </div>
    </div>
</article>

<div class="article-nav prev-next-wrap clearfix">
    
        <a href="/2018/05/03/2018-05-03-卷积神经网络/" class="pre-post btn btn-default" title='卷积神经网络'>
            <i class="fa fa-angle-left fa-fw"></i><span class="hidden-lg">上一篇</span>
            <span class="hidden-xs">卷积神经网络</span>
        </a>
    
    
        <a href="/2018/04/10/22深度学习的学习速率曲线/" class="next-post btn btn-default" title='深度学习的学习速率曲线'>
            <span class="hidden-lg">下一篇</span>
            <span class="hidden-xs">深度学习的学习速率曲线</span><i class="fa fa-angle-right fa-fw"></i>
        </a>
    
</div>


    <div id="comments">
        
	
<div id="lv-container" data-id="city" data-uid="MTAyMC8zMzA1MS85NjEz">
  <script type="text/javascript">
     (function(d, s) {
         var j, e = d.getElementsByTagName(s)[0];
         if (typeof LivereTower === 'function') { return; }
         j = d.createElement(s);
         j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
         j.async = true;
         e.parentNode.insertBefore(j, e);
     })(document, 'script');
  </script>
</div>


    </div>





                </main>
                
                    <aside id="article-toc" role="navigation" class="col-md-4">
    <div class="widget">
        <h3 class="title">文章目录</h3>
        
            <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#要点"><span class="toc-text">要点</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#函数"><span class="toc-text">函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-nn-dropout"><span class="toc-text">tf.nn.dropout</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-train-exponential-decay"><span class="toc-text">tf.train.exponential_decay</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#代码"><span class="toc-text">代码</span></a></li></ol>
        
    </div>
</aside>

                
            </div>
        </div>
    </section>
    <footer class="main-footer">
    <div class="container">
        <div class="row">
        </div>
    </div>
</footer>

<a id="back-to-top" class="icon-btn hide">
	<i class="fa fa-chevron-up"></i>
</a>




    <div class="copyright">
    <div class="container">
        <div class="row">
            <div class="col-sm-12"> 
                <span>Copyright &copy; 2017
                </span> | 
                <span>
                    Powered by <a href="//hexo.io" class="copyright-links" target="_blank" rel="nofollow">Hexo</a>
                </span> | 
                <span>
                    Theme by <a href="//github.com/shenliyang/hexo-theme-snippet.git" class="copyright-links" target="_blank" rel="nofollow">Snippet</a>
                </span>
            </div>
        </div>
    </div>
</div>



<script src="/js/app.js?rev=@@hash"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</body>
</html>