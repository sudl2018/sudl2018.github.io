<!DOCTYPE HTML>
<html lang="zh-CN">
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!--Setting-->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta http-equiv="Cache-Control" content="no-siteapp">
    <meta http-equiv="Cache-Control" content="no-transform">
    <meta name="renderer" content="webkit|ie-comp|ie-stand">
    <meta name="apple-mobile-web-app-capable" content="sudoli&#39;s blog">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="format-detection" content="telephone=no,email=no,adress=no">
    <meta name="browsermode" content="application">
    <meta name="screen-orientation" content="portrait">
    <link rel="dns-prefetch" href="http://yoursite.com">
    <!--SEO-->





<meta name="robots" content="all" />
<meta name="google" content="all" />
<meta name="googlebot" content="all" />
<meta name="verify" content="all" />
    <!--Title-->


<title>深度神经网络的基本概念——随机梯度下降与梯度下降 | sudoli&#39;s blog</title>


    <link rel="alternate" href="/atom.xml" title="sudoli&#39;s blog" type="application/atom+xml">


    <link rel="icon" href="/favicon.ico">

    



<link rel="stylesheet" href="/css/bootstrap.min.css?rev=3.3.7">
<link rel="stylesheet" href="/css/font-awesome.min.css?rev=4.5.0">
<link rel="stylesheet" href="/css/style.css?rev=@@hash">




    
	<div class="hide">
		<script type="text/javascript">
			var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan class='cnzz_stat_icon_1263868967 hide' %3E%3Cscript%20src%3D%22https%3A%2F%2Fs95.cnzz.com%2Fz_stat.php%3Fweb_id%3D1272564536%22%3E%3C%2Fscript%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s19.cnzz.com/z_stat.php%3Fid%3D1263868967%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
		</script>
	</div><!-- hexo-inject:begin --><!-- hexo-inject:end -->






    

</head>


<!--[if lte IE 8]>
<style>
    html{ font-size: 1em }
</style>
<![endif]-->
<!--[if lte IE 9]>
<div style="ie">你使用的浏览器版本过低，为了你更好的阅读体验，请更新浏览器的版本或者使用其他现代浏览器，比如Chrome、Firefox、Safari等。</div>
<![endif]-->

<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="main-header"  style="background-image:url(http://onq81n53u.bkt.clouddn.com/neweeess_%E5%89%AF%E6%9C%AC.png)"  >
    <div class="main-header-box">
        <a class="header-avatar" href="/" title='sudoli'>
            <img src="/img/avatar.jpg" alt="logo头像" class="img-responsive center-block">
        </a>
        <div class="branding">
        	<!--<h2 class="text-hide">Snippet主题,从未如此简单有趣</h2>-->
            
                <h2> best </h2>
            
    	</div>
    </div>
</header>
    <nav class="main-navigation">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="navbar-header"><span class="nav-toggle-button collapsed pull-right" data-toggle="collapse" data-target="#main-menu" id="mnav">
                    <span class="sr-only"></span>
                        <i class="fa fa-bars"></i>
                    </span>
                    <a class="navbar-brand" href="http://yoursite.com">sudoli&#39;s blog</a>
                </div>
                <div class="collapse navbar-collapse" id="main-menu">
                    <ul class="menu">
                        
                            <li role="presentation" class="text-center">
                                <a href="/"><i class="fa "></i>主页</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/机器学习/"><i class="fa "></i>机器学习</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/深度学习/"><i class="fa "></i>深度学习</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/算法/"><i class="fa "></i>算法</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/archives/"><i class="fa "></i>时间轴</a>
                            </li>
                        
                    </ul>
                </div>
            </div>
        </div>
    </div>
</nav>
    <section class="content-wrap">
        <div class="container">
            <div class="row">
                <main class="col-md-8 main-content m-post">
                    <p id="process"></p>
<article class="post">
    <div class="post-head">
        <h1 id="深度神经网络的基本概念——随机梯度下降与梯度下降">
            
	            深度神经网络的基本概念——随机梯度下降与梯度下降
            
        </h1>
        <div class="post-meta">
    
    
    <span class="categories-meta fa-wrap">
        <i class="fa fa-folder-open-o"></i>
        <a href="/categories/深度学习">
            深度学习
        </a>
    </span>
    
    
    <span class="fa-wrap">
        <i class="fa fa-tags"></i>
        <span class="tags-meta">
            
                
                    <a href="/tags/深度学习" title='深度学习'>
                        深度学习
                    </a>
                
                    <a href="/tags/TensorFlow" title='TensorFlow'>
                        TensorFlow
                    </a>
                
                    <a href="/tags/SGD" title='SGD'>
                        SGD
                    </a>
                
            
        </span>
    </span>
    

    
        
        <span class="fa-wrap">
            <i class="fa fa-clock-o"></i>
            <span class="date-meta">2018/04/03</span>
        </span>
    
</div>

            
            
    </div>
    
    <div class="post-body post-content">
        <h2 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h2><h4 id="使用梯度下降和随机梯度下降训练一个全连接网络"><a href="#使用梯度下降和随机梯度下降训练一个全连接网络" class="headerlink" title="使用梯度下降和随机梯度下降训练一个全连接网络"></a><strong>使用梯度下降和随机梯度下降训练一个全连接网络</strong></h4><ul>
<li>数据集<ul>
<li>数据集采用的是notMNIST数据集，这个更加像真实的数据集，不如MNIST 数据集干净，更加有难</li>
<li>数据集有训练集 500K ，测试集 19000，这个规模在PC机上跑很快</li>
<li>标签为A 到 J (10个类别)</li>
<li>每个图像特征 28*28 像素</li>
<li><a href="http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html" target="_blank" rel="noopener">原数据地</a></li>
</ul>
</li>
<li>TensorFlow 的工作流程如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"> graph TD</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Input[节点:输入]	--&gt;	Graph</span><br><span class="line">Var[节点:变量]		--&gt;	Graph</span><br><span class="line">oper[节点:操作]	--&gt;	Graph</span><br><span class="line">Graph[初始化Graph]--&gt;init(with graph.as_default)</span><br><span class="line">init--&gt;run(具体执行 session.run)</span><br><span class="line">     run--&gt;res(拿到graph的执行结果with tf.Session graph=graph  as session:)</span><br></pre></td></tr></table></figure>
<h4 id="用到的TensorFlow函数"><a href="#用到的TensorFlow函数" class="headerlink" title="用到的TensorFlow函数:"></a>用到的TensorFlow函数:</h4><ul>
<li><strong>tf.truncated_normal</strong><ul>
<li>从一个正态分布片段中输出随机数值，只保留两个标准差以内的值，超出的值会被弃掉重新生成</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf</span>.<span class="title">truncated_normal</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">      shape, #一个一维整数张量 或 一个Python数组。 这个值决定输出张量的形状。</span></span></span><br><span class="line"><span class="function"><span class="params">      mean=<span class="number">0.0</span>,#一个零维张量或 类型属于dtype的Python值. 这个值决定正态分布片段的平均值</span></span></span><br><span class="line"><span class="function"><span class="params">      stddev=<span class="number">1.0</span>,# 一个零维张量或 类型属于dtype的Python值. 这个值决定正态分布片段的标准差</span></span></span><br><span class="line"><span class="function"><span class="params">      dtype=tf.float32,# 输出的类型.</span></span></span><br><span class="line"><span class="function"><span class="params">      seed=None, # 一个Python整数. 被用来为正态分布创建一个随机种子. </span></span></span><br><span class="line"><span class="function"><span class="params">      name=None)</span>#操作的名字 <span class="params">(可选参数)</span>.</span></span><br></pre></td></tr></table></figure>
<ul>
<li></li>
<li><p><strong>reduce_mean</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.reduce_mean(</span><br><span class="line">  input_tensor, <span class="comment"># 需要求平均值的张量。应该存在数字类型。</span></span><br><span class="line">  axis=<span class="keyword">None</span>, 		<span class="comment"># 需要求平均值的维度. 如果没有设置（默认情况），所有的维度都会被减值。</span></span><br><span class="line">  keep_dims=<span class="keyword">False</span>, <span class="comment"># 如果为真，维持减少的维度长度为1.</span></span><br><span class="line">  name=<span class="keyword">None</span>, 			<span class="comment"># 操作的名字(可选值)</span></span><br><span class="line">  reduction_indices=<span class="keyword">None</span>) <span class="comment">#旧的**axis**参数的名字(已弃用)</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>可跨越维度的计算张量各元素的平均值</p>
</li>
<li><p>例子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">20</span>).reshape(<span class="number">4</span>,<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">array([[ 0,  1,  2,  3,  4],</span><br><span class="line">       [ 5,  6,  7,  8,  9],</span><br><span class="line">       [10, 11, 12, 13, 14],</span><br><span class="line">       [15, 16, 17, 18, 19]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对列求平均</span></span><br><span class="line">b = tf.reduce_mean(a,<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(b)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([ 7,  8,  9, 10, 11])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对行求平均</span></span><br><span class="line">c = tf.reduce_mean(a,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(c)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([ 2,  7, 12, 17])</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><strong>softmax_cross_entropy_with_logits</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_cross_entropy_with_logits</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    _sentinel=None,  </span></span></span><br><span class="line"><span class="function"><span class="params">    labels=None,  # labels one-hot 向量</span></span></span><br><span class="line"><span class="function"><span class="params">    logits=None,  # 预测值 one-hot 向量</span></span></span><br><span class="line"><span class="function"><span class="params">    dim=<span class="number">-1</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">    name=None )</span>:</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sparse_softmax_cross_entropy_with_logits</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    _sentinel=None,  </span></span></span><br><span class="line"><span class="function"><span class="params">    labels=None, # 原始标签</span></span></span><br><span class="line"><span class="function"><span class="params">    logits=None, # 预测值 one-hot 向量</span></span></span><br><span class="line"><span class="function"><span class="params">    name=None )</span>:</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>例子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 神经网络的输出</span></span><br><span class="line">logits=tf.constant([[<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>],[<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>],[<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>]]) </span><br><span class="line"><span class="comment"># 对输出做softmax操作</span></span><br><span class="line">y=tf.nn.softmax(logits)  </span><br><span class="line"><span class="comment"># 真实数据标签，one hot形式</span></span><br><span class="line">y_=tf.constant([[<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">1.0</span>],[<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">1.0</span>],[<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">1.0</span>]]) </span><br><span class="line"><span class="comment"># 将标签稠密化</span></span><br><span class="line">dense_y=tf.argmax(y_,<span class="number">1</span>)   <span class="comment"># dense_y = [2 2 2]</span></span><br><span class="line"><span class="comment"># 采用普通方式计算交叉熵</span></span><br><span class="line">cross_entropy = -tf.reduce_sum(y_*tf.log(y))</span><br><span class="line"><span class="comment"># 使用softmax_cross_entropy_with_logits方法计算交叉熵</span></span><br><span class="line">cross_entropy2=tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))</span><br><span class="line"><span class="comment"># 使用sparse_softmax_cross_entropy_with_logits方法计算交叉熵</span></span><br><span class="line">cross_entropy3=tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=dense_y))</span><br><span class="line">  </span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:  </span><br><span class="line">    softmax=sess.run(y)</span><br><span class="line">    c_e = sess.run(cross_entropy)</span><br><span class="line">    c_e2 = sess.run(cross_entropy2)</span><br><span class="line">    c_e3 = sess.run(cross_entropy3)</span><br><span class="line">    print(<span class="string">"step1:softmax result="</span>)  </span><br><span class="line">    print(softmax)  </span><br><span class="line">    print(<span class="string">"y_ = "</span>)</span><br><span class="line">    print(sess.run(y_))</span><br><span class="line">    print(<span class="string">"tf.log(y) = "</span>)</span><br><span class="line">    print(sess.run(tf.log(y)))</span><br><span class="line">    print(<span class="string">"dense_y ="</span>)</span><br><span class="line">    print(sess.run(dense_y))</span><br><span class="line">    print(<span class="string">"step2:cross_entropy result="</span>)  </span><br><span class="line">    print(c_e)  </span><br><span class="line">    print(<span class="string">"Function(softmax_cross_entropy_with_logits) result="</span>)  </span><br><span class="line">    print(c_e2)</span><br><span class="line">    print(<span class="string">"Function(sparse_softmax_cross_entropy_with_logits) result="</span>)</span><br><span class="line">    print(c_e3)</span><br><span class="line"></span><br><span class="line">作者：泊牧</span><br><span class="line">链接：https://www.jianshu.com/p/<span class="number">3</span>b084ec9ed80</span><br><span class="line">來源：简书</span><br><span class="line">简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><strong>np.argmax  tf.argmax</strong></p>
<ul>
<li>返回最大值的索引，常用于softmax 后的结果预测以及ont-hot 向量的处理</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.argmax(input,</span><br><span class="line">          axis=<span class="keyword">None</span>,<span class="comment"># axis = 0的时候返回每一列最大值的位置索引 ;axis = 1的时候返回每一行最大值的位置索引 ;axis = 2、3、4...，即为多维张量时，同理推断</span></span><br><span class="line">          name=<span class="keyword">None</span>, </span><br><span class="line">          dimension=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># These are all the modules we'll be using later. Make sure you can import them</span></span><br><span class="line"><span class="comment"># before proceeding further.</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> cPickle <span class="keyword">as</span> pickle</span><br><span class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> range</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据集采用的是notminist 数据集，这个更加像真实的数据集，不如mnist 数据集干净，更加有难度</span></span><br><span class="line"><span class="comment"># 数据集有训练集 500K ，测试集 19000，这个规模在PC机上跑很快</span></span><br><span class="line"><span class="comment"># 标签为A 到 J (10个类别)</span></span><br><span class="line"><span class="comment"># 每个图像特征 28*28 像素</span></span><br><span class="line"><span class="comment"># 原数据地址 http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html</span></span><br><span class="line"><span class="comment"># 导入训练集合，用的是notMNIST</span></span><br><span class="line">pickle_file = <span class="string">'notMNIST.pickle'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(pickle_file, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">  save = pickle.load(f)</span><br><span class="line">  <span class="comment"># 之前按键值对存放的，现在按键值对取出来</span></span><br><span class="line">  train_dataset = save[<span class="string">'train_dataset'</span>]</span><br><span class="line">  train_labels = save[<span class="string">'train_labels'</span>]</span><br><span class="line">  valid_dataset = save[<span class="string">'valid_dataset'</span>]</span><br><span class="line">  valid_labels = save[<span class="string">'valid_labels'</span>]</span><br><span class="line">  test_dataset = save[<span class="string">'test_dataset'</span>]</span><br><span class="line">  test_labels = save[<span class="string">'test_labels'</span>]</span><br><span class="line">  <span class="keyword">del</span> save  <span class="comment"># hint to help gc free up memory</span></span><br><span class="line">  print(<span class="string">'Training set'</span>, train_dataset.shape, train_labels.shape)</span><br><span class="line">  print(<span class="string">'Validation set'</span>, valid_dataset.shape, valid_labels.shape)</span><br><span class="line">  print(<span class="string">'Test set'</span>, test_dataset.shape, test_labels.shape)</span><br></pre></td></tr></table></figure>
<pre><code>Training set (20000, 28, 28) (20000,)
Validation set (1000, 28, 28) (1000,)
Test set (1000, 28, 28) (1000,)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把3维的训练数据转化为2维，即把28*28的图像像素矩阵转化成一维向量的转置 </span></span><br><span class="line"><span class="comment"># 把标签转化为one-hot 向量</span></span><br><span class="line">image_size = <span class="number">28</span></span><br><span class="line">num_labels = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reformat</span><span class="params">(dataset, labels)</span>:</span></span><br><span class="line">  dataset = dataset.reshape((<span class="number">-1</span>, image_size * image_size)).astype(np.float32)</span><br><span class="line">  <span class="comment"># Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]</span></span><br><span class="line">  labels = (np.arange(num_labels) == labels[:,<span class="keyword">None</span>]).astype(np.float32)</span><br><span class="line">  <span class="keyword">return</span> dataset, labels</span><br><span class="line">train_dataset, train_labels = reformat(train_dataset, train_labels)</span><br><span class="line">valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)</span><br><span class="line">test_dataset, test_labels = reformat(test_dataset, test_labels)</span><br><span class="line">print(<span class="string">'Training set'</span>, train_dataset.shape, train_labels.shape)</span><br><span class="line">print(<span class="string">'Validation set'</span>, valid_dataset.shape, valid_labels.shape)</span><br><span class="line">print(<span class="string">'Test set'</span>, test_dataset.shape, test_labels.shape)</span><br></pre></td></tr></table></figure>
<pre><code>Training set (20000, 784) (20000, 10)
Validation set (1000, 784) (1000, 10)
Test set (1000, 784) (1000, 10)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先我们用梯度下降来训练多项式回归模型</span></span><br><span class="line"><span class="comment"># 使用梯度下降的话，数目不能太多，不然计算很耗时</span></span><br><span class="line"><span class="comment"># 我们这里用10000的数据子集</span></span><br><span class="line">train_subset = <span class="number">10000</span></span><br><span class="line"></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 输入数据，常量</span></span><br><span class="line">  <span class="comment"># 这是图的输入节点</span></span><br><span class="line">  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])</span><br><span class="line">  tf_train_labels = tf.constant(train_labels[:train_subset])</span><br><span class="line">  tf_valid_dataset = tf.constant(valid_dataset)</span><br><span class="line">  tf_test_dataset = tf.constant(test_dataset)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Variables.变量</span></span><br><span class="line">  <span class="comment"># 变量是我们模型训练中需要改变的量：权重和偏移</span></span><br><span class="line">  <span class="comment"># 权重使用truncated进行初始化，并且归一化参数，这个是为了防止梯度爆炸和消失</span></span><br><span class="line">  <span class="comment"># 偏移全部初始化为0</span></span><br><span class="line">  weights = tf.Variable(</span><br><span class="line">    tf.truncated_normal([image_size * image_size, num_labels]))</span><br><span class="line">  biases = tf.Variable(tf.zeros([num_labels]))</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 训练模型</span></span><br><span class="line">  <span class="comment"># 模型很简单，特征 * 权重 + 偏移</span></span><br><span class="line">  <span class="comment"># 使用softmax 以及交叉熵来计算损失函数</span></span><br><span class="line">  logits = tf.matmul(tf_train_dataset, weights) + biases</span><br><span class="line">  loss = tf.reduce_mean(</span><br><span class="line">    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 优化器</span></span><br><span class="line">  <span class="comment"># GD 梯度下降.</span></span><br><span class="line">  optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(loss)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 用softmax 预测结果</span></span><br><span class="line">  <span class="comment"># 计算出验证集和测试集的预测结果</span></span><br><span class="line">  train_prediction = tf.nn.softmax(logits)</span><br><span class="line">  valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)</span><br><span class="line">  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line">localtime = time.asctime( time.localtime(time.time()) )</span><br><span class="line">print(localtime)</span><br></pre></td></tr></table></figure>
<pre><code>Fri Aug 31 16:01:56 2018
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 接下来让我们迭代运行</span></span><br><span class="line">num_steps = <span class="number">801</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(predictions, labels)</span>:</span></span><br><span class="line">  <span class="comment"># argmax  axis = 0的时候返回每一列最大值的位置索引</span></span><br><span class="line">  <span class="comment"># axis = 1的时候返回每一行最大值的位置索引</span></span><br><span class="line">  <span class="comment"># axis = 2、3、4...，即为多维张量时，同理推断</span></span><br><span class="line">  <span class="comment"># 得到true，fasle 的向量，然后统计1的个数，就是正确率</span></span><br><span class="line">  <span class="keyword">return</span> (<span class="number">100.0</span> * np.sum(np.argmax(predictions, <span class="number">1</span>) == np.argmax(labels, <span class="number">1</span>))</span><br><span class="line">          / predictions.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> session:</span><br><span class="line">  <span class="comment"># 一次性的初始化操作，把graph 中的所有变量一次性初始化好</span></span><br><span class="line">  tf.global_variables_initializer().run()</span><br><span class="line">  print(<span class="string">'Initialized'</span>)</span><br><span class="line">  starttime = time.clock()</span><br><span class="line">  print(<span class="string">'start time:'</span>,time.asctime( time.localtime(time.time()) ))</span><br><span class="line">  <span class="keyword">for</span> step <span class="keyword">in</span> range(num_steps):</span><br><span class="line">    <span class="comment"># 通过session.run 计算 optimizer，loss,以及 预测结果</span></span><br><span class="line">    _, l, predictions = session.run([optimizer, loss, train_prediction])</span><br><span class="line">    <span class="comment"># 每一百步输入损失函数，</span></span><br><span class="line">    <span class="keyword">if</span> (step % <span class="number">100</span> == <span class="number">0</span>):</span><br><span class="line">      print(<span class="string">'Loss at step %d: %f'</span> % (step, l))</span><br><span class="line">      print(<span class="string">'Training accuracy: %.1f%%'</span> % accuracy(</span><br><span class="line">        predictions, train_labels[:train_subset])) <span class="comment"># 这里不能使用tensor 因为accuracy里面调用的是numpy；numpy使用的是np的ndarray</span></span><br><span class="line">      <span class="comment">#在验证集合上调用.eval() 等价于 session.run()方法，但是返回值是one-hot 的预测结果，它有图的所有信息</span></span><br><span class="line">      print(<span class="string">'Validation accuracy: %.1f%%'</span> % accuracy(</span><br><span class="line">        valid_prediction.eval(), valid_labels))</span><br><span class="line">  print(<span class="string">'Test accuracy: %.1f%%'</span> % accuracy(test_prediction.eval(), test_labels))</span><br><span class="line">  print(<span class="string">'end time:'</span>,time.asctime( time.localtime(time.time()) ))</span><br><span class="line">  elapsed = (time.clock() - starttime)</span><br><span class="line">  print(<span class="string">"Time used:"</span>,elapsed)</span><br></pre></td></tr></table></figure>
<pre><code>Initialized
start time: Fri Aug 31 16:09:36 2018
Loss at step 0: 17.316351
Training accuracy: 9.8%
Validation accuracy: 13.1%
Loss at step 100: 2.382646
Training accuracy: 72.3%
Validation accuracy: 73.9%
Loss at step 200: 1.911187
Training accuracy: 74.5%
Validation accuracy: 75.0%
Loss at step 300: 1.647586
Training accuracy: 75.9%
Validation accuracy: 75.3%
Loss at step 400: 1.472351
Training accuracy: 76.6%
Validation accuracy: 75.6%
Loss at step 500: 1.344603
Training accuracy: 77.3%
Validation accuracy: 75.5%
Loss at step 600: 1.245956
Training accuracy: 77.9%
Validation accuracy: 76.1%
Loss at step 700: 1.166571
Training accuracy: 78.6%
Validation accuracy: 76.0%
Loss at step 800: 1.100793
Training accuracy: 78.9%
Validation accuracy: 75.9%
Test accuracy: 81.9%
end time: Fri Aug 31 16:10:02 2018
Time used: 26.420718226680037
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下面训练随机梯度下降</span></span><br><span class="line"><span class="comment"># 将数据保存到常量节点，创建一个占位节点,每次用数据代入占位节点</span></span><br><span class="line"><span class="comment"># 图的初始化操作</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 输入数据，对于训练数据，我们使用一个占位符</span></span><br><span class="line">  <span class="comment"># 在运行时，用一个批次</span></span><br><span class="line">  tf_train_dataset = tf.placeholder(tf.float32,</span><br><span class="line">                                    shape=(batch_size, image_size * image_size))</span><br><span class="line">  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))</span><br><span class="line">  tf_valid_dataset = tf.constant(valid_dataset)</span><br><span class="line">  tf_test_dataset = tf.constant(test_dataset)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 初始化变量</span></span><br><span class="line">  weights = tf.Variable(</span><br><span class="line">    tf.truncated_normal([image_size * image_size, num_labels]))</span><br><span class="line">  biases = tf.Variable(tf.zeros([num_labels]))</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 训练计算</span></span><br><span class="line">  logits = tf.matmul(tf_train_dataset, weights) + biases</span><br><span class="line">  loss = tf.reduce_mean(</span><br><span class="line">    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 优化器还是</span></span><br><span class="line">  optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(loss)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 计算预测值</span></span><br><span class="line">  train_prediction = tf.nn.softmax(logits)</span><br><span class="line">  valid_prediction = tf.nn.softmax(</span><br><span class="line">    tf.matmul(tf_valid_dataset, weights) + biases)</span><br><span class="line">  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">num_steps = <span class="number">3001</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> session:</span><br><span class="line">  tf.global_variables_initializer().run()</span><br><span class="line">  print(<span class="string">"Initialized"</span>)</span><br><span class="line">  starttime = time.clock()</span><br><span class="line">  print(<span class="string">'start time:'</span>,time.asctime( time.localtime(time.time()) ))</span><br><span class="line">  <span class="keyword">for</span> step <span class="keyword">in</span> range(num_steps):</span><br><span class="line">    <span class="comment"># 用偏移量来选择数据，10一批次， 101个数据，当取到12批次，offset = 110%101 = 9,可以保证批次有效</span></span><br><span class="line">    <span class="comment"># Note:我们可以用更好的随机策略选择批次</span></span><br><span class="line">    offset = (step * batch_size) % (train_labels.shape[<span class="number">0</span>] - batch_size)</span><br><span class="line">    <span class="comment"># 创建一个批次.</span></span><br><span class="line">    batch_data = train_dataset[offset:(offset + batch_size), :]</span><br><span class="line">    batch_labels = train_labels[offset:(offset + batch_size), :]</span><br><span class="line">    <span class="comment"># 准备一个字典，来存放批数据，以便于 placeholder 来放数据</span></span><br><span class="line">    <span class="comment"># 数据类型 numpy 的数组</span></span><br><span class="line">    feed_dict = &#123;tf_train_dataset : batch_data, tf_train_labels : batch_labels&#125;</span><br><span class="line">    _, l, predictions = session.run(</span><br><span class="line">      [optimizer, loss, train_prediction], feed_dict=feed_dict)</span><br><span class="line">    <span class="keyword">if</span> (step % <span class="number">500</span> == <span class="number">0</span>):</span><br><span class="line">      print(<span class="string">"Minibatch loss at step %d: %f"</span> % (step, l))</span><br><span class="line">      print(<span class="string">"Minibatch accuracy: %.1f%%"</span> % accuracy(predictions, batch_labels))</span><br><span class="line">      print(<span class="string">"Validation accuracy: %.1f%%"</span> % accuracy(</span><br><span class="line">        valid_prediction.eval(), valid_labels))</span><br><span class="line">  print(<span class="string">"Test accuracy: %.1f%%"</span> % accuracy(test_prediction.eval(), test_labels))</span><br><span class="line">  print(<span class="string">'end time:'</span>,time.asctime( time.localtime(time.time()) ))</span><br><span class="line">  elapsed = (time.clock() - starttime)</span><br><span class="line">  print(<span class="string">"Time used:"</span>,elapsed)</span><br></pre></td></tr></table></figure>
<pre><code>Initialized
start time: Fri Aug 31 18:31:58 2018
Minibatch loss at step 0: 17.747850
Minibatch accuracy: 14.1%
Validation accuracy: 14.1%
Minibatch loss at step 500: 1.452345
Minibatch accuracy: 76.6%
Validation accuracy: 76.6%
Minibatch loss at step 1000: 1.574866
Minibatch accuracy: 76.6%
Validation accuracy: 77.6%
Minibatch loss at step 1500: 0.657050
Minibatch accuracy: 84.4%
Validation accuracy: 77.0%
Minibatch loss at step 2000: 0.710017
Minibatch accuracy: 83.6%
Validation accuracy: 78.5%
Minibatch loss at step 2500: 1.029955
Minibatch accuracy: 77.3%
Validation accuracy: 78.4%
Minibatch loss at step 3000: 0.693648
Minibatch accuracy: 84.4%
Validation accuracy: 78.4%
Test accuracy: 84.5%
end time: Fri Aug 31 18:32:01 2018
Time used: 2.942584584146971
</code></pre>
    </div>

    <div class="post-footer">
        <div>
            
                转载声明：商业转载请联系作者获得授权,非商业转载请注明出处 © <a href="" target="_blank">sudl</a>
            
        </div>
        <div>
            
        </div>
    </div>
</article>

<div class="article-nav prev-next-wrap clearfix">
    
        <a href="/2018/04/03/1.多层神经网络TensorFlow ReLUs/" class="pre-post btn btn-default" title='多层神经网络'>
            <i class="fa fa-angle-left fa-fw"></i><span class="hidden-lg">上一篇</span>
            <span class="hidden-xs">多层神经网络</span>
        </a>
    
    
        <a href="/2018/04/02/20 型性能评估/" class="next-post btn btn-default" title='模型的性能评估'>
            <span class="hidden-lg">下一篇</span>
            <span class="hidden-xs">模型的性能评估</span><i class="fa fa-angle-right fa-fw"></i>
        </a>
    
</div>


    <div id="comments">
        
	
<div id="lv-container" data-id="city" data-uid="MTAyMC8zMzA1MS85NjEz">
  <script type="text/javascript">
     (function(d, s) {
         var j, e = d.getElementsByTagName(s)[0];
         if (typeof LivereTower === 'function') { return; }
         j = d.createElement(s);
         j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
         j.async = true;
         e.parentNode.insertBefore(j, e);
     })(document, 'script');
  </script>
</div>


    </div>





                </main>
                
                    <aside id="article-toc" role="navigation" class="col-md-4">
    <div class="widget">
        <h3 class="title">文章目录</h3>
        
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#要点"><span class="toc-text">要点</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#使用梯度下降和随机梯度下降训练一个全连接网络"><span class="toc-text">使用梯度下降和随机梯度下降训练一个全连接网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#用到的TensorFlow函数"><span class="toc-text">用到的TensorFlow函数:</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#代码"><span class="toc-text">代码</span></a></li></ol>
        
    </div>
</aside>

                
            </div>
        </div>
    </section>
    <footer class="main-footer">
    <div class="container">
        <div class="row">
        </div>
    </div>
</footer>

<a id="back-to-top" class="icon-btn hide">
	<i class="fa fa-chevron-up"></i>
</a>




    <div class="copyright">
    <div class="container">
        <div class="row">
            <div class="col-sm-12"> 
                <span>Copyright &copy; 2017
                </span> | 
                <span>
                    Powered by <a href="//hexo.io" class="copyright-links" target="_blank" rel="nofollow">Hexo</a>
                </span> | 
                <span>
                    Theme by <a href="//github.com/shenliyang/hexo-theme-snippet.git" class="copyright-links" target="_blank" rel="nofollow">Snippet</a>
                </span>
            </div>
        </div>
    </div>
</div>



<script src="/js/app.js?rev=@@hash"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</body>
</html>