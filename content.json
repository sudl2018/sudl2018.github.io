{"meta":{"title":"sudoli's blog","subtitle":null,"description":null,"author":"sudoli","url":"http://yoursite.com"},"pages":[],"posts":[{"title":"904. 水果成篮","slug":"334.904. 水果成篮","date":"2018-09-16T06:06:32.000Z","updated":"2018-09-16T05:56:25.769Z","comments":true,"path":"2018/09/16/334.904. 水果成篮/","link":"","permalink":"http://yoursite.com/2018/09/16/334.904. 水果成篮/","excerpt":"","text":"问题904. 水果成篮在一排树中，第 i 棵树产生 tree[i] 型的水果。你可以从你选择的任何树开始，然后重复执行以下步骤： 把这棵树上的水果放进你的篮子里。如果你做不到，就停下来。 移动到当前树右侧的下一棵树。如果右边没有树，就停下来。 请注意，在选择一颗树后，你没有任何选择：你必须执行步骤 1，然后执行步骤 2，然后返回步骤 1，然后执行步骤 2，依此类推，直至停止。 你有两个篮子，每个篮子可以携带任何数量的水果，但你希望每个篮子只携带一种类型的水果。用这个程序你能收集的水果总量是多少？ 示例 1： 123输入：[1,2,1]输出：3解释：我们可以收集 [1,2,1]。 示例 2： 1234输入：[0,1,2,2]输出：3解释：我们可以收集 [1,2,2].如果我们从第一棵树开始，我们将只能收集到 [0, 1]。 示例 3： 1234输入：[1,2,3,2,2]输出：4解释：我们可以收集 [2,3,2,2].如果我们从第一棵树开始，我们将只能收集到 [1, 2]。 示例 4： 1234输入：[3,3,3,1,2,1,1,2,3,3,4]输出：5解释：我们可以收集 [1,2,1,1,2].如果我们从第一棵树或第八棵树开始，我们将只能收集到 3 个水果。 提示： 1 &lt;= tree.length &lt;= 40000 0 &lt;= tree[i] &lt; tree.length 思路 题目的意思就是，序列中只能有两种元素，求最长连续序列 那就是只要记录下与当前元素前面的最近的不同元素以及这个不同元素的连续个数就可以了 遍历的时候一边记录序列长度，一边记录两种元素的连续个数，当遇到新元素时候，map清空后，加入新元素 代码123456789101112131415161718192021222324252627282930313233343536373839class Solution &#123; public int totalFruit(int[] tree) &#123; //记录总的结果 int res = 0; //记录新的两种元素组成的序列长度 int temp = 0; //记录两种元素，以及各自连续个数 Map&lt;Integer,Integer&gt; map = new HashMap&lt;&gt;(); for(int i=0;i&lt;tree.length;i++)&#123; if(map.containsKey(tree[i]))&#123; temp ++;// 前提map中有tree[i],所以i不是0 if(tree[i]==tree[i-1])&#123; map.put(tree[i],map.get(tree[i])+1); &#125;else&#123; map.put(tree[i],1); &#125; &#125;else&#123; if(map.size() &lt;2)&#123; temp++; map.put(tree[i],1); res = Math.max(res,temp); &#125;else&#123; res = Math.max(res,temp);//把tree[i]的前一个元素的个数拿出来，即计算从tree[i]的前一个元素开始计数 temp = map.get(tree[i-1]); //我们有下一个新元素，以及【i-1】元素，所以不需要知道map中的另一个元素，直接清空，加入新元素 map.clear(); map.put(tree[i-1],temp); map.put(tree[i],1); temp ++; &#125; &#125; res = Math.max(res,temp); &#125; return res; &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"},{"name":"两端遍历","slug":"算法/两端遍历","permalink":"http://yoursite.com/categories/算法/两端遍历/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/tags/算法/"},{"name":"medium","slug":"medium","permalink":"http://yoursite.com/tags/medium/"},{"name":"两端遍历","slug":"两端遍历","permalink":"http://yoursite.com/tags/两端遍历/"}],"keywords":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"},{"name":"两端遍历","slug":"算法/两端遍历","permalink":"http://yoursite.com/categories/算法/两端遍历/"}]},{"title":"907. 子数组的最小值之和","slug":"332.907. 子数组的最小值之和","date":"2018-09-16T06:06:32.000Z","updated":"2018-09-16T05:41:29.339Z","comments":true,"path":"2018/09/16/332.907. 子数组的最小值之和/","link":"","permalink":"http://yoursite.com/2018/09/16/332.907. 子数组的最小值之和/","excerpt":"","text":"问题907. 子数组的最小值之和给定一个整数数组 A，找到 min(B) 的总和，其中 B 的范围为 A 的每个（连续）子数组。 由于答案可能很大，因此返回答案模 10^9 + 7。 示例 1： 12345输入：[3,1,2,4]输出：17解释：子数组为 [3]，[1]，[2]，[4]，[3,1]，[1,2]，[2,4]，[3,1,2]，[1,2,4]，[3,1,2,4]。 最小值为 3，1，2，4，1，1，2，1，1，1，和为 17。 提示： 1 &lt;= A &lt;= 30000 1 &lt;= A[i] &lt;= 30000 思路一 思路一：计算每个元素作为起始元素的所有可能性 暴利遍历——双重遍历； 假设：result(i,j) = min(A[i]，A[i+i],...A[j]) \\\\ 那么：result(i,j+1) = min(result(i,j) , A[j+1])1但是超时了 时间复杂度 O（N^2） 【N是数组长度】 代码一123456789101112131415class Solution &#123; public int sumSubarrayMins(int[] A) &#123;// 思路一，就是把起始元素的所以最小可能找出来 long res = 0; for(int i=0;i&lt;A.length;i++)&#123; int min = Integer.MAX_VALUE; for(int j=i;j&lt;A.length;j++)&#123; min = Math.min(min,A[j]); res =res + min; &#125; &#125; return (int)(res%(1e9 + 7)); &#125;&#125; 思路二 在思路一的基础上改变一点：计算当前元素作为结束元素的所有序列数，用《最小值，序列个数》的map表示，目的减少重复遍历 在计算i时，只保留前面i-1个元素的所有res[i-1]情况，即 res[i-1] = \\{B[0][i-1] ,B[1][i-1],B[2][i-1]...B[i-1][i-1]\\} 把这些放入hashmap中就是&lt;最小值，序列个数&gt;，但是还是超时了，在96/100个测试案例的时候 代码二12345678910111213141516171819202122232425262728293031323334353637383940class Solution &#123; public int sumSubarrayMins(int[] A) &#123;// 思路二,采用hashmap ,把前面的最小值的集合用《最小值，序列个数》保存起来，减少重复遍历 long res = 0; // 存放结果 // Set&lt;Integer&gt; res = new HashSet&lt;&gt;(); // 存放i个元素的前面的或的可能 Map&lt;Integer,Integer&gt; cur ; // 存放i-1个元素的前面的或的可能 Map&lt;Integer,Integer&gt; pre = new HashMap&lt;&gt;(); for(Integer i:A)&#123; // cur要和前面没有任何联系，所有每次都要new cur = new HashMap&lt;&gt;(); // 当前的这个元素一定要加入进去，B 可以只包含i cur.put(i,1); for(Integer j:pre.keySet())&#123; // 在i-1的所有可能的基础上计算i,放入cur中 // if(Math.min(i,j)==i)&#123; // cur.put(i,cur.get(i)+pre.get(j)); // &#125;else&#123; // cur.put(j,pre.get(j)); // &#125; cur.put(Math.min(i,j),cur.getOrDefault(Math.min(i,j),0)+pre.get(j)); &#125; // 把cur的所有可能放入结果中 for(Integer temp:cur.keySet())&#123; // System.out.println(temp+\"num\"+cur.get(temp)); res += temp*cur.get(temp); &#125; // System.out.println(\"===========================\"); // 下一次计算时，cur变成了pre pre = cur; &#125; return (int)(res%(1e9 + 7)); &#125; &#125; 分析 hashset 减少了多少重复值呢？ 123456例如：[3,1,2,4]以3结尾：[3] ==&gt; &lt;3,1&gt;以1结尾：[1] [3,1] ==&gt;&lt;1,2&gt;, 以2结尾:[2][1,2] [3,1,2] ==&gt; &lt;2,1&gt; &lt;1,2&gt;以4结尾:[4][2,4][1,2,4][3,1,2,4] ==&gt; &lt;4,1&gt; &lt;2,1&gt; &lt;1,2&gt; 最差情况，就是顺序排序的数组， O（N*N）思路三以[3 1 2 4 ]为例： 左边比当前元素A【i】大的序列长度，包含A【i】==&gt; 【1,2,1,1】 ===》这个就是能以当前A【i】作为最小元素且是结束元素的序列长度； 右边比当前元素A【i】大的序列长度，包含A【i】==&gt; 【1,3,2,1】===》 这个就是能以当前A【i】作为最小元素且是开始元素的序列长度； 把左右两个序列组合起来【1,2,1,1】 * 【1,3,2,1】===》【1,6,2,1】就是所有能以A【i】作为最小元素的序列个数啦！！！ 所以只要求出左边比A【i】大的元素个数以及右边比A【i】大的元素个数就可以了 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class Solution &#123; public int sumSubarrayMins(int[] A) &#123; long res = 0; int [] left = new int[A.length]; int [] right = new int[A.length]; //暴力方法，就是每个元素向左遍历，遇到比当前元素小的就返回//这里我们用个stack ,里面都是从小到大排列的元素，每个元素都记录着前面比他大的元素个数 Stack&lt;int[]&gt; s1 = new Stack&lt;&gt;(); Stack&lt;int[]&gt; s2 = new Stack&lt;&gt;(); for(int i=0;i&lt;A.length;i++)&#123;//元素本身就是长度1 int temp = 1;//把所有比当前元素大的都pop 出去，这里特别特别要注意的是，左右两边不能有集合重复，比如 3,4 5 3，计算最后一个元素3的左边的序列是 【3,4,5,3】，计算第一个3右边的序列时，也是【3,4,5,3】，那么就会重复，所以左右两边只有一个地方可以 &lt;= A[i]&lt;s1.peek()[0] while(!s1.isEmpty() &amp;&amp; A[i]&lt;=s1.peek()[0])&#123; temp += s1.pop()[1]; &#125;//当前元素是stack 中最上面的元素，记录着所有比当前元素大的个数，而且当前元素比前一个元素大， s1.push(new int[]&#123;A[i],temp&#125;); left[i] = temp; // System.out.println(temp); &#125; // System.out.println(\"======================\"); for(int i=A.length-1;i&gt;=0;i--)&#123;//元素本身就是长度1 int temp = 1;//把所有比当前元素大的都pop 出去， while(!s2.isEmpty() &amp;&amp; A[i]&lt;s2.peek()[0])&#123; temp += s2.pop()[1]; &#125;//当前元素是stack 中最上面的元素，记录着所有比当前元素大的个数，而且当前元素比前一个元素大， s2.push(new int[]&#123;A[i],temp&#125;); right[i] = temp; // System.out.println(temp); &#125;// 计算结果啦 for(int i=0;i&lt;A.length;i++)&#123; res += left[i]*right[i]*A[i]; &#125; return (int)(res%(1e9 + 7)); &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"},{"name":"stack","slug":"算法/stack","permalink":"http://yoursite.com/categories/算法/stack/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/tags/算法/"},{"name":"Medium","slug":"Medium","permalink":"http://yoursite.com/tags/Medium/"},{"name":"子数组","slug":"子数组","permalink":"http://yoursite.com/tags/子数组/"},{"name":"stack","slug":"stack","permalink":"http://yoursite.com/tags/stack/"}],"keywords":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"},{"name":"stack","slug":"算法/stack","permalink":"http://yoursite.com/categories/算法/stack/"}]},{"title":"899.有序队列","slug":"330.899. 有序队列","date":"2018-09-02T06:06:32.000Z","updated":"2018-09-03T00:34:15.499Z","comments":true,"path":"2018/09/02/330.899. 有序队列/","link":"","permalink":"http://yoursite.com/2018/09/02/330.899. 有序队列/","excerpt":"","text":"问题899. 有序队列给出了一个由小写字母组成的字符串 S。然后，我们可以进行任意次数的移动。 在每次移动中，我们选择前 K 个字母中的一个（从左侧开始），将其从原位置移除，并放置在字符串的末尾。 返回我们在任意次数的移动之后可以拥有的按字典顺序排列的最小字符串。 示例 1： 12345输入：S = &quot;cba&quot;, K = 1输出：&quot;acb&quot;解释：在第一步中，我们将第一个字符（“c”）移动到最后，获得字符串 “bac”。在第二步中，我们将第一个字符（“b”）移动到最后，获得最终结果 “acb”。 示例 2： 12345输入：S = &quot;baaca&quot;, K = 3输出：&quot;aaabc&quot;解释：在第一步中，我们将第一个字符（“b”）移动到最后，获得字符串 “aacab”。在第二步中，我们将第三个字符（“c”）移动到最后，获得最终结果 “aaabc”。 提示： 1 &lt;= K &lt;= S.length &lt;= 1000 S 只由小写字母组成。 思路 这题其实很简单，注重分析；当K 等于2的情况，就是任何字符可以移动到任何字符前后，所以这个就意味着都可以排成最小字典顺序 k==1的时候，用Java的compareTo(String anotherString)按字典顺序比较两个字符串。如果String对象按字典顺序排列在参数字符串之前，结果为负整数 代码12345678910111213141516171819202122232425262728class Solution &#123; public String orderlyQueue(String S, int K) &#123; //这题其实很简单，注重分析 //当K 等于2的情况，就是任何字符可以移动到任何字符前后，所以这个就意味着都可以排成最小字典顺序 if(K&gt;1)&#123; // 转化为char数组 char[] s = S.toCharArray(); Arrays.sort(s); return new String(s); &#125; // 关键是K=1的情况 // acbca 与 accba 的字典顺序比较 // compareTo(String anotherString)按字典顺序比较两个字符串。如果String对象按字典顺序排列在参数字符串之前，结果为负整数 String res = S; for(int i=1;i&lt;S.length();i++)&#123; String sub_r = S.substring(i); String sub_l = S.substring(0,i); String tmp = sub_r+sub_l; if(res.compareTo(tmp)&gt;0)&#123; res = tmp; &#125; &#125; return res; &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"},{"name":"String","slug":"算法/String","permalink":"http://yoursite.com/categories/算法/String/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/tags/算法/"},{"name":"hard","slug":"hard","permalink":"http://yoursite.com/tags/hard/"},{"name":"String","slug":"String","permalink":"http://yoursite.com/tags/String/"},{"name":"Math","slug":"Math","permalink":"http://yoursite.com/tags/Math/"}],"keywords":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"},{"name":"String","slug":"算法/String","permalink":"http://yoursite.com/categories/算法/String/"}]},{"title":"898.子数组按位或操作","slug":"331 898.子数组按位或操作","date":"2018-09-02T06:06:32.000Z","updated":"2018-09-03T01:04:12.885Z","comments":true,"path":"2018/09/02/331 898.子数组按位或操作/","link":"","permalink":"http://yoursite.com/2018/09/02/331 898.子数组按位或操作/","excerpt":"","text":"问题898. 子数组按位或操作我们有一个非负整数数组 A。 对于每个（连续的）子数组 B = [A[i], A[i+1], ..., A[j]] （ i &lt;= j），我们对 B 中的每个元素进行按位或操作，获得结果 A[i] | A[i+1] | ... | A[j]。 返回可能结果的数量。 （多次出现的结果在最终答案中仅计算一次。） 示例 1： 1234输入：[0]输出：1解释：只有一个可能的结果 0 。 示例 2： 123456输入：[1,1,2]输出：3解释：可能的子数组为 [1]，[1]，[2]，[1, 1]，[1, 2]，[1, 1, 2]。产生的结果为 1，1，2，1，3，3 。有三个唯一值，所以答案是 3 。 示例 3： 1234输入：[1,2,4]输出：6解释：可能的结果是 1，2，3，4，6，以及 7 。 提示： 1 &lt;= A.length &lt;= 50000 0 &lt;= A[i] &lt;= 10^9 思路一 思路一：计算每个元素作为起始元素的所有可能性 暴利遍历——双重遍历； 假设：result(i,j) = A[i]|A[i+i]|...A[j] \\\\ 那么：result(i,j+1) = result(i,j) | A[j+1]但是超时了 时间复杂度 O（N^2） 【N是数组长度】 代码一12345678910111213141516171819class Solution &#123;// 思路一： // 双重遍历： // 把result(i,j) = A[i]|A[i+i]|...A[j] // 那么 result(i,j+1) = result(i,j) | A[j+1] // 但是超时了 public int subarrayBitwiseORs(int[] A) &#123; Set&lt;Integer&gt; res = new HashSet&lt;&gt;(); for(int i=0;i&lt;A.length;i++)&#123; int temp_res = A[i]; for(int j =i;j&lt;A.length;j++)&#123; temp_res = temp_res|A[j]; res.add(temp_res); &#125; &#125; return res.size(); &#125;&#125; 思路二 在思路一的基础上改变一点：计算每个元素作为结束元素的所有可能性，可以减少大量重复 在计算i时，只保留前面i-1个元素的所有res[i-1]情况，即 res[i-1] = \\{B[0][i-1] ,B[1][i-1],B[2][i-1]...B[i-1][i-1]\\} 把这些放入hashset 中，减少重复值，大量减少重复计算 代码二1234567891011121314151617181920212223242526class Solution &#123; public int subarrayBitwiseORs(int[] A) &#123; // 存放结果 Set&lt;Integer&gt; res = new HashSet&lt;&gt;(); // 存放i个元素的前面的或的可能 Set&lt;Integer&gt; cur ; // 存放i-1个元素的前面的或的可能 Set&lt;Integer&gt; pre = new HashSet&lt;&gt;(); for(Integer i:A)&#123; // cur要和前面没有任何联系，所有每次都要new cur = new HashSet&lt;&gt;(); // 当前的这个元素一定要加入进去，B 可以只包含i cur.add(i); for(Integer j:pre)&#123; // 在i-1的所有可能的基础上计算i,放入cur中 cur.add(i|j); &#125; // 把cur的所有可能放入结果中 res.addAll(cur); // 下一次计算时，cur变成了pre pre = cur; &#125; return res.size(); &#125;&#125; 分析 hashset 减少了多少重复值呢？ 12345678910111213例如：[4, 5, 10, 2, 16, 4, 5, 2, 1, 3]例举以3结尾的可能[00011] ==&gt; 00011[00001, 00011] ==&gt; 00011[00010, 00001, 00011] ==&gt; 00011[00101, 00010, 00001, 00011] ==&gt; 00111[00100, 00101, 00010, 00001, 00011] ==&gt; 00111[10000, 00100, 00101, 00010, 00001, 00011] ==&gt; 10111[00010, 10000, 00100, 00101, 00010, 00001, 00011] ==&gt; 10111[01010, 00010, 10000, 00100, 00101, 00010, 00001, 00011] ==&gt; 11111[00101, 01010, 00010, 10000, 00100, 00101, 00010, 00001, 00011] ==&gt; 11111[00100, 00101, 01010, 00010, 10000, 00100, 00101, 00010, 00001, 00011] ==&gt; 11111 瞧一瞧以3结尾的有很多重复值吧，但是注意发现产生新值的时候就是增加了二进制1，才会产生新值，为啥呢，因为是或操作啊，所以新值就是在旧值得基础上把0变成1； 总共有32位，每一个新值至少要增加一个1 ，否则就重复了，那么最多不就是32个数么 所以时间复杂度为 O（32*N）","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"},{"name":"Math","slug":"算法/Math","permalink":"http://yoursite.com/categories/算法/Math/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/tags/算法/"},{"name":"Medium","slug":"Medium","permalink":"http://yoursite.com/tags/Medium/"},{"name":"Math","slug":"Math","permalink":"http://yoursite.com/tags/Math/"},{"name":"位运算","slug":"位运算","permalink":"http://yoursite.com/tags/位运算/"}],"keywords":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"},{"name":"Math","slug":"算法/Math","permalink":"http://yoursite.com/categories/算法/Math/"}]},{"title":"所有可能的完整二叉树","slug":"329.找出所有可能的完整二叉树","date":"2018-08-26T06:06:32.000Z","updated":"2018-08-31T00:42:57.571Z","comments":true,"path":"2018/08/26/329.找出所有可能的完整二叉树/","link":"","permalink":"http://yoursite.com/2018/08/26/329.找出所有可能的完整二叉树/","excerpt":"","text":"问题找出所有可能的完整二叉树完整二叉树是一类二叉树，其中每个结点恰好有 0 或 2 个子结点。 返回包含 N 个结点的所有可能完整二叉树的列表。 答案的每个元素都是一个可能树的根结点。 答案中每个树的每个结点都必须有 node.val=0。 你可以按任何顺序返回树的最终列表。 注意完整二叉树不是完全二叉树 思路一（深度优先搜索尝试所有可能节点，深拷贝二叉树） 找出所有的树，那必须有很多根节点，返回是个根节点的list 我不知道结果有多少颗树，所以不能一下子列出所有根节点，所以我首先考虑： 从根节点构造树的所有合理可能 dfs——从根节点开始，每次递归 都是把左右子节点放入树中的某个节点，所有可能节点都要尝试（for 循环），但是使用for 循环要考虑重复访问的问题 把每一种可能进行深拷贝后添加到结果中 代码一12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; List&lt;TreeNode&gt; res = new LinkedList&lt;&gt;(); int N = 0; public List&lt;TreeNode&gt; allPossibleFBT(int N) &#123; if(N%2==0)&#123; return res; &#125; this.N = N; TreeNode root = new TreeNode(0); LinkedList&lt;TreeNode&gt; nodes = new LinkedList&lt;&gt;(); nodes.add(root); helper(1,nodes,root); return res; &#125; public void helper(int count,LinkedList&lt;TreeNode&gt; nodes,TreeNode root)&#123; if(count == N)&#123; if(root != null)&#123; // 要用拷贝，因为原来root 会被我不停的改变 TreeNode new_root = new TreeNode(0); copyTree(root,new_root); res.add(new_root); &#125; return; &#125; if(count &gt; N)&#123; return; &#125; //这里一定要用poll ,因为链表中的元素已经被使用过的节点，如果再次被访问就会出现重复访问 //如果节点一旦被访问过，那就要移除 while(nodes.size()!=0)&#123; TreeNode node = nodes.poll(); node.left = new TreeNode(0); node.right = new TreeNode(0); // 将剩余没有访问的节点都放入list中，以便于下次访问 LinkedList&lt;TreeNode&gt; nodelist = new LinkedList&lt;&gt;(nodes); nodelist.add(node.left); nodelist.add(node.right); helper(count+2,nodelist,root); node.left = null; node.right = null; &#125; &#125; //深拷贝一颗树 public void copyTree(TreeNode node,TreeNode new_node)&#123; if(node == null)&#123; return; &#125; if(node.left != null)&#123; new_node.left = new TreeNode(0); copyTree(node.left,new_node.left); &#125; if(node.right != null)&#123; new_node.right = new TreeNode(0); copyTree(node.right,new_node.right); &#125; &#125; &#125; 思路二（动态规划） 完整二叉树的左右子树也都是完整二叉树 左子树的所有可能和右子树的所有可能的组合就是答案 从节点数是1开始构造，一直构造到N 代码一1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public List&lt;TreeNode&gt; allPossibleFBT(int N) &#123; // 构造存储记忆，存放所有的完整二叉树 Map&lt;Integer,List&lt;TreeNode&gt;&gt; mem = new HashMap&lt;&gt;(); // 初始化，从0开始到N，可以不需要从0开始，为什么不需要从0开始，下面解释 for(int i=0;i&lt;=N;i++)&#123; mem.put(i,new LinkedList&lt;TreeNode&gt;()); &#125; // 初始化N=0没有节点，N=1一个根节点 mem.get(1).add(new TreeNode(0)); // 从N=2开始推算 for(int i=2;i&lt;=N;i++)&#123; // 左子树可能节点个数,但是由于是完整二叉树，左子树节点数不可能是0个节点，所以mem不加入N==0也可以,左子树节点个数范围[0,i-1] for(int l=0;l&lt;i;l++)&#123; List&lt;TreeNode&gt; lefts = mem.get(l); List&lt;TreeNode&gt; rights = mem.get(i-l-1);//总共i个节点，左边l个，一个根节点1，那么右边i-l-1 // 左右子树的组合就是新树,for 循环确保了左子树，右子树不可能是空，；例如 N == 0，for就不执行了 for(TreeNode left:lefts)&#123; for(TreeNode right:rights)&#123; TreeNode root = new TreeNode(0); // 完整二叉树——左右子树都是完整二叉树 root.left = left; root.right = right; mem.get(i).add(root); &#125; &#125; &#125; &#125; return mem.get(N); &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"},{"name":"二叉树","slug":"算法/二叉树","permalink":"http://yoursite.com/categories/算法/二叉树/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/tags/算法/"},{"name":"二叉树","slug":"二叉树","permalink":"http://yoursite.com/tags/二叉树/"},{"name":"完整二叉树","slug":"完整二叉树","permalink":"http://yoursite.com/tags/完整二叉树/"},{"name":"medium","slug":"medium","permalink":"http://yoursite.com/tags/medium/"},{"name":"894","slug":"894","permalink":"http://yoursite.com/tags/894/"}],"keywords":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"},{"name":"二叉树","slug":"算法/二叉树","permalink":"http://yoursite.com/categories/算法/二叉树/"}]},{"title":"序列模型RNN","slug":"序列模型RNN","date":"2018-06-20T12:19:32.000Z","updated":"2018-09-14T08:47:17.071Z","comments":true,"path":"2018/06/20/序列模型RNN/","link":"","permalink":"http://yoursite.com/2018/06/20/序列模型RNN/","excerpt":"","text":"要点怎么样处理作为一个单词序列的文本？或是一段音频呢？之前的神经网络都是只能处理固定长度的输入，如果输入不固定怎么办； 拿CNN来看看，CNN是在不同的空间上共享参数；扩展一下，是否可以在时间维度上共享参数呢。这个就是递归神经网络的初心了，现在流行这个词。 如果只是需要在每个时间点根据序列中的事件（状态）做出预测，那么你就可以在每个时间点用同样的分类器，比较简单； 但是我们说的话是前面的词语对后面的词语都有影响，所以早期的思想是用递归地使用先前分类器的，记录下先前分类器的状态，所以你需要一个很深的神经网络，这样网络可能要几百，几千层； 现在的我们采用的是一种较为简单的分类器，一部分连接到输入层，一部分连接到过去的事件 为什么RNN的反向传播不使用梯度下降？梯度下降要从输出层一直计算到输入层，这些导数都会作用到w参数上，但是这个模型都是共用参数w的，梯度下降偏好于无关联的参数更新，这样每层的学到东西才会保留下来；如果RNN反向传播使用梯度下降，那么要么梯度变到无穷大，要么变到无穷小…就没办法训练了 如何解决梯度爆炸？梯度裁剪，超过一定范围，就进行缩放 如何解决梯度消失？梯度消失就是模型只记住了近期的事件，序列稍长久不管用了，如何解决这个问题呢，LSTM(长短记忆模型登场了) LSTM是啥？RNN的细胞单元有 两个输入 （过去的状态，当前的序列）；两个输出（当前的预测，当前的状态） LSTM的细胞需要做3件事情： w:将数据写到记忆中 r:读取记忆中的数据 d:忘记记忆中的数据 如何给LSTM的门下指令呢？设置权重范围【0,1】，如果是0全部不做，如果是1 全部做，0.6 部分做；这样的话权重范围就是个连续的函数，就可以求导，并且进行反向传播 实际上，每个指令都是由一个共享参数的逻辑回归分类器来控制的，并且还通过tanh来保障输出值在【-1,1】之间 如何对LSTM 进行正则化？可以进行L2 以及Droupout； 使用Dropout时要记住只能用在序列输入，以及预测输出，不能用在过去和未来的状态传递上 Beam搜索是啥？当你预测句子中下一个出现的单词时，你可以遍历所有的词汇库来找出最大概率的词； 更复杂一点是每次保存所有单词，在所有单词的基础上，继续预测，最后通过概率相乘来计算所有预测的概率，但是这个很奢侈； 所以改进的方法就是只保留每个时间步最有可能的几个单词，这就是束搜索","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"RNN","slug":"深度学习/RNN","permalink":"http://yoursite.com/categories/深度学习/RNN/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"序列","slug":"序列","permalink":"http://yoursite.com/tags/序列/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"RNN","slug":"深度学习/RNN","permalink":"http://yoursite.com/categories/深度学习/RNN/"}]},{"title":"使用TensorFlow训练一个 CBOW模型并可视化输出","slug":"使用TensorFlow训练一个 CBOW模型并可视化输出","date":"2018-06-17T12:19:32.000Z","updated":"2018-09-14T08:47:07.681Z","comments":true,"path":"2018/06/17/使用TensorFlow训练一个 CBOW模型并可视化输出/","link":"","permalink":"http://yoursite.com/2018/06/17/使用TensorFlow训练一个 CBOW模型并可视化输出/","excerpt":"","text":"要点思路一： 训练数据的变化： 如果原来的训练数据是 1data: [&apos;anarchism&apos;, &apos;originated&apos;, &apos;as&apos;, &apos;a&apos;, &apos;term&apos;, &apos;of&apos;, &apos;abuse&apos;, &apos;first&apos;] 就是用上下文来预测这个单词，比如单词originated，左右的skip_window = 1，那么可以产生以下预测： 123with skip_window = 1: batch: [&apos;anarchism&apos;, &apos;as&apos;] labels: [&apos;originated&apos;, &apos;originated&apos;] 例如整个句子可以在【skip_window：batch_size-skip_window】中随机选择 batch_size/num_skips个单词，然后预测这个单词周围的上下文,如下方式早出训练数据 123with num_skips = 2 and skip_window = 1: batch: [&apos;anarchism&apos;, &apos;as&apos;, &apos;a&apos;, &apos;of&apos;, &apos;of&apos;, &apos;first&apos;, &apos;term&apos;, &apos;abuse&apos;] labels: [&apos;originated&apos;, &apos;originated&apos;, &apos;term&apos;, &apos;term&apos;, &apos;abuse&apos;, &apos;abuse&apos;, &apos;of&apos;, &apos;of&apos;] 思路二： 利用上下文的单词的sum ,来预测单词 1data: [&apos;anarchism&apos;, &apos;originated&apos;, &apos;as&apos;, &apos;a&apos;, &apos;term&apos;, &apos;of&apos;, &apos;abuse&apos;, &apos;first&apos;] 比如左右上下文范围都是1 比如单词originated 123with skip_window = 1: batch: [&apos;anarchism&apos;+&apos;as&apos;] labels: [&apos;originated&apos;] 整个句子可以做出如下样子 123with num_skips = 2 and skip_window = 1: batch: [&apos;anarchism&apos;+&apos;as&apos;, &apos;originated&apos;+&apos;a&apos;, &apos;as&apos;+&apos;term&apos; ... ] labels: [&apos;originated&apos;,&apos;as&apos;, &apos;a&apos; ...]","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"RNN","slug":"深度学习/RNN","permalink":"http://yoursite.com/categories/深度学习/RNN/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"CBOW","slug":"CBOW","permalink":"http://yoursite.com/tags/CBOW/"},{"name":"词嵌入","slug":"词嵌入","permalink":"http://yoursite.com/tags/词嵌入/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"RNN","slug":"深度学习/RNN","permalink":"http://yoursite.com/categories/深度学习/RNN/"}]},{"title":"使用TensorFlow训练一个 skip-gram 模型并可视化输出","slug":"TensorFlow实现 skip-gram 模型并可视化输出","date":"2018-06-15T12:19:32.000Z","updated":"2018-09-07T05:38:20.444Z","comments":true,"path":"2018/06/15/TensorFlow实现 skip-gram 模型并可视化输出/","link":"","permalink":"http://yoursite.com/2018/06/15/TensorFlow实现 skip-gram 模型并可视化输出/","excerpt":"","text":"要点我们看深度学习如何处理文本的 1. 目标：训练一个skip-gram模型2.什么是Skip-Gram模型 和 CBOW 模型如果是用一个词语作为输入，来预测它周围的上下文，那这个模型叫做【Skip-gram】 而如果是拿一个词语的上下文作为输入，来预测这个词语本身，则是 【CBOW 】 CBOW对小型数据库比较合适，而Skip-Gram在大型语料中表现更好。 3.简单了解下Skip-Gram模型比如『她们 夸 吴彦祖 帅 到 没朋友』，如果输入 x 是『吴彦祖』，那么 y 可以是『她们』、『夸』、『帅』、『没朋友』这些词 我们的目的就是通过词嵌入 计算 目标和预测相同的概率 one-hot ——&gt; 词嵌入 ——&gt;softmax——&gt; 计算概率 4.简单了解下CBOW跟 Skip-gram 相似，只不过: Skip-gram 是预测一个词的上下文，而 CBOW 是用上下文预测这个词 网络结构如下 5.介绍下数据集使用的是Text8数据集，压缩后大小是29.9M，包含单词个数 17005207个，大约1700万 函数collections.CounterCounter类的目的是用来跟踪值出现的次数。它是一个无序的容器类型，以字典的键值对形式存储，其中元素作为key，其计数作为value；类似于java的map most_common([n])返回一个TopN列表。如果n没有被指定，则返回所有元素。当多个元素计数值相同时，排列是无确定顺序的。 12345words = [&apos;hello&apos;,&apos;world&apos;,&apos;hello&apos;,&apos;hello&apos;,&apos;me&apos;]collections.Counter(words)&gt;&gt;&gt;&gt; Counter(&#123;&apos;hello&apos;: 3, &apos;world&apos;: 1, &apos;me&apos;: 1&#125;)collections.Counter(words).most_common(2)&gt;&gt;&gt;&gt;&gt; [(&apos;hello&apos;, 3), (&apos;world&apos;, 1)] extend()Python List extend()方法,extend() 函数用于在列表末尾一次性追加另一个序列中的多个值（用新列表扩展原来的列表）。 dict()字典是另一种可变容器模型，且可存储任意类型对象。字典的每个键值 key=&gt;value 对用冒号 : 分割 1mydict = &#123;&apos;a&apos;: 1, &apos;b&apos;: 2, &apos;b&apos;: &apos;3&apos;&#125;; 如何反转键值对呢 1reversed_dict = dict(zip(mydict.values(),mydict.keys())) tf.truncated_normal与tf.random_normal区别就是truncated_normal的随机值范围是 两个标准差以内 (mean?2stddev,mean+2stddev)\\\\ stddev=\\sigma=\\sqrt{\\frac{\\sum_{i=0}^{N}(x_i-u)^2}{N}}12345tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) shape：1-D向量 mean :均值，默认为0，即正态分布中的μstddev :标准差，默认为1，即正态分布中的σseed : 种子，同一个seed下的分布值均相同； 1234567def tf.truncated_normal( shape, #一个一维整数张量 或 一个Python数组。 这个值决定输出张量的形状。 mean=0.0,#一个零维张量或 类型属于dtype的Python值. 这个值决定正态分布片段的平均值 stddev=1.0,# 一个零维张量或 类型属于dtype的Python值. 这个值决定正态分布片段的标准差 dtype=tf.float32,# 输出的类型. seed=None, # 一个Python整数. 被用来为正态分布创建一个随机种子. name=None)#操作的名字 (可选参数). tf.random_uniform从字面uniform看就是均匀分布的；random_uniform:均匀分布随机数，范围为[minval,maxval] 123tf.random_uniform(shape,minval=0,maxval=None,dtype=tf.float32) #例如 返回6*6的矩阵，产生于low和high之间，产生的值是均匀分布的tf.random_uniform((6, 6), minval=low,maxval=high,dtype=tf.float32))) tf.nn.embedding_lookup123tf.nn.embedding_lookup(params, ids, partition_strategy=&apos;mod&apos;, name=None, validate_indices=True, max_norm=None)partition_strategy 是&quot;mod&quot; 和 &quot;div&quot;两种参数 embedding_lookup(params, ids)其实就是按照ids，返回params中的ids行的向量。 比如说，ids=[1,3,2],就是返回params中第1,3,2行。返回结果为由params的1,3,2行向量组成的tensor。 tf.nn.sampled_softmax_loss先采样后计算交叉熵损失 12345678910111213sampled_softmax_loss( weights, biases, labels, inputs, num_sampled, #采样的个数 num_classes, #总的类别个数 num_true=1, sampled_values=None, remove_accidental_hits=True, partition_strategy='mod', name='sampled_softmax_loss') 余弦相似度 similarity = cos(\\theta) = { A \\cdot B \\over \\|A\\| \\cdot \\|B\\| } = A_{normalized} \\cdot B_{normalized}所以归一化所有的词嵌入向量后，方便计算余弦相似度啊，只要做矩阵乘法，就可以得出所有向量的向量的相似度 numpy.argsort元素从小到大排序，返回其索引 1234argsort(a, axis=-1, kind=&apos;quicksort&apos;, order=None)例子：np.argsort(-x) #按降序排列np.argsort(x) #按升序排列 扩展一下： 对list 排序用sort(). 对任何可迭代序列排序用sorted(). 代码12345678910111213%matplotlib inlinefrom __future__ import print_functionimport collectionsimport mathimport numpy as npimport osimport randomimport tensorflow as tfimport zipfilefrom matplotlib import pylabfrom six.moves import rangefrom six.moves.urllib.request import urlretrievefrom sklearn.manifold import TSNE C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters 1234567891011121314151617# 下载文本数据集url = 'http://mattmahoney.net/dc/'def maybe_download(filename, expected_bytes): \"\"\"Download a file if not present, and make sure it's the right size.\"\"\" if not os.path.exists(filename): filename, _ = urlretrieve(url + filename, filename) statinfo = os.stat(filename) if statinfo.st_size == expected_bytes: print('Found and verified %s' % filename) else: print(statinfo.st_size) raise Exception( 'Failed to verify ' + filename + '. Can you get to it with a browser?') return filenamefilename = maybe_download('text8.zip', 31344016) Found and verified text8.zip 123456789# 将数据集读成Stringdef read_data(filename): \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\" with zipfile.ZipFile(filename) as f: data = tf.compat.as_str(f.read(f.namelist()[0])).split() return data words = read_data(filename)print('Data size %d' % len(words)) Data size 17005207 12345678910111213141516171819202122232425262728293031323334# 弄个字典存放单词，去掉频率极少的单词，加入UNK 作为未见单词分类，# 先弄个5万个单词vocabulary_size = 50000def build_dataset(words): count = [['UNK', -1]] # extend() 函数用于在列表末尾一次性追加另一个序列中的多个值（用新列表扩展原来的列表）。 # count 中存放着最常用5万个单词，unk 作为第一个单词 count.extend(collections.Counter(words).most_common(vocabulary_size - 1)) dictionary = dict() for word, _ in count: # 把最常见5万的word 放入 dictionary dictionary[word] = len(dictionary) data = list() unk_count = 0 # 清点words中不在5万个单词内的unk的个数； # 把words文本表示成 dict 中的value # 比如 “ my name is sudoli” ==&gt; \"[12,45,48,unk]\" for word in words: if word in dictionary: index = dictionary[word] else: index = 0 # dictionary['UNK'] unk_count = unk_count + 1 data.append(index) count[0][1] = unk_count # 把键值对交换一下 reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) return data, count, dictionary, reverse_dictionarydata, count, dictionary, reverse_dictionary = build_dataset(words)print('Most common words (+UNK)', count[:5])print('Sample data', data[:10])del words # 减少内存损耗 Most common words (+UNK) [[&#39;UNK&#39;, 418391], (&#39;the&#39;, 1061396), (&#39;of&#39;, 593677), (&#39;and&#39;, 416629), (&#39;one&#39;, 411764)] Sample data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156] 12345sentens = [];for i in data[100:110]: sentens.append(reverse_dictionary[i]); print(sentens) [&#39;interpretations&#39;, &#39;of&#39;, &#39;what&#39;, &#39;this&#39;, &#39;means&#39;, &#39;anarchism&#39;, &#39;also&#39;, &#39;refers&#39;, &#39;to&#39;, &#39;related&#39;] 12345678910111213141516171819202122232425262728293031323334353637383940# 为skip-gram 生成一个训练批次data_index = 0# batch_size 批次大小# num_skips 以这个单词为中心选择多少个单词# skip_window 在多大的窗口中选择，关键词的两边的窗口中的所有单词==num_skips；所以有num_skips = 2 * skip_window,但是也可以不取满def generate_batch(batch_size, num_skips, skip_window): global data_index assert batch_size % num_skips == 0 assert num_skips &lt;= 2 * skip_window # batch 是个list batch = np.ndarray(shape=(batch_size), dtype=np.int32) #labels 是个矩阵，bacth_size *1 的矩阵 labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32) span = 2 * skip_window + 1 # [ skip_window target skip_window ] # 双向队列 buffer = collections.deque(maxlen=span) for _ in range(span): buffer.append(data[data_index]) data_index = (data_index + 1) % len(data) for i in range(batch_size // num_skips): target = skip_window # target label at the center of the buffer targets_to_avoid = [ skip_window ] for j in range(num_skips): while target in targets_to_avoid: target = random.randint(0, span - 1) targets_to_avoid.append(target) batch[i * num_skips + j] = buffer[skip_window] labels[i * num_skips + j, 0] = buffer[target] buffer.append(data[data_index]) data_index = (data_index + 1) % len(data) return batch, labelsprint('data:', [reverse_dictionary[di] for di in data[:12]])for num_skips, skip_window in [(2, 1), (4, 2)]: data_index = 0 batch, labels = generate_batch(batch_size=12, num_skips=num_skips, skip_window=skip_window) print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window)) print(' batch:', [reverse_dictionary[bi] for bi in batch]) print(' labels:', [reverse_dictionary[li] for li in labels.reshape(12)]) data: [&#39;anarchism&#39;, &#39;originated&#39;, &#39;as&#39;, &#39;a&#39;, &#39;term&#39;, &#39;of&#39;, &#39;abuse&#39;, &#39;first&#39;, &#39;used&#39;, &#39;against&#39;, &#39;early&#39;, &#39;working&#39;] with num_skips = 2 and skip_window = 1: batch: [&#39;originated&#39;, &#39;originated&#39;, &#39;as&#39;, &#39;as&#39;, &#39;a&#39;, &#39;a&#39;, &#39;term&#39;, &#39;term&#39;, &#39;of&#39;, &#39;of&#39;, &#39;abuse&#39;, &#39;abuse&#39;] labels: [&#39;anarchism&#39;, &#39;as&#39;, &#39;a&#39;, &#39;originated&#39;, &#39;term&#39;, &#39;as&#39;, &#39;a&#39;, &#39;of&#39;, &#39;abuse&#39;, &#39;term&#39;, &#39;first&#39;, &#39;of&#39;] with num_skips = 4 and skip_window = 2: batch: [&#39;as&#39;, &#39;as&#39;, &#39;as&#39;, &#39;as&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;term&#39;, &#39;term&#39;, &#39;term&#39;, &#39;term&#39;] labels: [&#39;anarchism&#39;, &#39;originated&#39;, &#39;term&#39;, &#39;a&#39;, &#39;originated&#39;, &#39;term&#39;, &#39;of&#39;, &#39;as&#39;, &#39;abuse&#39;, &#39;a&#39;, &#39;as&#39;, &#39;of&#39;] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 接下来就是核心部分了 skip-grambatch_size = 128embedding_size = 128 # 这个就是最终词嵌入向量的维度数skip_window = 1 # 这个就是左右窗口的要考虑的大小了num_skips = 2 # 总共预测关键词周围几个单词# 我们再随机构建一个验证集，验证集选择最常见的单词valid_size = 16 # Random set of words to evaluate similarity on.valid_window = 100 # Only pick dev samples in the head of the distribution.# 从100的窗口范围内，采样16个样本大小valid_examples = np.array(random.sample(range(valid_window), valid_size))num_sampled = 64 # 负采样样本个数graph = tf.Graph()# TF不区分CPU的设备号，设置为0即可；若设置GPU 对显存要求大with graph.as_default(), tf.device('/cpu:0'): # Input data. train_dataset = tf.placeholder(tf.int32, shape=[batch_size]) train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1]) valid_dataset = tf.constant(valid_examples, dtype=tf.int32) # Variables. # 均匀分布的【-1,1】的embeddings embeddings = tf.Variable( tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)) # 权重参数采用两个标准差内的truncated_normal，叫做权重归一化，防止梯度消失 softmax_weights = tf.Variable( tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size))) # 偏移量直接就是0 softmax_biases = tf.Variable(tf.zeros([vocabulary_size])) # Model. # Look up embeddings for inputs.就是把train_dataset中的数字变成embeddings 中128维向量 embed = tf.nn.embedding_lookup(embeddings, train_dataset) # Compute the softmax loss, using a sample of the negative labels each time. # 采用负采样计算softmax loss = tf.reduce_mean( tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed, labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size)) # Optimizer. # Note: The optimizer will optimize the softmax_weights AND the embeddings. # 优化器将更新softmax_weights 以及 embeddings.因为embeddings被定义为变量，minimize()默认修改所有的变量 # 这个就是风格画产生的原因之一 optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss) # 计算所有批次和valid_dataset 的余弦相似度 # 余弦相似度 # 求出每个embeddings每个向量的长度，就是求模 norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True)) # embeddings 每个向量进行归一化，将每个向量各自做归一化，就是去掉了向量的长度，只关注向量的方向 normalized_embeddings = embeddings / norm # 所以比较向量的时候只用了方向信息，余弦相似度 valid_embeddings = tf.nn.embedding_lookup( normalized_embeddings, valid_dataset) similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings)) WARNING:tensorflow:From C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version. Instructions for updating: Future major versions of TensorFlow will allow gradients to flow into the labels input on backprop by default. See tf.nn.softmax_cross_entropy_with_logits_v2. 1234567891011121314151617181920212223242526272829303132num_steps = 100001with tf.Session(graph=graph) as session: tf.global_variables_initializer().run() print('Initialized') average_loss = 0 for step in range(num_steps): batch_data, batch_labels = generate_batch( batch_size, num_skips, skip_window) feed_dict = &#123;train_dataset : batch_data, train_labels : batch_labels&#125; _, l = session.run([optimizer, loss], feed_dict=feed_dict) average_loss += l if step % 2000 == 0: if step &gt; 0: average_loss = average_loss / 2000 # 平均损失时对过去2000批次的损失的估计 print('Average loss at step %d: %f' % (step, average_loss)) average_loss = 0 # 这个计算代价很大 5万的矩阵归一化，后矩阵相乘； if step % 10000 == 0: sim = similarity.eval() for i in range(valid_size): valid_word = reverse_dictionary[valid_examples[i]] top_k = 8 # 选取to_k 的近似值,这里argsort是 nearest = (-sim[i, :]).argsort()[1:top_k+1] log = 'Nearest to %s:' % valid_word for k in range(top_k): close_word = reverse_dictionary[nearest[k]] log = '%s %s,' % (log, close_word) print(log) final_embeddings = normalized_embeddings.eval() Initialized Average loss at step 0: 7.906703 Nearest to is: bobble, venn, rodeo, ric, walks, shemini, graphite, instituting, Nearest to s: impart, understandably, waveguide, yakko, whittier, lingual, protestant, lautering, Nearest to who: haken, sol, javier, dressage, clay, almeida, fruiting, plans, Nearest to time: meninges, tley, attends, unimpressed, sep, acct, wrestlers, disabled, Nearest to four: repulsed, carrying, boeing, adjectival, zarathustra, locate, bittorrent, scilly, Nearest to or: starvation, headphones, statisticians, destitute, pragmatics, aten, hogs, chamada, Nearest to has: spenser, showings, cassiodorus, taste, afterwards, numeration, periods, judicial, Nearest to about: baker, lasting, great, spiny, hobbs, jiang, mistakenly, enemy, Nearest to such: insistent, awacs, accomplices, jokes, trivial, cigarettes, vista, enactments, Nearest to five: airspace, isomorphic, hahnemann, ddd, hallstatt, algonquian, substance, fantasia, Nearest to when: buffalo, formulas, homerun, karate, indic, andalus, subservient, klan, Nearest to there: sacks, yaoi, textrm, indigenous, inhibition, somerville, kournikova, repainted, Nearest to use: javan, islands, mengele, karim, carr, conses, syllogism, goodfellas, Nearest to i: regan, adagio, premiership, memos, payroll, mccormick, varicella, gow, Nearest to will: xiang, prosper, moldavian, stitch, geothermal, attracting, infection, livius, Nearest to no: commanded, massacre, gluck, philippine, hundred, chordata, dark, shor, Average loss at step 2000: 4.378162 Average loss at step 4000: 3.866741 Average loss at step 6000: 3.789374 Average loss at step 8000: 3.693026 Average loss at step 10000: 3.616336 Nearest to is: was, are, has, mjf, seconded, malenkov, balm, be, Nearest to s: his, and, my, haram, ghiorso, aloe, consequently, kusanagi, Nearest to who: he, they, still, and, proviso, then, fruiting, underwent, Nearest to time: unimpressed, him, intravenous, wrestlers, key, way, miyamoto, kahan, Nearest to four: three, eight, seven, six, five, two, nine, zero, Nearest to or: and, propanol, dissipation, zombies, than, overtraining, archaeologists, hearted, Nearest to has: had, is, was, have, cassiodorus, afterwards, numeration, taste, Nearest to about: docklands, mistakenly, raymond, rapcore, alcaeus, friends, fuelling, spiny, Nearest to such: known, conifers, robot, ok, adventurers, well, creator, accomplices, Nearest to five: eight, seven, nine, six, four, three, two, zero, Nearest to when: donohue, parson, bewildering, andalus, describes, saimei, buffalo, honorably, Nearest to there: it, they, sacks, repainted, he, haley, spitz, spiked, Nearest to use: crew, appending, mengele, bands, generators, cowardice, dtv, karim, Nearest to i: minute, constitutes, swnt, singaporean, uttered, peachtree, filaments, slaughterhouse, Nearest to will: xiang, would, can, to, must, prosper, moldavian, cdma, Nearest to no: massacre, named, machina, it, dried, minimized, dark, gluck, Average loss at step 12000: 3.605883 Average loss at step 14000: 3.577284 Average loss at step 16000: 3.407318 Average loss at step 18000: 3.461814 Average loss at step 20000: 3.540494 Nearest to is: was, are, has, mjf, be, rectangles, myrrh, were, Nearest to s: chew, gtpase, byron, ghiorso, bulfinch, expletive, aleksander, legnica, Nearest to who: he, then, which, they, also, often, still, married, Nearest to time: way, intravenous, unimpressed, wrestlers, key, bakunin, chernobyl, diatessaron, Nearest to four: three, five, six, two, seven, eight, nine, zero, Nearest to or: and, than, with, encouraging, mishna, dissipation, like, stingray, Nearest to has: had, is, have, was, multipart, afterwards, cassiodorus, pap, Nearest to about: mistakenly, docklands, fuelling, alcaeus, derived, heathen, next, indulging, Nearest to such: known, irrigation, conifers, well, robot, many, kanal, circulatory, Nearest to five: six, four, three, seven, zero, eight, two, nine, Nearest to when: since, before, donohue, but, parson, cryptographer, culver, reputations, Nearest to there: it, they, he, which, often, excluding, bunny, algebra, Nearest to use: crew, mengele, bands, appending, generators, dtv, deceive, cowardice, Nearest to i: ii, minute, constitutes, foregoing, filaments, mastered, we, fatal, Nearest to will: would, can, must, may, to, but, could, xiang, Nearest to no: massacre, dried, dark, it, a, gluck, topple, flagpole, Average loss at step 22000: 3.504591 Average loss at step 24000: 3.490641 Average loss at step 26000: 3.483303 Average loss at step 28000: 3.475509 Average loss at step 30000: 3.505294 Nearest to is: was, has, are, be, were, mjf, myrrh, became, Nearest to s: his, haram, antonin, byron, of, bends, harmonically, bey, Nearest to who: he, they, which, also, still, then, often, she, Nearest to time: unsympathetic, way, belief, key, intravenous, diatessaron, maligned, chernobyl, Nearest to four: five, six, eight, seven, three, two, nine, zero, Nearest to or: and, but, than, bakelite, like, mishna, skier, as, Nearest to has: had, have, is, was, praises, osvaldo, afterwards, having, Nearest to about: alcaeus, mistakenly, docklands, domestic, financial, reformation, devotees, friends, Nearest to such: known, well, these, irrigation, conifers, many, other, robot, Nearest to five: four, eight, seven, six, three, nine, zero, two, Nearest to when: before, but, if, after, was, since, while, fz, Nearest to there: they, it, he, often, still, usually, bunny, we, Nearest to use: crew, mengele, bands, goods, demography, maitreya, purify, deceive, Nearest to i: ii, we, minute, iii, constitutes, doubleday, foregoing, prestige, Nearest to will: can, would, may, must, could, should, to, cannot, Nearest to no: dried, any, dark, a, it, only, machina, there, Average loss at step 32000: 3.502657 Average loss at step 34000: 3.492980 Average loss at step 36000: 3.460367 Average loss at step 38000: 3.306287 Average loss at step 40000: 3.429836 Nearest to is: was, has, be, are, gaylord, myrrh, inr, cryptography, Nearest to s: his, bey, vinegar, her, while, haram, harmonically, disguise, Nearest to who: he, which, also, often, still, they, broadly, condensates, Nearest to time: way, year, unsympathetic, key, unimpressed, shays, poetical, maligned, Nearest to four: six, three, five, seven, eight, two, nine, one, Nearest to or: and, than, a, the, with, tony, querying, cca, Nearest to has: had, have, was, is, having, osvaldo, afterwards, inquisitorial, Nearest to about: alcaeus, domestic, mistakenly, docklands, koan, financial, on, before, Nearest to such: known, well, these, irrigation, many, willing, conifers, lahore, Nearest to five: seven, six, four, three, eight, nine, zero, two, Nearest to when: before, if, while, where, after, but, was, fz, Nearest to there: it, they, he, still, often, usually, these, which, Nearest to use: purify, mengele, goods, interfere, form, measure, bands, dtv, Nearest to i: ii, we, you, t, they, he, volumes, constitutes, Nearest to will: would, can, must, may, could, should, cannot, to, Nearest to no: any, dried, dark, it, profoundly, flip, than, theories, Average loss at step 42000: 3.443375 Average loss at step 44000: 3.449424 Average loss at step 46000: 3.455859 Average loss at step 48000: 3.352189 Average loss at step 50000: 3.383797 Nearest to is: was, are, myrrh, has, be, mjf, gaylord, lucian, Nearest to s: his, bey, ztas, stranded, antonin, and, ghanaian, briand, Nearest to who: he, which, she, also, manchu, then, there, often, Nearest to time: way, year, unsympathetic, period, key, maligned, shays, unimpressed, Nearest to four: six, eight, three, five, seven, nine, two, zero, Nearest to or: and, than, marcius, while, extrasolar, murray, like, landlocked, Nearest to has: had, have, was, is, does, having, applicable, osvaldo, Nearest to about: koan, mistakenly, alcaeus, docklands, nous, leni, how, meters, Nearest to such: well, these, known, many, irrigation, conifers, perish, guericke, Nearest to five: six, four, seven, eight, three, zero, nine, two, Nearest to when: while, if, after, before, where, but, since, although, Nearest to there: they, it, he, often, still, usually, these, bunny, Nearest to use: mengele, measure, portions, goods, purify, form, maitreya, interfere, Nearest to i: ii, we, you, they, foregoing, iii, he, t, Nearest to will: would, can, must, could, may, should, cannot, might, Nearest to no: any, dried, a, she, flip, distinguishable, ellefson, sharps, Average loss at step 52000: 3.436850 Average loss at step 54000: 3.430339 Average loss at step 56000: 3.442089 Average loss at step 58000: 3.393397 Average loss at step 60000: 3.393661 Nearest to is: was, has, are, although, myrrh, be, becomes, gaylord, Nearest to s: diff, decrypted, haram, px, ghiorso, bulfinch, bey, hne, Nearest to who: he, which, she, they, still, never, also, married, Nearest to time: way, unsympathetic, period, maligned, shays, place, year, comprehensively, Nearest to four: six, five, eight, three, seven, nine, zero, two, Nearest to or: and, than, but, including, like, ollie, manchukuo, olson, Nearest to has: had, have, is, was, having, plays, thinks, although, Nearest to about: koan, alcaeus, lasting, how, docklands, nous, what, domestic, Nearest to such: known, these, well, many, including, irrigation, lahore, venezia, Nearest to five: four, six, three, eight, seven, nine, zero, two, Nearest to when: if, before, after, while, where, although, because, though, Nearest to there: they, it, often, still, he, usually, she, now, Nearest to use: measure, mengele, portions, goods, infraclass, cause, most, norah, Nearest to i: we, ii, you, they, foregoing, t, doubleday, iii, Nearest to will: would, must, may, can, could, should, cannot, might, Nearest to no: any, sex, paintings, than, sharps, considerably, ellefson, arkham, Average loss at step 62000: 3.246356 Average loss at step 64000: 3.253968 Average loss at step 66000: 3.403710 Average loss at step 68000: 3.393377 Average loss at step 70000: 3.360259 Nearest to is: was, has, are, be, mjf, although, myrrh, became, Nearest to s: haram, diff, decrypted, madman, bey, ztas, isbn, whose, Nearest to who: he, which, never, they, married, she, still, manchu, Nearest to time: unsympathetic, way, comprehensively, maligned, year, place, season, shays, Nearest to four: six, five, eight, seven, three, nine, two, zero, Nearest to or: and, than, like, any, while, but, dissection, the, Nearest to has: had, have, is, was, although, having, applicable, borrower, Nearest to about: koan, alcaeus, regarding, lasting, over, mistakenly, docklands, derry, Nearest to such: these, known, many, well, certain, lahore, including, venezia, Nearest to five: six, eight, four, seven, three, nine, zero, two, Nearest to when: if, before, while, after, where, however, though, although, Nearest to there: they, it, still, he, often, usually, we, now, Nearest to use: measure, mengele, form, appear, dominoes, think, lesund, deceive, Nearest to i: we, you, ii, g, doubleday, foregoing, god, t, Nearest to will: would, must, could, may, can, should, might, cannot, Nearest to no: any, little, than, considerably, paintings, there, flagpole, flip, Average loss at step 72000: 3.373728 Average loss at step 74000: 3.351606 Average loss at step 76000: 3.323821 Average loss at step 78000: 3.353756 Average loss at step 80000: 3.377470 Nearest to is: was, has, are, becomes, although, be, became, includes, Nearest to s: haram, ztas, fealty, bey, dtp, justly, isbn, whose, Nearest to who: he, never, she, married, which, they, often, manchu, Nearest to time: unsympathetic, year, maligned, season, way, khanate, boas, period, Nearest to four: five, six, seven, eight, three, nine, two, zero, Nearest to or: and, while, per, than, dissection, van, restrain, pekka, Nearest to has: had, have, is, was, having, although, since, borrower, Nearest to about: alcaeus, koan, derry, indulging, lasting, regarding, over, docklands, Nearest to such: well, these, known, follows, certain, including, opposed, conifers, Nearest to five: six, four, seven, eight, three, nine, zero, two, Nearest to when: before, if, after, though, during, although, while, where, Nearest to there: it, they, he, usually, she, often, still, we, Nearest to use: measure, form, think, mengele, cause, dtv, treatment, analysing, Nearest to i: ii, you, we, iii, they, t, foregoing, doubleday, Nearest to will: would, could, can, must, may, should, might, cannot, Nearest to no: any, little, teng, flagpole, she, prespa, arkham, ellefson, Average loss at step 82000: 3.409646 Average loss at step 84000: 3.412236 Average loss at step 86000: 3.390080 Average loss at step 88000: 3.352896 Average loss at step 90000: 3.365889 Nearest to is: was, has, are, be, although, gaylord, becomes, myrrh, Nearest to s: his, whose, isbn, haram, bey, briand, isaac, barclays, Nearest to who: he, often, she, never, also, which, married, then, Nearest to time: unsympathetic, season, year, period, maligned, way, length, sort, Nearest to four: five, seven, six, eight, three, nine, two, one, Nearest to or: and, email, affreightment, taney, wideawake, pensacola, while, dissection, Nearest to has: had, have, is, was, having, since, although, bu, Nearest to about: koan, alcaeus, regarding, over, derry, docklands, indulging, on, Nearest to such: known, well, these, certain, follows, described, separate, many, Nearest to five: eight, seven, four, three, six, nine, zero, two, Nearest to when: if, before, while, after, although, where, until, though, Nearest to there: they, it, he, still, she, usually, we, now, Nearest to use: treatment, list, cause, measure, most, analysing, notion, many, Nearest to i: we, you, ii, iii, doubleday, g, t, minute, Nearest to will: would, could, can, must, may, should, might, cannot, Nearest to no: any, little, only, than, another, distinguishable, considerably, she, Average loss at step 92000: 3.398345 Average loss at step 94000: 3.252411 Average loss at step 96000: 3.359638 Average loss at step 98000: 3.247843 Average loss at step 100000: 3.358810 Nearest to is: was, has, becomes, became, be, although, gaylord, myrrh, Nearest to s: whose, his, haram, lebeau, wields, bey, isbn, isaac, Nearest to who: he, never, she, actually, often, and, which, still, Nearest to time: way, unsympathetic, step, khanate, year, maligned, season, sort, Nearest to four: six, seven, five, eight, three, two, nine, zero, Nearest to or: and, than, per, while, geq, merck, influencing, pekka, Nearest to has: had, have, is, was, since, having, borrower, kidney, Nearest to about: regarding, alcaeus, koan, indulging, derry, over, docklands, vigorous, Nearest to such: known, well, these, separate, certain, follows, many, including, Nearest to five: four, seven, six, eight, three, zero, nine, two, Nearest to when: if, while, before, although, where, after, though, until, Nearest to there: they, it, he, still, now, we, usually, sometimes, Nearest to use: treatment, measure, form, mengele, maximally, principles, rounding, cause, Nearest to i: you, we, ii, they, doubleday, iii, t, schaff, Nearest to will: would, must, can, could, should, may, might, cannot, Nearest to no: little, any, concerned, profoundly, steamships, considerably, xviii, she, 12345# 抽取400点到TSNE 来可视化num_points = 400tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')two_d_embeddings = tsne.fit_transform(final_embeddings[1:num_points+1, :]) 12345678910111213# 画出400个点的二维向量def plot(embeddings, labels): assert embeddings.shape[0] &gt;= len(labels), 'More labels than embeddings' pylab.figure(figsize=(15,15)) # in inches for i, label in enumerate(labels): x, y = embeddings[i,:] pylab.scatter(x, y) pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom') pylab.show()words = [reverse_dictionary[i] for i in range(1, num_points+1)]plot(two_d_embeddings, words)","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"RNN","slug":"深度学习/RNN","permalink":"http://yoursite.com/categories/深度学习/RNN/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"skip-gram","slug":"skip-gram","permalink":"http://yoursite.com/tags/skip-gram/"},{"name":"RNN","slug":"RNN","permalink":"http://yoursite.com/tags/RNN/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"RNN","slug":"深度学习/RNN","permalink":"http://yoursite.com/categories/深度学习/RNN/"}]},{"title":"文本和序列的深度模型01","slug":"文本和序列的深度模型01","date":"2018-06-10T12:19:32.000Z","updated":"2018-09-07T03:09:23.621Z","comments":true,"path":"2018/06/10/文本和序列的深度模型01/","link":"","permalink":"http://yoursite.com/2018/06/10/文本和序列的深度模型01/","excerpt":"","text":"要点我们看深度学习如何处理文本的 1. 例如文本分类的问题，判断一个文本是属于娱乐，体育，游戏，汽车，财经？这个问题估计今日头条会遇到吧，新华字典现已经收录20959个汉字、52万个词语，有些我们可能这辈子都用不到；但是对于文本分类来说，极少用到的词，才是重要的，常见词“我你他是”没啥用；例如“二十一三体综合症”，这个可能是篇医学文章，所以我们需要很多的训练样本 2.为什么要把词语转化成向量呢？我们有很多的词是近义词，虽然形式上有区别，但是意思是一样的，要知道词与词之间的关系 要找出词语的内在含义，并且有一个统一的数值化的标准来表示词语的内在含义 3.为什么采用非监督学习学习词向量？标记这些海量的文本似乎是不现实的，所以考虑采用非监督学习，用的核心理论就是 相似的词出现在相似的上下文中；这个既简单又实用 4.词嵌入的过程？词嵌入的过程就是寻找词之间内在关系的一种过程，例如52万个词语，那岂不是一个词要用52万维向量来表示，那太夸张了，其实词嵌入也是降维的过程，把词语一个个嵌入到一个低维空间中，在这个低维空间中，越靠近的点（词），意思越相近；通过无监督学习，你并不需要了解这些词语的实际含义，通过相似的词出现在相似的上下文中可以让机器学习到词语的内在含义，前提是文本量充足 5.经典模型word2vec？从字面上看就知道它是干啥的了，word转化成vec; 这个模型极其简单却效果极佳，只做了简单线性分类（逻辑回归） 把句子的词语嵌入到词向量空间，（刚开始是随机初始化的） 在句子中随机选一个词，选定这个词附近的窗口大小（上下文） 通过嵌入后的词向量预测该词语的上下文（上下文就是窗口中的词），词语的相似性通过余弦相似性度量，欧氏距离强调每个维度的差异，余弦相似性既可以度量近义词，也可以度量反义词，有归一化操作在里面； 负采样：在计算softmax时，ont-hot 向量52万维，太大了，所以对非目标词进行采样，就是选取一小部分变成一个小的one-hot 向量，大大加速了word2vec的softmax 计算 123456 graph LRInput[单词] --&gt; Embedding[词嵌入]Embedding --&gt; LinerModel[线性模型 wx+b]LinerModel--&gt; softmaxsoftmax--&gt;sampling[负采样]sampling --&gt; cross[与上下文单词交叉熵计算] 6.T-SNE在word2vec起到什么作用?你怎么判断你的词嵌入已经效果很好了呢，两种思路： 第一种，最近邻，找出某个词的最近邻，看看是不是其近义词 第二种，降维到2维空间，但是PCA 会损失很多关键信息，建议用T-SNE 如果向量空间维数过高，200维以上，或者更高，那么这里建议先使用PCA降维方法，降到50维左右以后，再使用t-SNE来继续降到二维或者三维。因为直接用t-SNE给高维向量空间做降维的话，效率会比较低 参考文档 7.什么是词嵌入后的类比属性？词语经过嵌入后，每一个单词都可以表示成一个向量，比如“漂亮” 的近义词 “美丽”，他们的词嵌入向量是非常接近的，余弦相似度可能接近1；而“高大” 与 “矮小” 余弦相似度可能接近-1 更加有趣的是： “猫咪” - “猫” +“狗” = “狗” “更高” -“高” + “强” = “更强”","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"RNN","slug":"深度学习/RNN","permalink":"http://yoursite.com/categories/深度学习/RNN/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"RNN","slug":"RNN","permalink":"http://yoursite.com/tags/RNN/"},{"name":"序列模型","slug":"序列模型","permalink":"http://yoursite.com/tags/序列模型/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"RNN","slug":"深度学习/RNN","permalink":"http://yoursite.com/categories/深度学习/RNN/"}]},{"title":"卷积神经网络的TensorFlow实践","slug":"卷积神经网络的TensorFlow实践","date":"2018-05-10T12:19:32.000Z","updated":"2018-09-05T06:11:33.873Z","comments":true,"path":"2018/05/10/卷积神经网络的TensorFlow实践/","link":"","permalink":"http://yoursite.com/2018/05/10/卷积神经网络的TensorFlow实践/","excerpt":"","text":"要点 本文主要示范如何使用神经网络的卷积 如何使用卷积核以及conv2d函数以及padding设置，stride设置 我们这次用了两个卷积层，和一个全连接层 这里我要说下的是padding same:填充，比如我的图像是[16,28,28,3] stride[1,2,2,1] filter[3,3,3,8] 那么结果是 【16,28/2,28/2，8】==》【16,14,14,8】保证了stride的缩放肯定是成比例的，stride的宽度步长是2，那么宽度的采样后就是28/2 valid 不填充，比如我的图像是[16,28,28,3] stride[1,2,2,1] filter[3,3,3,8] ，那么结果就是【16,25/3+1,25/3+1,8】==》【16,13,13,8】 看到没有，宽度变小了，不是原来的一半 函数tf.nn.conv2d12345678910tf.nn.conv2d( input, filter, strides, padding, use_cudnn_on_gpu=True, data_format='NHWC', dilations=[1, 1, 1, 1], name=None) Args Annotation 第一个参数input 指需要做卷积的输入图像，它要求是一个Tensor，具有[batch, in_height, in_width, in_channels]这样的shape，具体含义是[训练时一个batch的图片数量, 图片高度, 图片宽度, 图像通道数]，注意这是一个4维的Tensor，要求类型为float32和float64其中之一 第二个参数filter 相当于CNN中的卷积核，它要求是一个Tensor，具有[filter_height, filter_width, in_channels, out_channels]这样的shape，具体含义是[卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]，有一个地方需要注意，第三维in_channels，就是参数input的第四维 第三个参数strides 卷积时在图像每一维的步长，这是一个一维的向量，长度4，和input 是一一对应的；一般我都设置为【1,2,2,1】或者【1,1,1,1】； 第四个参数padding string类型的量，只能是”SAME”,”VALID”其中之一，这个值决定了不同的卷积方式 第五个参数 use_cudnn_on_gpu:bool类型，是否使用cudnn加速，默认为true 结果返回： 一个Tensor，这个输出，就是我们常说的feature map 作者：Niling 链接：https://www.jianshu.com/p/510bb4bc590f 來源：简书 简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。 tf.nn.max_pool()参数 value：需要池化的输入，一般池化层接在卷积层后面，所以输入通常是feature map，依然是[batch, height, width, channels]这样的shape ksize：池化窗口的大小，取一个四维向量，一般是[1, height, width, 1]，因为我们不想在batch和channels上做池化，所以这两个维度设为了1 strides：和卷积类似，窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1] padding：和卷积类似，可以取’VALID’ 或者’SAME’ use_cudnn_on_gpu：bool类型，是否使用cudnn加速，默认为true name：指定该操作的name 返回返回一个Tensor，类型不变，shape仍然是[batch, height, width, channels]这种形式 作者：我是谁的小超人 链接：https://www.jianshu.com/p/1d73fd1a256e 來源：简书 简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。 代码1234567# These are all the modules we'll be using later. Make sure you can import them# before proceeding further.from __future__ import print_functionimport numpy as npimport tensorflow as tffrom six.moves import cPickle as picklefrom six.moves import range C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters 1234567891011121314pickle_file = 'notMNIST.pickle'with open(pickle_file, 'rb') as f: save = pickle.load(f) train_dataset = save['train_dataset'] train_labels = save['train_labels'] valid_dataset = save['valid_dataset'] valid_labels = save['valid_labels'] test_dataset = save['test_dataset'] test_labels = save['test_labels'] del save # hint to help gc free up memory print('Training set', train_dataset.shape, train_labels.shape) print('Validation set', valid_dataset.shape, valid_labels.shape) print('Test set', test_dataset.shape, test_labels.shape) Training set (20000, 28, 28) (20000,) Validation set (1000, 28, 28) (1000,) Test set (1000, 28, 28) (1000,) 1234567891011121314151617181920# 这里我们把数据转化成 图像立方体：（宽，高，色道）# 把标签转化成one-hot 向量image_size = 28num_labels = 10num_channels = 1 # grayscaleimport numpy as npdef reformat(dataset, labels): dataset = dataset.reshape( (-1, image_size, image_size, num_channels)).astype(np.float32) # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...] labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32) return dataset, labelstrain_dataset, train_labels = reformat(train_dataset, train_labels)valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)test_dataset, test_labels = reformat(test_dataset, test_labels)print('Training set', train_dataset.shape, train_labels.shape)print('Validation set', valid_dataset.shape, valid_labels.shape)print('Test set', test_dataset.shape, test_labels.shape) Training set (20000, 28, 28, 1) (20000, 10) Validation set (1000, 28, 28, 1) (1000, 10) Test set (1000, 28, 28, 1) (1000, 10) 123def accuracy(predictions, labels): return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0]) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# 构建一个由两层卷积层以及一个完全层组成的神经网络# 卷积层计算代价高，所以要设置好卷积层的深度和宽度batch_size = 16patch_size = 5depth = 16num_hidden = 64graph = tf.Graph()with graph.as_default(): # Input data. tf_train_dataset = tf.placeholder( tf.float32, shape=(batch_size, image_size, image_size, num_channels)) tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels)) tf_valid_dataset = tf.constant(valid_dataset) tf_test_dataset = tf.constant(test_dataset) # Variables. # 第一层我们用5*5*1的过滤器，总共depth 个，就是说一共有16个过滤器 # 这里问个问题？为什么要设置方差是0.1 layer1_weights = tf.Variable(tf.truncated_normal( [patch_size, patch_size, num_channels, depth], stddev=0.1)) layer1_biases = tf.Variable(tf.zeros([depth])) # 第一层我们用5*5*16的过滤器，总共depth 个，就是说一共有16个过滤器 layer2_weights = tf.Variable(tf.truncated_normal( [patch_size, patch_size, depth, depth], stddev=0.1)) # 偏移量都是1 layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth])) # 第三层全连接层，产生一个64的节点的层 layer3_weights = tf.Variable(tf.truncated_normal( [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1)) layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden])) # 第四层全连接层，64个节点的层 layer4_weights = tf.Variable(tf.truncated_normal( [num_hidden, num_labels], stddev=0.1)) layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels])) # Model. 定义模型计算 def model(data): # 这个conv2d常用函数，详见我的函数说明 conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME') hidden = tf.nn.relu(conv + layer1_biases) conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME') hidden = tf.nn.relu(conv + layer2_biases) # 拿到shape,然后把shape 值变成list，方便后面reshape shape = hidden.get_shape().as_list() reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]]) hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases) return tf.matmul(hidden, layer4_weights) + layer4_biases # Training computation. logits = model(tf_train_dataset) loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) # Optimizer. optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss) # Predictions for the training, validation, and test data. train_prediction = tf.nn.softmax(logits) valid_prediction = tf.nn.softmax(model(tf_valid_dataset)) test_prediction = tf.nn.softmax(model(tf_test_dataset)) WARNING:tensorflow:From &lt;ipython-input-5-bb43204e2f1e&gt;:55: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version. Instructions for updating: Future major versions of TensorFlow will allow gradients to flow into the labels input on backprop by default. See tf.nn.softmax_cross_entropy_with_logits_v2. 123456789101112131415161718num_steps = 1001with tf.Session(graph=graph) as session: tf.global_variables_initializer().run() print('Initialized') for step in range(num_steps): offset = (step * batch_size) % (train_labels.shape[0] - batch_size) batch_data = train_dataset[offset:(offset + batch_size), :, :, :] batch_labels = train_labels[offset:(offset + batch_size), :] feed_dict = &#123;tf_train_dataset : batch_data, tf_train_labels : batch_labels&#125; _, l, predictions = session.run( [optimizer, loss, train_prediction], feed_dict=feed_dict) if (step % 50 == 0): print('Minibatch loss at step %d: %f' % (step, l)) print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels)) print('Validation accuracy: %.1f%%' % accuracy( valid_prediction.eval(), valid_labels)) print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels)) Initialized Minibatch loss at step 0: 2.871578 Minibatch accuracy: 12.5% Validation accuracy: 11.4% Minibatch loss at step 50: 1.603516 Minibatch accuracy: 37.5% Validation accuracy: 55.2% Minibatch loss at step 100: 1.132913 Minibatch accuracy: 62.5% Validation accuracy: 74.9% Minibatch loss at step 150: 0.959747 Minibatch accuracy: 68.8% Validation accuracy: 75.4% Minibatch loss at step 200: 0.925898 Minibatch accuracy: 68.8% Validation accuracy: 78.4% Minibatch loss at step 250: 0.961354 Minibatch accuracy: 75.0% Validation accuracy: 76.4% Minibatch loss at step 300: 0.885228 Minibatch accuracy: 68.8% Validation accuracy: 79.8% Minibatch loss at step 350: 0.329915 Minibatch accuracy: 87.5% Validation accuracy: 80.0% Minibatch loss at step 400: 0.525234 Minibatch accuracy: 93.8% Validation accuracy: 80.2% Minibatch loss at step 450: 0.728237 Minibatch accuracy: 75.0% Validation accuracy: 81.3% Minibatch loss at step 500: 0.781117 Minibatch accuracy: 75.0% Validation accuracy: 80.7% Minibatch loss at step 550: 0.725993 Minibatch accuracy: 75.0% Validation accuracy: 81.2% Minibatch loss at step 600: 0.588381 Minibatch accuracy: 81.2% Validation accuracy: 80.9% Minibatch loss at step 650: 0.313426 Minibatch accuracy: 87.5% Validation accuracy: 81.9% Minibatch loss at step 700: 0.076320 Minibatch accuracy: 100.0% Validation accuracy: 83.8% Minibatch loss at step 750: 0.580707 Minibatch accuracy: 87.5% Validation accuracy: 83.4% Minibatch loss at step 800: 0.641274 Minibatch accuracy: 75.0% Validation accuracy: 82.4% Minibatch loss at step 850: 0.665619 Minibatch accuracy: 75.0% Validation accuracy: 83.4% Minibatch loss at step 900: 0.026958 Minibatch accuracy: 100.0% Validation accuracy: 83.8% Minibatch loss at step 950: 0.789471 Minibatch accuracy: 81.2% Validation accuracy: 83.2% Minibatch loss at step 1000: 0.559735 Minibatch accuracy: 87.5% Validation accuracy: 83.9% Test accuracy: 89.4% 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 构建一个由两层卷积层以及一个完全层组成的神经网络# 卷积层计算代价高，所以要设置好卷积层的深度和宽度batch_size = 16patch_size = 5depth = 16num_hidden = 64graph = tf.Graph()with graph.as_default(): # Input data. tf_train_dataset = tf.placeholder( tf.float32, shape=(batch_size, image_size, image_size, num_channels)) tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels)) tf_valid_dataset = tf.constant(valid_dataset) tf_test_dataset = tf.constant(test_dataset) # Variables. # 第一层我们用5*5*1的过滤器，总共depth 个，就是说一共有16个过滤器 # 这里问个问题？为什么要设置方差是0.1 layer1_weights = tf.Variable(tf.truncated_normal( [patch_size, patch_size, num_channels, depth], stddev=0.1)) layer1_biases = tf.Variable(tf.zeros([depth])) # 第一层我们用5*5*16的过滤器，总共depth 个，就是说一共有16个过滤器 layer2_weights = tf.Variable(tf.truncated_normal( [patch_size, patch_size, depth, depth], stddev=0.1)) # 偏移量都是1 layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth])) # 第三层全连接层，产生一个64的节点的层 layer3_weights = tf.Variable(tf.truncated_normal( [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1)) layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden])) # 第四层全连接层，64个节点的层 layer4_weights = tf.Variable(tf.truncated_normal( [num_hidden, num_labels], stddev=0.1)) layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels])) # Model. 定义模型计算 def model(data): # 这个conv2d常用函数，详见我的函数说明 conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME') hidden = tf.nn.relu(conv + layer1_biases) print(hidden.get_shape) # conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME') # hidden = tf.nn.relu(conv + layer2_biases) # 添加了池化层 hidden = tf.nn.max_pool(hidden,[1,2,2,1],[1,2,2,1],padding='SAME') print(hidden.get_shape) # 拿到shape,然后把shape 值变成list，方便后面reshape shape = hidden.get_shape().as_list() reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]]) print(reshape.get_shape) hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases) return tf.matmul(hidden, layer4_weights) + layer4_biases # Training computation. logits = model(tf_train_dataset) loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) # Optimizer. optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss) # Predictions for the training, validation, and test data. train_prediction = tf.nn.softmax(logits) valid_prediction = tf.nn.softmax(model(tf_valid_dataset)) test_prediction = tf.nn.softmax(model(tf_test_dataset)) &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;Relu:0&#39; shape=(16, 14, 14, 16) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;MaxPool:0&#39; shape=(16, 7, 7, 16) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;Reshape:0&#39; shape=(16, 784) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;Relu_2:0&#39; shape=(1000, 14, 14, 16) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;MaxPool_1:0&#39; shape=(1000, 7, 7, 16) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;Reshape_1:0&#39; shape=(1000, 784) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;Relu_4:0&#39; shape=(1000, 14, 14, 16) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;MaxPool_2:0&#39; shape=(1000, 7, 7, 16) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;Reshape_2:0&#39; shape=(1000, 784) dtype=float32&gt;&gt; 12345678910111213141516171819num_steps = 1001with tf.Session(graph=graph) as session: tf.global_variables_initializer().run() print('Initialized') for step in range(num_steps): offset = (step * batch_size) % (train_labels.shape[0] - batch_size) batch_data = train_dataset[offset:(offset + batch_size), :, :, :] batch_labels = train_labels[offset:(offset + batch_size), :] feed_dict = &#123;tf_train_dataset : batch_data, tf_train_labels : batch_labels&#125; _, l, predictions = session.run( [optimizer, loss, train_prediction], feed_dict=feed_dict) if (step % 50 == 0): print('Minibatch loss at step %d: %f' % (step, l)) print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels)) print('Validation accuracy: %.1f%%' % accuracy( valid_prediction.eval(), valid_labels)) print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels)) Initialized Minibatch loss at step 0: 2.481559 Minibatch accuracy: 18.8% Validation accuracy: 9.8% Minibatch loss at step 50: 1.541600 Minibatch accuracy: 50.0% Validation accuracy: 51.1% Minibatch loss at step 100: 0.922436 Minibatch accuracy: 75.0% Validation accuracy: 75.2% Minibatch loss at step 150: 0.821155 Minibatch accuracy: 75.0% Validation accuracy: 79.4% Minibatch loss at step 200: 0.842788 Minibatch accuracy: 75.0% Validation accuracy: 79.3% Minibatch loss at step 250: 1.074179 Minibatch accuracy: 62.5% Validation accuracy: 79.5% Minibatch loss at step 300: 0.586223 Minibatch accuracy: 75.0% Validation accuracy: 80.2% Minibatch loss at step 350: 0.439200 Minibatch accuracy: 93.8% Validation accuracy: 80.4% Minibatch loss at step 400: 0.518278 Minibatch accuracy: 93.8% Validation accuracy: 81.0% Minibatch loss at step 450: 0.722640 Minibatch accuracy: 81.2% Validation accuracy: 81.7% Minibatch loss at step 500: 0.790449 Minibatch accuracy: 75.0% Validation accuracy: 81.8% Minibatch loss at step 550: 0.557499 Minibatch accuracy: 75.0% Validation accuracy: 80.9% Minibatch loss at step 600: 0.579446 Minibatch accuracy: 81.2% Validation accuracy: 81.9% Minibatch loss at step 650: 0.408497 Minibatch accuracy: 87.5% Validation accuracy: 82.1% Minibatch loss at step 700: 0.196559 Minibatch accuracy: 87.5% Validation accuracy: 82.5% Minibatch loss at step 750: 0.745032 Minibatch accuracy: 87.5% Validation accuracy: 83.3% Minibatch loss at step 800: 0.657614 Minibatch accuracy: 81.2% Validation accuracy: 82.0% Minibatch loss at step 850: 0.823133 Minibatch accuracy: 62.5% Validation accuracy: 82.5% Minibatch loss at step 900: 0.030666 Minibatch accuracy: 100.0% Validation accuracy: 83.5% Minibatch loss at step 950: 0.385080 Minibatch accuracy: 87.5% Validation accuracy: 82.5% Minibatch loss at step 1000: 0.597917 Minibatch accuracy: 87.5% Validation accuracy: 83.8% Test accuracy: 89.9% 12# 任务二：调试卷积网络，使其达到最佳效果# 可以参考LENET-5;添加dropout 还有学习速率衰减来试试 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293batch_size = 50patch_size = 5depth = 16num_hidden = 64graph = tf.Graph()with graph.as_default(): # Input data. tf_train_dataset = tf.placeholder( tf.float32, shape=(batch_size, image_size, image_size, num_channels)) tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels)) tf_valid_dataset = tf.constant(valid_dataset) tf_test_dataset = tf.constant(test_dataset) # Variables. # 第一层我们用5*5*1的过滤器，总共6 个，步长1==》[16,28,28,6] layer1_weights = tf.Variable(tf.truncated_normal( [5, 5, num_channels, 6], stddev=0.1)) layer1_biases = tf.Variable(tf.zeros([6])) # 第二层我们用2*2的最大池化，步长是2==》【16,14,14,6】 # 第三层卷积层【5,5】的过滤器16个，步长是1，valid ==》【16,10，10,16】 layer3_weights = tf.Variable(tf.truncated_normal( [5, 5, 6, 16], stddev=0.1)) # 偏移量都是1 layer3_biases = tf.Variable(tf.constant(1.0, shape=[16])) # 第四层最大池化，2*2的，步长2 ===》【16,5,5,16】 # 第五层卷积层 【5,5】的过滤器120个，步长1，valid ===》【16,1,1,120】 layer5_weights = tf.Variable(tf.truncated_normal( [5, 5,16,120], stddev=0.1)) layer5_biases = tf.Variable(tf.constant(1.0, shape=[120])) # 第六层全连接层，输出是84 layer6_weights = tf.Variable(tf.truncated_normal( [120, 84], stddev=0.1)) layer6_biases = tf.Variable(tf.constant(1.0, shape=[84])) # 第七层全连接层--输出层，输出是10 layer7_weights = tf.Variable(tf.truncated_normal( [84, 10], stddev=0.1)) layer7_biases = tf.Variable(tf.constant(1.0, shape=[10])) # Model. 定义模型计算 def model(data): # 这个conv2d常用函数，详见我的函数说明 # 第一层卷积 conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME') hidden = tf.nn.relu(conv + layer1_biases) # conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME') # hidden = tf.nn.relu(conv + layer2_biases) # 添加了池化层 # 第二层池化 print(hidden.get_shape) hidden = tf.nn.max_pool(hidden,[1,2,2,1],[1,2,2,1],padding='SAME') print(hidden.get_shape) # 第三层卷积，加上了dropout # conv = tf.nn.conv2d(tf.nn.dropout(hidden,keep_prob=0.5), layer3_weights, [1, 1, 1, 1], padding='VALID') conv = tf.nn.conv2d(hidden, layer3_weights, [1, 1, 1, 1], padding='VALID') hidden = conv + layer3_biases print(hidden.get_shape) # 第四层最大池化 hidden = tf.nn.max_pool(hidden,[1,2,2,1],[1,2,2,1],padding='SAME') print(hidden.get_shape) # 第五层卷积层 conv = tf.nn.conv2d(hidden, layer5_weights, [1, 1, 1, 1], padding='VALID') hidden = conv + layer5_biases print(hidden.get_shape) # 第六层全连接层 # 拿到shape,然后把shape 值变成list，方便后面reshape shape = hidden.get_shape().as_list() reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]]) print(reshape.get_shape) hidden = tf.nn.sigmoid(tf.matmul(reshape, layer6_weights) + layer6_biases) return tf.matmul(hidden, layer7_weights) + layer7_biases # Training computation. logits = model(tf_train_dataset) loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) # # Optimizer. # optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss) # 优化器采用GD+学习速率衰减 global_step = tf.Variable(0) # count the number of steps taken. learning_rate = tf.train.exponential_decay(0.05, global_step,decay_steps=500,decay_rate=0.96) optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step) # Predictions for the training, validation, and test data. train_prediction = tf.nn.softmax(logits) valid_prediction = tf.nn.softmax(model(tf_valid_dataset)) test_prediction = tf.nn.softmax(model(tf_test_dataset)) &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;Relu:0&#39; shape=(50, 28, 28, 6) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;MaxPool:0&#39; shape=(50, 14, 14, 6) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;add_1:0&#39; shape=(50, 10, 10, 16) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;MaxPool_1:0&#39; shape=(50, 5, 5, 16) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;add_2:0&#39; shape=(50, 1, 1, 120) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;Reshape:0&#39; shape=(50, 120) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;Relu_1:0&#39; shape=(1000, 28, 28, 6) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;MaxPool_2:0&#39; shape=(1000, 14, 14, 6) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;add_6:0&#39; shape=(1000, 10, 10, 16) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;MaxPool_3:0&#39; shape=(1000, 5, 5, 16) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;add_7:0&#39; shape=(1000, 1, 1, 120) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;Reshape_1:0&#39; shape=(1000, 120) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;Relu_2:0&#39; shape=(1000, 28, 28, 6) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;MaxPool_4:0&#39; shape=(1000, 14, 14, 6) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;add_11:0&#39; shape=(1000, 10, 10, 16) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;MaxPool_5:0&#39; shape=(1000, 5, 5, 16) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;add_12:0&#39; shape=(1000, 1, 1, 120) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;Reshape_2:0&#39; shape=(1000, 120) dtype=float32&gt;&gt; 12345678910111213141516171819num_steps = 5001with tf.Session(graph=graph) as session: tf.global_variables_initializer().run() print('Initialized') for step in range(num_steps): offset = (step * batch_size) % (train_labels.shape[0] - batch_size) batch_data = train_dataset[offset:(offset + batch_size), :, :, :] batch_labels = train_labels[offset:(offset + batch_size), :] feed_dict = &#123;tf_train_dataset : batch_data, tf_train_labels : batch_labels&#125; _, l, predictions = session.run( [optimizer, loss, train_prediction], feed_dict=feed_dict) if (step % 300 == 0): print('Minibatch loss at step %d: %f' % (step, l)) print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels)) print('Validation accuracy: %.1f%%' % accuracy( valid_prediction.eval(), valid_labels)) print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels)) Initialized Minibatch loss at step 0: 2.309445 Minibatch accuracy: 22.0% Validation accuracy: 10.0% Minibatch loss at step 300: 0.865154 Minibatch accuracy: 78.0% Validation accuracy: 78.5% Minibatch loss at step 600: 1.000986 Minibatch accuracy: 78.0% Validation accuracy: 80.1% Minibatch loss at step 900: 0.359938 Minibatch accuracy: 92.0% Validation accuracy: 82.1% Minibatch loss at step 1200: 0.665972 Minibatch accuracy: 82.0% Validation accuracy: 82.4% Minibatch loss at step 1500: 0.526232 Minibatch accuracy: 84.0% Validation accuracy: 83.3% Minibatch loss at step 1800: 0.601502 Minibatch accuracy: 84.0% Validation accuracy: 84.0% Minibatch loss at step 2100: 0.366273 Minibatch accuracy: 88.0% Validation accuracy: 85.1% Minibatch loss at step 2400: 0.402447 Minibatch accuracy: 88.0% Validation accuracy: 85.2% Minibatch loss at step 2700: 0.464672 Minibatch accuracy: 88.0% Validation accuracy: 85.0% Minibatch loss at step 3000: 0.415956 Minibatch accuracy: 90.0% Validation accuracy: 86.0% Minibatch loss at step 3300: 0.414384 Minibatch accuracy: 86.0% Validation accuracy: 86.8% Minibatch loss at step 3600: 0.384747 Minibatch accuracy: 88.0% Validation accuracy: 86.2% Minibatch loss at step 3900: 0.443196 Minibatch accuracy: 84.0% Validation accuracy: 86.2% Minibatch loss at step 4200: 0.301673 Minibatch accuracy: 92.0% Validation accuracy: 86.4% Minibatch loss at step 4500: 0.493804 Minibatch accuracy: 86.0% Validation accuracy: 87.1% Minibatch loss at step 4800: 0.311406 Minibatch accuracy: 92.0% Validation accuracy: 86.8% Test accuracy: 92.1%","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"卷积神经网络","slug":"深度学习/卷积神经网络","permalink":"http://yoursite.com/categories/深度学习/卷积神经网络/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://yoursite.com/tags/TensorFlow/"},{"name":"池化","slug":"池化","permalink":"http://yoursite.com/tags/池化/"},{"name":"卷积","slug":"卷积","permalink":"http://yoursite.com/tags/卷积/"},{"name":"LENET-5","slug":"LENET-5","permalink":"http://yoursite.com/tags/LENET-5/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"卷积神经网络","slug":"深度学习/卷积神经网络","permalink":"http://yoursite.com/categories/深度学习/卷积神经网络/"}]},{"title":"卷积神经网络","slug":"2018-05-03-卷积神经网络","date":"2018-05-03T12:19:32.000Z","updated":"2018-09-03T07:30:17.209Z","comments":true,"path":"2018/05/03/2018-05-03-卷积神经网络/","link":"","permalink":"http://yoursite.com/2018/05/03/2018-05-03-卷积神经网络/","excerpt":"","text":"要点颜色 思考一个问题，当你在识别手写数字时，数字的颜色重不重要？ 在轮廓识别的时候，颜色信息会耗费神经网络的训练资源，所以采用灰度更好 位置 思考：一张图像上有一只猫，但是猫的位置是变动的，猫的位置改变影响它是只猫的识别么？ 如果猫的位置信息也影响猫的识别，那么神经网络将会耗费大量的计算资源在位置记忆上 好的识别应该具有平移不变性，即使你位置再变，我同样的参数都可以识别你 词语的在句子中的位置 思考：句子中“老苏喜欢吃牛排，牛排还不能太老”；“牛排”在句子中的位置影响“牛排”的意思吗？ 不影响，我们说的话中的词语，大多数时候的意思是一样的，所有的词语要是有统一的权重就好了——权重共享 卷积 卷积的本质就是空间上共享参数的神经网络 神经网络的卷积过程，可以看成是一个金字塔的过程 金字塔的底部是输入图像（RGB） 通过卷积操作，不断压缩图像的维度，增加图像的深度 术语（lingo）池化 池化没有新的权重需要训练，但是有超参数要调整 池化大小 池化步长 操作分类 最大池化：对特征点周围像素进行最大值采样 平均池化：对特征点周围像素进行平均值采样 1*1卷积 卷积操作相当于在图像的一小块上运行了一个线性分类器 如果你在这个线性分类器前加上一个11的卷积，那就变成了一个小的*神经网络分类器了 1*1卷积使神经网络变得更深，更加低耗高效 计算代价较低，可以看成是矩阵相乘 Inception 就是冗余神经网络了，不再考虑每层到底用池化还是卷积，是用1x1的卷积还是3x3的卷积 在1x1的卷积后面把1x1 3x3 5x5 的卷积以及池化统统算一下 我认为就是全面的特征提取 经典的神经网络 LENET-5 1998年YANN LECUN 在字母识别中设计 ALEXNET 2012年赢得了ImageNet的物体识别挑战赛 参考资料卷积神经网络的各层","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"卷积神经网络","slug":"深度学习/卷积神经网络","permalink":"http://yoursite.com/categories/深度学习/卷积神经网络/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"池化","slug":"池化","permalink":"http://yoursite.com/tags/池化/"},{"name":"1x1","slug":"1x1","permalink":"http://yoursite.com/tags/1x1/"},{"name":"卷积","slug":"卷积","permalink":"http://yoursite.com/tags/卷积/"},{"name":"LENET-5","slug":"LENET-5","permalink":"http://yoursite.com/tags/LENET-5/"},{"name":"ALEXNET","slug":"ALEXNET","permalink":"http://yoursite.com/tags/ALEXNET/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"卷积神经网络","slug":"深度学习/卷积神经网络","permalink":"http://yoursite.com/categories/深度学习/卷积神经网络/"}]},{"title":"使用正则化去优化深度学习模型","slug":"使用正则化去优化深度学习模型","date":"2018-05-01T12:19:32.000Z","updated":"2018-09-04T07:03:37.536Z","comments":true,"path":"2018/05/01/使用正则化去优化深度学习模型/","link":"","permalink":"http://yoursite.com/2018/05/01/使用正则化去优化深度学习模型/","excerpt":"","text":"要点 了解正则化对模型的影响 训练数据采用notMNIST 采用L2 正则化 采用Dropout随机失活 采用学习速率衰减 我在增加神经网络层时遇到了梯度消失问题，就是loss 值算出来是nan;我有一种实用但是不能彻底解决梯度消失问题的策略； 首先RELU 可以帮助防止梯度消失 其次就是权重的初始化很重要，梯度消失是多个小于1的数相乘，所以如何避免呢，就是初始化时，假设前面一层有n个节点，那么我会这样初始化： relu : stddev=np.sqrt(\\frac{2.0}{n} ) \\\\ sigmod : stddev=np.sqrt(\\frac{1.0}{n} ) \\\\ tanh: stddev=np.sqrt(\\frac{1.0}{n} ） 函数 tf.nn.dropout 按照keep_prob失活神经元； 这种操作类似于缩放，但是记住缩放后，元素的和是不变的，所以不需要再手动乘上倍数 12345678910111213141516tf.nn.dropout( x, keep_prob, noise_shape=None, seed=None, name=None)Args:x: A floating point tensor.keep_prob: A scalar Tensor with the same type as x. The probability that each element is kept.noise_shape: A 1-D Tensor of type int32, representing the shape for randomly generated keep/drop flags.seed: A Python integer. Used to create random seeds. See tf.set_random_seed for behavior.name: A name for this operation (optional).Returns:A Tensor of the same shape of x. tf.train.exponential_decay 这个就是专门用来对学习速率进行衰减的 一句话概括这个函数，每decay_steps（步）就以decay_rate（率）乘以learning_rate 计算公式如下 12decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps) 123456789101112131415161718192021tf.train.exponential_decay( learning_rate, global_step, decay_steps, decay_rate, staircase=False, name=None)Args:learning_rate: A scalar float32 or float64 Tensor or a Python number. The initial learning rate.global_step: A scalar int32 or int64 Tensor or a Python number. Global step to use for the decay computation. Must not be negative.decay_steps: A scalar int32 or int64 Tensor or a Python number. Must be positive. See the decay computation above.decay_rate: A scalar float32 or float64 Tensor or a Python number. The decay rate.staircase: Boolean. If True decay the learning rate at discrete intervalsname: String. Optional name of the operation. Defaults to 'ExponentialDecay'.Returns:A scalar Tensor of the same type as learning_rate. The decayed learning rate.Raises:ValueError: if global_step is not supplied. 代码123456# These are all the modules we'll be using later. Make sure you can import them# before proceeding further.from __future__ import print_functionimport numpy as npimport tensorflow as tffrom six.moves import cPickle as pickle C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters 1234567891011121314pickle_file = 'notMNIST.pickle'with open(pickle_file, 'rb') as f: save = pickle.load(f) train_dataset = save['train_dataset'] train_labels = save['train_labels'] valid_dataset = save['valid_dataset'] valid_labels = save['valid_labels'] test_dataset = save['test_dataset'] test_labels = save['test_labels'] del save # hint to help gc free up memory print('Training set', train_dataset.shape, train_labels.shape) print('Validation set', valid_dataset.shape, valid_labels.shape) print('Test set', test_dataset.shape, test_labels.shape) Training set (20000, 28, 28) (20000,) Validation set (1000, 28, 28) (1000,) Test set (1000, 28, 28) (1000,) 12345678910111213141516# 把3维的训练数据转化为2维，即把28*28的图像像素矩阵转化成一维向量的转置 # 把标签转化为one-hot 向量image_size = 28num_labels = 10def reformat(dataset, labels): dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32) # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...] labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32) return dataset, labelstrain_dataset, train_labels = reformat(train_dataset, train_labels)valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)test_dataset, test_labels = reformat(test_dataset, test_labels)print('Training set', train_dataset.shape, train_labels.shape)print('Validation set', valid_dataset.shape, valid_labels.shape)print('Test set', test_dataset.shape, test_labels.shape) Training set (20000, 784) (20000, 10) Validation set (1000, 784) (1000, 10) Test set (1000, 784) (1000, 10) 1234567def accuracy(predictions, labels): # argmax axis = 0的时候返回每一列最大值的位置索引 # axis = 1的时候返回每一行最大值的位置索引 # axis = 2、3、4...，即为多维张量时，同理推断 # 得到true，fasle 的向量，然后统计1的个数，就是正确率 return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0]) 123import timelocaltime = time.asctime( time.localtime(time.time()) )print(localtime) Tue Sep 4 09:08:04 2018 1234567891011121314151617181920212223242526272829303132333435363738394041424344# 任务1，利用nn.l2_loss(t)来训练模型，包含了L2正则化# TODO:将全连接层转化成1024节点，并且有relu的隐藏层# 图的初始化操作batch_size = 128graph = tf.Graph()with graph.as_default(): # 输入数据，对于训练数据，我们使用一个占位符 # 在运行时，用一个批次 tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size)) tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels)) tf_valid_dataset = tf.constant(valid_dataset) tf_test_dataset = tf.constant(test_dataset) # 初始化变量 weights = tf.Variable( tf.truncated_normal([image_size * image_size, 1024])) biases = tf.Variable(tf.zeros([1024])) # 训练计算 hidden1 = tf.matmul(tf_train_dataset, weights) + biases hidden1 = tf.nn.relu(hidden1) weights1 = tf.Variable( tf.truncated_normal([1024, num_labels])) biases1 = tf.Variable(tf.zeros([num_labels])) logits = tf.matmul(hidden1, weights1) + biases1 loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))+\\ 1e-3 *(tf.nn.l2_loss(weights)+tf.nn.l2_loss(weights1)) # 优化器还是 optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss) # 计算预测值,这里的定义直接可以通过.evl()来调用 train_prediction = tf.nn.softmax(logits) valid_prediction = tf.nn.softmax( tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases),weights1) + biases1) test_prediction = tf.nn.softmax( tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases),weights1) + biases1) 12345678910111213141516171819202122232425262728num_steps = 3001with tf.Session(graph=graph) as session: tf.global_variables_initializer().run() print(\"Initialized\") starttime = time.clock() print('start time:',time.asctime( time.localtime(time.time()) )) for step in range(num_steps): # 用偏移量来选择数据，10一批次， 101个数据，当取到12批次，offset = 110%101 = 9,可以保证批次有效 # Note:我们可以用更好的随机策略选择批次 offset = (step * batch_size) % (train_labels.shape[0] - batch_size) # 创建一个批次. batch_data = train_dataset[offset:(offset + batch_size), :] batch_labels = train_labels[offset:(offset + batch_size), :] # 准备一个字典，来存放批数据，以便于 placeholder 来放数据 # 数据类型 numpy 的数组 feed_dict = &#123;tf_train_dataset : batch_data, tf_train_labels : batch_labels&#125; _, l, predictions = session.run( [optimizer, loss, train_prediction], feed_dict=feed_dict) if (step % 500 == 0): print(\"Minibatch loss at step %d: %f\" % (step, l)) print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)) print(\"Validation accuracy: %.1f%%\" % accuracy( valid_prediction.eval(), valid_labels)) print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels)) print('end time:',time.asctime( time.localtime(time.time()) )) elapsed = (time.clock() - starttime) print(\"Time used:\",elapsed) Initialized start time: Tue Sep 4 09:45:29 2018 Minibatch loss at step 0: 677.508362 Minibatch accuracy: 6.2% Validation accuracy: 10.6% Test accuracy: 11.5% Minibatch loss at step 500: 342.153320 Minibatch accuracy: 76.6% Validation accuracy: 74.5% Test accuracy: 82.2% Minibatch loss at step 1000: 336.932465 Minibatch accuracy: 75.0% Validation accuracy: 74.6% Test accuracy: 83.3% Minibatch loss at step 1500: 315.919678 Minibatch accuracy: 84.4% Validation accuracy: 75.6% Test accuracy: 84.2% Minibatch loss at step 2000: 310.289490 Minibatch accuracy: 85.9% Validation accuracy: 75.8% Test accuracy: 84.5% Minibatch loss at step 2500: 308.458069 Minibatch accuracy: 85.2% Validation accuracy: 76.7% Test accuracy: 84.8% Minibatch loss at step 3000: 304.995117 Minibatch accuracy: 84.4% Validation accuracy: 76.5% Test accuracy: 84.7% end time: Tue Sep 4 09:46:01 2018 Time used: 31.499022337961833 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475# TODO:制作过拟合情况,通过减少训练批次的大小# 下面训练随机梯度下降# 将数据保存到常量节点，创建一个占位节点,每次用数据代入占位节点# 图的初始化操作batch_size = 50graph = tf.Graph()with graph.as_default(): # 输入数据，对于训练数据，我们使用一个占位符 # 在运行时，用一个批次 tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size)) tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels)) tf_valid_dataset = tf.constant(valid_dataset) tf_test_dataset = tf.constant(test_dataset) # 初始化变量 weights = tf.Variable( tf.truncated_normal([image_size * image_size, 1024])) biases = tf.Variable(tf.zeros([1024])) # 训练计算 hidden1 = tf.matmul(tf_train_dataset, weights) + biases hidden1 = tf.nn.relu(hidden1) weights1 = tf.Variable( tf.truncated_normal([1024, num_labels])) biases1 = tf.Variable(tf.zeros([num_labels])) logits = tf.matmul(hidden1, weights1) + biases1 loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))+\\ 0 *(tf.nn.l2_loss(weights)+tf.nn.l2_loss(weights1)) # 优化器还是 optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss) # 计算预测值,这里的定义直接可以通过.evl()来调用 train_prediction = tf.nn.softmax(logits) valid_prediction = tf.nn.softmax( tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases),weights1) + biases1) test_prediction = tf.nn.softmax( tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases),weights1) + biases1)num_steps = 300with tf.Session(graph=graph) as session: tf.global_variables_initializer().run() print(\"Initialized\") starttime = time.clock() print('start time:',time.asctime( time.localtime(time.time()) )) for step in range(num_steps): # 用偏移量来选择数据，10一批次， 101个数据，当取到12批次，offset = 110%101 = 9,可以保证批次有效 # Note:我们可以用更好的随机策略选择批次 offset = (step * batch_size) % (train_labels.shape[0] - batch_size) # 创建一个批次. batch_data = train_dataset[offset:(offset + batch_size), :] batch_labels = train_labels[offset:(offset + batch_size), :] # 准备一个字典，来存放批数据，以便于 placeholder 来放数据 # 数据类型 numpy 的数组 feed_dict = &#123;tf_train_dataset : batch_data, tf_train_labels : batch_labels&#125; _, l, predictions = session.run( [optimizer, loss, train_prediction], feed_dict=feed_dict) if (step % 30 == 0): print(\"Minibatch loss at step %d: %f\" % (step, l)) print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)) print(\"Validation accuracy: %.1f%%\" % accuracy( valid_prediction.eval(), valid_labels)) print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels)) print('end time:',time.asctime( time.localtime(time.time()) )) elapsed = (time.clock() - starttime) print(\"Time used:\",elapsed) Initialized start time: Tue Sep 4 09:51:58 2018 Minibatch loss at step 0: 301.258759 Minibatch accuracy: 4.0% Validation accuracy: 21.8% Test accuracy: 22.0% Minibatch loss at step 30: 135.621140 Minibatch accuracy: 68.0% Validation accuracy: 75.7% Test accuracy: 80.1% Minibatch loss at step 60: 62.916607 Minibatch accuracy: 80.0% Validation accuracy: 75.6% Test accuracy: 80.8% Minibatch loss at step 90: 75.922493 Minibatch accuracy: 68.0% Validation accuracy: 72.6% Test accuracy: 77.4% Minibatch loss at step 120: 126.814140 Minibatch accuracy: 70.0% Validation accuracy: 72.4% Test accuracy: 80.6% Minibatch loss at step 150: 41.706875 Minibatch accuracy: 72.0% Validation accuracy: 76.6% Test accuracy: 83.5% Minibatch loss at step 180: 46.694637 Minibatch accuracy: 78.0% Validation accuracy: 75.7% Test accuracy: 81.1% Minibatch loss at step 210: 101.218369 Minibatch accuracy: 80.0% Validation accuracy: 75.6% Test accuracy: 82.2% Minibatch loss at step 240: 22.329721 Minibatch accuracy: 76.0% Validation accuracy: 78.1% Test accuracy: 83.4% Minibatch loss at step 270: 29.404995 Minibatch accuracy: 70.0% Validation accuracy: 79.2% Test accuracy: 83.8% end time: Tue Sep 4 09:52:01 2018 Time used: 2.9922106035760407 123456789101112131415161718192021222324252627282930313233343536373839404142# task 3,加入Dropout 随机失活# TODO: 使用nn.dropout来处理，但是只是用于训练时，评估时不需要用batch_size = 128graph = tf.Graph()with graph.as_default(): # 输入数据，对于训练数据，我们使用一个占位符 # 在运行时，用一个批次 tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size)) tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels)) tf_valid_dataset = tf.constant(valid_dataset) tf_test_dataset = tf.constant(test_dataset) # 初始化变量 weights = tf.Variable( tf.truncated_normal([image_size * image_size, 1024])) biases = tf.Variable(tf.zeros([1024])) # 训练计算 hidden1 = tf.matmul(tf_train_dataset, weights) + biases hidden1 = tf.nn.relu(hidden1) weights1 = tf.Variable( tf.truncated_normal([1024, num_labels])) biases1 = tf.Variable(tf.zeros([num_labels])) # 采用dropout 随机失活 logits = tf.matmul(tf.nn.dropout(hidden1,0.5), weights1) + biases1 loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) # 优化器还是 optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss) # 计算预测值,这里的定义直接可以通过.evl()来调用 train_prediction = tf.nn.softmax(logits) valid_prediction = tf.nn.softmax( tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases),weights1) + biases1) test_prediction = tf.nn.softmax( tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases),weights1) + biases1) 123456789101112131415161718192021222324252627282930313233343536num_steps = 3001def accuracy(predictions, labels): # argmax axis = 0的时候返回每一列最大值的位置索引 # axis = 1的时候返回每一行最大值的位置索引 # axis = 2、3、4...，即为多维张量时，同理推断 # 得到true，fasle 的向量，然后统计1的个数，就是正确率 return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])with tf.Session(graph=graph) as session: tf.global_variables_initializer().run() print(\"Initialized\") starttime = time.clock() print('start time:',time.asctime( time.localtime(time.time()) )) for step in range(num_steps): # 用偏移量来选择数据，10一批次， 101个数据，当取到12批次，offset = 110%101 = 9,可以保证批次有效 # Note:我们可以用更好的随机策略选择批次 offset = (step * batch_size) % (train_labels.shape[0] - batch_size) # 创建一个批次. batch_data = train_dataset[offset:(offset + batch_size), :] batch_labels = train_labels[offset:(offset + batch_size), :] # 准备一个字典，来存放批数据，以便于 placeholder 来放数据 # 数据类型 numpy 的数组 feed_dict = &#123;tf_train_dataset : batch_data, tf_train_labels : batch_labels&#125; _, l, predictions = session.run( [optimizer, loss, train_prediction], feed_dict=feed_dict) if (step % 500 == 0): print(\"Minibatch loss at step %d: %f\" % (step, l)) print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)) print(\"Validation accuracy: %.1f%%\" % accuracy( valid_prediction.eval(), valid_labels)) print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels)) print('end time:',time.asctime( time.localtime(time.time()) )) elapsed = (time.clock() - starttime) print(\"Time used:\",elapsed) Initialized start time: Tue Sep 4 12:01:16 2018 Minibatch loss at step 0: 431.407959 Minibatch accuracy: 14.8% Validation accuracy: 25.6% Test accuracy: 28.3% Minibatch loss at step 500: 13.565708 Minibatch accuracy: 79.7% Validation accuracy: 82.1% Test accuracy: 86.1% Minibatch loss at step 1000: 12.175354 Minibatch accuracy: 75.0% Validation accuracy: 82.2% Test accuracy: 88.3% Minibatch loss at step 1500: 4.373288 Minibatch accuracy: 78.1% Validation accuracy: 81.5% Test accuracy: 87.0% Minibatch loss at step 2000: 4.806540 Minibatch accuracy: 83.6% Validation accuracy: 81.6% Test accuracy: 87.6% Minibatch loss at step 2500: 7.298935 Minibatch accuracy: 79.7% Validation accuracy: 81.0% Test accuracy: 89.0% Minibatch loss at step 3000: 1.783429 Minibatch accuracy: 85.2% Validation accuracy: 82.0% Test accuracy: 89.3% end time: Tue Sep 4 12:01:46 2018 Time used: 30.266830585829666 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# task4 ,调整训练精确度，最高据说达到了97%# TODO:两种思路，一种是学习速率衰减# 另一种是增加多层# 我先来试试学习速率衰减+dropout +正则化：准确率89%batch_size = 128graph = tf.Graph()with graph.as_default(): # 输入数据，对于训练数据，我们使用一个占位符 # 在运行时，用一个批次 tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size)) tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels)) tf_valid_dataset = tf.constant(valid_dataset) tf_test_dataset = tf.constant(test_dataset) # 第一层,层数多了会出现梯度消失，所有初始化时很有讲究，就是初始化为前一层所有节点的个数 w1 = tf.Variable(tf.truncated_normal([image_size * image_size, 1024],stddev=np.sqrt(2.0/(image_size * image_size) ))) b1 = tf.Variable(tf.zeros([1024])) a1 = tf.nn.relu(tf.matmul(tf_train_dataset, w1) + b1) # 第二层 w2 = tf.Variable(tf.truncated_normal([1024, 512],stddev=np.sqrt(2.0/1024 ))) b2 = tf.Variable(tf.zeros([512])) a2 = tf.nn.tanh(tf.matmul(a1, w2) + b2) # # 第三层 w3 = tf.Variable(tf.truncated_normal([512, 256],stddev=np.sqrt(2.0/512 ))) b3 = tf.Variable(tf.zeros([256])) a3 = tf.nn.relu(tf.matmul(a2, w3) + b3) # # 第四层 w4 = tf.Variable(tf.truncated_normal([256, 128],stddev=np.sqrt(2.0/256 ))) b4 = tf.Variable(tf.zeros([128])) a4 = tf.nn.relu(tf.matmul(a3, w4) + b4) # # 第五层 w5 = tf.Variable(tf.truncated_normal([128, num_labels],stddev=np.sqrt(2.0/128 ))) b5 = tf.Variable(tf.zeros([num_labels])) # 特别注意，最后一层不要用激活函数，会梯度消失的，最后都是全连接层 logits = tf.matmul(a4, w5) + b5 loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) # 优化器还是GD+学习速率衰减 global_step = tf.Variable(0) # count the number of steps taken. learning_rate = tf.train.exponential_decay(0.5, global_step,decay_steps=500,decay_rate=0.96) optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step) # optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss) # 计算预测值,这里的定义直接可以通过.evl()来调用 train_prediction = tf.nn.softmax(logits) p1 = tf.nn.relu(tf.matmul(tf_valid_dataset, w1) + b1) p1 = tf.nn.relu(tf.matmul(p1, w2) + b2) p1 = tf.nn.relu(tf.matmul(p1, w3) + b3) p1 = tf.nn.relu(tf.matmul(p1, w4) + b4) p1 = tf.nn.relu(tf.matmul(p1, w5) + b5) valid_prediction = tf.nn.softmax(p1) t1 = tf.nn.relu(tf.matmul(tf_test_dataset, w1) + b1) t1 = tf.nn.relu(tf.matmul(t1, w2) + b2) t1 = tf.nn.relu(tf.matmul(t1, w3) + b3) t1 = tf.nn.relu(tf.matmul(t1, w4) + b4) t1 = tf.nn.relu(tf.matmul(t1, w5) + b5) test_prediction = tf.nn.softmax(t1) 1234567891011121314151617181920212223242526272829num_steps = 6000with tf.Session(graph=graph) as session: tf.global_variables_initializer().run() print(\"Initialized\") starttime = time.clock() print('start time:',time.asctime( time.localtime(time.time()) )) for step in range(num_steps): # 用偏移量来选择数据，10一批次， 101个数据，当取到12批次，offset = 110%101 = 9,可以保证批次有效 # Note:我们可以用更好的随机策略选择批次 offset = (step * batch_size) % (train_labels.shape[0] - batch_size) # 创建一个批次. batch_data = train_dataset[offset:(offset + batch_size), :] batch_labels = train_labels[offset:(offset + batch_size), :] # 准备一个字典，来存放批数据，以便于 placeholder 来放数据 # 数据类型 numpy 的数组 feed_dict = &#123;tf_train_dataset : batch_data, tf_train_labels : batch_labels&#125; _, l, predictions = session.run( [optimizer, loss, train_prediction], feed_dict=feed_dict) if (step % 500 == 0): print(\"Minibatch loss at step %d: %f\" % (step, l)) print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)) print(\"Validation accuracy: %.1f%%\" % accuracy( valid_prediction.eval(), valid_labels)) print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels)) print('end time:',time.asctime( time.localtime(time.time()) )) elapsed = (time.clock() - starttime) print(\"Time used:\",elapsed) Initialized start time: Tue Sep 4 14:47:32 2018 Minibatch loss at step 0: 2.305960 Minibatch accuracy: 14.8% Validation accuracy: 39.5% Test accuracy: 39.8% Minibatch loss at step 500: 0.361337 Minibatch accuracy: 89.8% Validation accuracy: 84.6% Test accuracy: 90.0% Minibatch loss at step 1000: 0.176127 Minibatch accuracy: 92.2% Validation accuracy: 84.7% Test accuracy: 89.9% Minibatch loss at step 1500: 0.134203 Minibatch accuracy: 95.3% Validation accuracy: 83.9% Test accuracy: 89.3% Minibatch loss at step 2000: 0.032851 Minibatch accuracy: 99.2% Validation accuracy: 86.0% Test accuracy: 91.0% Minibatch loss at step 2500: 0.023428 Minibatch accuracy: 100.0% Validation accuracy: 86.1% Test accuracy: 90.8% Minibatch loss at step 3000: 0.051366 Minibatch accuracy: 98.4% Validation accuracy: 86.1% Test accuracy: 91.7% Minibatch loss at step 3500: 0.026836 Minibatch accuracy: 99.2% Validation accuracy: 86.1% Test accuracy: 91.9% Minibatch loss at step 4000: 0.002356 Minibatch accuracy: 100.0% Validation accuracy: 86.1% Test accuracy: 91.4% Minibatch loss at step 4500: 0.002357 Minibatch accuracy: 100.0% Validation accuracy: 86.3% Test accuracy: 91.6% Minibatch loss at step 5000: 0.000582 Minibatch accuracy: 100.0% Validation accuracy: 86.7% Test accuracy: 91.1% Minibatch loss at step 5500: 0.024401 Minibatch accuracy: 99.2% Validation accuracy: 86.4% Test accuracy: 90.7% end time: Tue Sep 4 14:49:17 2018 Time used: 105.25818238910142","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://yoursite.com/tags/TensorFlow/"},{"name":"L2","slug":"L2","permalink":"http://yoursite.com/tags/L2/"},{"name":"Dropout","slug":"Dropout","permalink":"http://yoursite.com/tags/Dropout/"},{"name":"正则化","slug":"正则化","permalink":"http://yoursite.com/tags/正则化/"},{"name":"梯度消失","slug":"梯度消失","permalink":"http://yoursite.com/tags/梯度消失/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}]},{"title":"深度学习的学习速率曲线","slug":"22深度学习的学习速率曲线","date":"2018-04-10T12:19:32.000Z","updated":"2018-08-29T08:57:12.190Z","comments":true,"path":"2018/04/10/22深度学习的学习速率曲线/","link":"","permalink":"http://yoursite.com/2018/04/10/22深度学习的学习速率曲线/","excerpt":"","text":"深度学习的学习速率 问个问题，学习速率越高，越能快速训练好模型对不对？ 显然是错的，如果你降低学习速率，有时反而能更快的收敛模型，有时候是适得其反，留在鞍点上 可以观察损失曲线 1.你的损失函数下降的快与慢与你的模型好坏无关 2.较低的学习曲线反而能更快收敛 最后非常重要的一点，如果你的模型出现问题，首先降低学习速率","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"学习速率","slug":"学习速率","permalink":"http://yoursite.com/tags/学习速率/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}]},{"title":"深度学习的优化函数","slug":"21深度学习的优化函数","date":"2018-04-08T12:19:32.000Z","updated":"2018-09-03T01:04:56.781Z","comments":true,"path":"2018/04/08/21深度学习的优化函数/","link":"","permalink":"http://yoursite.com/2018/04/08/21深度学习的优化函数/","excerpt":"","text":"MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]} }); 深度学习的优化函数 深度学习产生的一个重要原因就是数据计算的庞大规模 梯度下降GD是一种很好的优化损失函数的策略，运行效果如下图蓝线所示 如此庞大的数据集如果运行梯度下降来最优化损失函数，将会非常的耗时 所以我们这里将会使用估计的方法，实话说这是个相当糟糕的估计，很差很差 我们随机从数据集中抽取很小的一部分数据[1,1000]的平均损失; 这里强调下随机，这个我认为是这个方法有效的核心——随机梯度下降（SGD）上图的紫色线 SGD的每次都不大准确，但是它很快所以要执行更多次数来弥补它的不精确性 针对他的精确性提高有动量梯度下降法以及ADAM 算法 动量梯度下降 V_dw=0.9*V_dw+0.1*dW \\\\ V_db=0.9*V_db+0.1*db 其实就是把前面梯度的移动平均值计算了出来，用平均值代替前一批数据的梯度 这个动量梯度下降非常有用，也能帮助模型收敛 学习速率衰减，越接近目标损失时，越是要用较小的学习速率 第一种 设置衰减率，decayrate称为衰减率，epochnum为代数，a0为初始学习率） \\alpha=\\frac{1}{1+decayrate*epochnum} 第二种是设置指数 \\alpha=0.95^{epochnum}*\\alpha0 还有手动减少，离散衰减等 AdaGrad——首选优化算法 它是SGD的优化版本 使用了动量防止过拟合 使用了学习速率衰减（自动衰减） 准确率比使用动量的SGD低一点","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"机器学习","slug":"深度学习/机器学习","permalink":"http://yoursite.com/categories/深度学习/机器学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"优化函数","slug":"优化函数","permalink":"http://yoursite.com/tags/优化函数/"},{"name":"SGD，GD","slug":"SGD，GD","permalink":"http://yoursite.com/tags/SGD，GD/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"机器学习","slug":"深度学习/机器学习","permalink":"http://yoursite.com/categories/深度学习/机器学习/"}]},{"title":"深度神经网络的基本概念","slug":"2.深度神经网络的基本概念","date":"2018-04-03T12:19:32.000Z","updated":"2018-08-31T00:37:50.906Z","comments":true,"path":"2018/04/03/2.深度神经网络的基本概念/","link":"","permalink":"http://yoursite.com/2018/04/03/2.深度神经网络的基本概念/","excerpt":"","text":"深度神经网络线性模型的局限性 假设你有28x28的图片,经过一个简单的线性变换 只有一层W和b,然后预测10类，那么你的参数个数就有28x28x10+10 = 7850个参数 GPU就是专门用来应对大型矩阵相乘，成本低，计算速度快 线性模型可以很好表示特征之间相加的关系，但是不能表示特征之间相乘的关系 线性运行非常的稳定：y=wx ,x的微小变化不会引起y的巨大变化，所以模型中使用线性计算使模型保持稳定，还要加入非线性计算，使模型能够预测非线性的情况 链式法则 深度神经网络之间能够传递导数的秘诀就是——链式法则 g(f(x))' = g'(f(x))*f(x)'反向传播 预测的过程是前向传播 模型调整参数的过程是反向传播； 核心就是用了链式法则，这里不展开讲反向传播了； 但是要记住的是反正传播相对于前向传播需要2倍的计算和空间，当你要改变模型的大小并放入内存中的时候就要考虑这个问题了 神经网络的训练过程 我们可以增加很多隐藏层来学习复杂的特征 例如图片，第一层就是线边点，第二层是一些鼻子，眼睛的轮廓…..第N层出现了人脸","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"链式法则","slug":"链式法则","permalink":"http://yoursite.com/tags/链式法则/"},{"name":"反向传播","slug":"反向传播","permalink":"http://yoursite.com/tags/反向传播/"},{"name":"线性模型优势","slug":"线性模型优势","permalink":"http://yoursite.com/tags/线性模型优势/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}]},{"title":"深度神经网络的基本概念——随机梯度下降与梯度下降","slug":"4使用梯度下降和随机梯度下降训练一个全连接网络","date":"2018-04-03T12:19:32.000Z","updated":"2018-09-03T06:21:37.338Z","comments":true,"path":"2018/04/03/4使用梯度下降和随机梯度下降训练一个全连接网络/","link":"","permalink":"http://yoursite.com/2018/04/03/4使用梯度下降和随机梯度下降训练一个全连接网络/","excerpt":"","text":"要点使用梯度下降和随机梯度下降训练一个全连接网络 数据集 数据集采用的是notMNIST数据集，这个更加像真实的数据集，不如MNIST 数据集干净，更加有难 数据集有训练集 500K ，测试集 19000，这个规模在PC机上跑很快 标签为A 到 J (10个类别) 每个图像特征 28*28 像素 原数据地 TensorFlow 的工作流程如下： 12345678910 graph TDInput[节点:输入] --&gt; GraphVar[节点:变量] --&gt; Graphoper[节点:操作] --&gt; GraphGraph[初始化Graph]--&gt;init(with graph.as_default)init--&gt;run(具体执行 session.run) run--&gt;res(拿到graph的执行结果with tf.Session graph=graph as session:) 用到的TensorFlow函数: tf.truncated_normal 从一个正态分布片段中输出随机数值，只保留两个标准差以内的值，超出的值会被弃掉重新生成 1234567def tf.truncated_normal( shape, #一个一维整数张量 或 一个Python数组。 这个值决定输出张量的形状。 mean=0.0,#一个零维张量或 类型属于dtype的Python值. 这个值决定正态分布片段的平均值 stddev=1.0,# 一个零维张量或 类型属于dtype的Python值. 这个值决定正态分布片段的标准差 dtype=tf.float32,# 输出的类型. seed=None, # 一个Python整数. 被用来为正态分布创建一个随机种子. name=None)#操作的名字 (可选参数). reduce_mean 123456tf.reduce_mean( input_tensor, # 需要求平均值的张量。应该存在数字类型。 axis=None, # 需要求平均值的维度. 如果没有设置（默认情况），所有的维度都会被减值。 keep_dims=False, # 如果为真，维持减少的维度长度为1. name=None, # 操作的名字(可选值) reduction_indices=None) #旧的**axis**参数的名字(已弃用) 可跨越维度的计算张量各元素的平均值 例子 12import numpy as npimport tensorflow as tf 1a = np.arange(20).reshape(4,5) 1234array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]]) 12# 对列求平均b = tf.reduce_mean(a,0) 1sess = tf.Session() 1sess.run(b) 1array([ 7, 8, 9, 10, 11]) 12# 对行求平均c = tf.reduce_mean(a,1) 1sess.run(c) 1array([ 2, 7, 12, 17]) softmax_cross_entropy_with_logits 123456def softmax_cross_entropy_with_logits( _sentinel=None, labels=None, # labels one-hot 向量 logits=None, # 预测值 one-hot 向量 dim=-1, name=None ): 12345def sparse_softmax_cross_entropy_with_logits( _sentinel=None, labels=None, # 原始标签 logits=None, # 预测值 one-hot 向量 name=None ): 例子 1234567891011121314151617181920212223242526272829303132333435363738394041import tensorflow as tf # 神经网络的输出logits=tf.constant([[1.0,2.0,3.0],[1.0,2.0,3.0],[1.0,2.0,3.0]]) # 对输出做softmax操作y=tf.nn.softmax(logits) # 真实数据标签，one hot形式y_=tf.constant([[0.0,0.0,1.0],[0.0,0.0,1.0],[0.0,0.0,1.0]]) # 将标签稠密化dense_y=tf.argmax(y_,1) # dense_y = [2 2 2]# 采用普通方式计算交叉熵cross_entropy = -tf.reduce_sum(y_*tf.log(y))# 使用softmax_cross_entropy_with_logits方法计算交叉熵cross_entropy2=tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))# 使用sparse_softmax_cross_entropy_with_logits方法计算交叉熵cross_entropy3=tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=dense_y)) with tf.Session() as sess: softmax=sess.run(y) c_e = sess.run(cross_entropy) c_e2 = sess.run(cross_entropy2) c_e3 = sess.run(cross_entropy3) print(\"step1:softmax result=\") print(softmax) print(\"y_ = \") print(sess.run(y_)) print(\"tf.log(y) = \") print(sess.run(tf.log(y))) print(\"dense_y =\") print(sess.run(dense_y)) print(\"step2:cross_entropy result=\") print(c_e) print(\"Function(softmax_cross_entropy_with_logits) result=\") print(c_e2) print(\"Function(sparse_softmax_cross_entropy_with_logits) result=\") print(c_e3)作者：泊牧链接：https://www.jianshu.com/p/3b084ec9ed80來源：简书简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。 np.argmax tf.argmax 返回最大值的索引，常用于softmax 后的结果预测以及ont-hot 向量的处理 1234tf.argmax(input, axis=None,# axis = 0的时候返回每一列最大值的位置索引 ;axis = 1的时候返回每一行最大值的位置索引 ;axis = 2、3、4...，即为多维张量时，同理推断 name=None, dimension=None) 代码1234567# These are all the modules we'll be using later. Make sure you can import them# before proceeding further.from __future__ import print_functionimport numpy as npimport tensorflow as tffrom six.moves import cPickle as picklefrom six.moves import range 123456789101112131415161718192021# 数据集采用的是notminist 数据集，这个更加像真实的数据集，不如mnist 数据集干净，更加有难度# 数据集有训练集 500K ，测试集 19000，这个规模在PC机上跑很快# 标签为A 到 J (10个类别)# 每个图像特征 28*28 像素# 原数据地址 http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html# 导入训练集合，用的是notMNISTpickle_file = 'notMNIST.pickle'with open(pickle_file, 'rb') as f: save = pickle.load(f) # 之前按键值对存放的，现在按键值对取出来 train_dataset = save['train_dataset'] train_labels = save['train_labels'] valid_dataset = save['valid_dataset'] valid_labels = save['valid_labels'] test_dataset = save['test_dataset'] test_labels = save['test_labels'] del save # hint to help gc free up memory print('Training set', train_dataset.shape, train_labels.shape) print('Validation set', valid_dataset.shape, valid_labels.shape) print('Test set', test_dataset.shape, test_labels.shape) Training set (20000, 28, 28) (20000,) Validation set (1000, 28, 28) (1000,) Test set (1000, 28, 28) (1000,) 12345678910111213141516# 把3维的训练数据转化为2维，即把28*28的图像像素矩阵转化成一维向量的转置 # 把标签转化为one-hot 向量image_size = 28num_labels = 10def reformat(dataset, labels): dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32) # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...] labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32) return dataset, labelstrain_dataset, train_labels = reformat(train_dataset, train_labels)valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)test_dataset, test_labels = reformat(test_dataset, test_labels)print('Training set', train_dataset.shape, train_labels.shape)print('Validation set', valid_dataset.shape, valid_labels.shape)print('Test set', test_dataset.shape, test_labels.shape) Training set (20000, 784) (20000, 10) Validation set (1000, 784) (1000, 10) Test set (1000, 784) (1000, 10) 123456789101112131415161718192021222324252627282930313233343536373839# 首先我们用梯度下降来训练多项式回归模型# 使用梯度下降的话，数目不能太多，不然计算很耗时# 我们这里用10000的数据子集train_subset = 10000graph = tf.Graph()with graph.as_default(): # 输入数据，常量 # 这是图的输入节点 tf_train_dataset = tf.constant(train_dataset[:train_subset, :]) tf_train_labels = tf.constant(train_labels[:train_subset]) tf_valid_dataset = tf.constant(valid_dataset) tf_test_dataset = tf.constant(test_dataset) # Variables.变量 # 变量是我们模型训练中需要改变的量：权重和偏移 # 权重使用truncated进行初始化，并且归一化参数，这个是为了防止梯度爆炸和消失 # 偏移全部初始化为0 weights = tf.Variable( tf.truncated_normal([image_size * image_size, num_labels])) biases = tf.Variable(tf.zeros([num_labels])) # 训练模型 # 模型很简单，特征 * 权重 + 偏移 # 使用softmax 以及交叉熵来计算损失函数 logits = tf.matmul(tf_train_dataset, weights) + biases loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) # 优化器 # GD 梯度下降. optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss) # 用softmax 预测结果 # 计算出验证集和测试集的预测结果 train_prediction = tf.nn.softmax(logits) valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases) test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases) 123import timelocaltime = time.asctime( time.localtime(time.time()) )print(localtime) Fri Aug 31 16:01:56 2018 1234567891011121314151617181920212223242526272829303132# 接下来让我们迭代运行num_steps = 801def accuracy(predictions, labels): # argmax axis = 0的时候返回每一列最大值的位置索引 # axis = 1的时候返回每一行最大值的位置索引 # axis = 2、3、4...，即为多维张量时，同理推断 # 得到true，fasle 的向量，然后统计1的个数，就是正确率 return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])with tf.Session(graph=graph) as session: # 一次性的初始化操作，把graph 中的所有变量一次性初始化好 tf.global_variables_initializer().run() print('Initialized') starttime = time.clock() print('start time:',time.asctime( time.localtime(time.time()) )) for step in range(num_steps): # 通过session.run 计算 optimizer，loss,以及 预测结果 _, l, predictions = session.run([optimizer, loss, train_prediction]) # 每一百步输入损失函数， if (step % 100 == 0): print('Loss at step %d: %f' % (step, l)) print('Training accuracy: %.1f%%' % accuracy( predictions, train_labels[:train_subset])) # 这里不能使用tensor 因为accuracy里面调用的是numpy；numpy使用的是np的ndarray #在验证集合上调用.eval() 等价于 session.run()方法，但是返回值是one-hot 的预测结果，它有图的所有信息 print('Validation accuracy: %.1f%%' % accuracy( valid_prediction.eval(), valid_labels)) print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels)) print('end time:',time.asctime( time.localtime(time.time()) )) elapsed = (time.clock() - starttime) print(\"Time used:\",elapsed) Initialized start time: Fri Aug 31 16:09:36 2018 Loss at step 0: 17.316351 Training accuracy: 9.8% Validation accuracy: 13.1% Loss at step 100: 2.382646 Training accuracy: 72.3% Validation accuracy: 73.9% Loss at step 200: 1.911187 Training accuracy: 74.5% Validation accuracy: 75.0% Loss at step 300: 1.647586 Training accuracy: 75.9% Validation accuracy: 75.3% Loss at step 400: 1.472351 Training accuracy: 76.6% Validation accuracy: 75.6% Loss at step 500: 1.344603 Training accuracy: 77.3% Validation accuracy: 75.5% Loss at step 600: 1.245956 Training accuracy: 77.9% Validation accuracy: 76.1% Loss at step 700: 1.166571 Training accuracy: 78.6% Validation accuracy: 76.0% Loss at step 800: 1.100793 Training accuracy: 78.9% Validation accuracy: 75.9% Test accuracy: 81.9% end time: Fri Aug 31 16:10:02 2018 Time used: 26.420718226680037 12345678910111213141516171819202122232425262728293031323334# 下面训练随机梯度下降# 将数据保存到常量节点，创建一个占位节点,每次用数据代入占位节点# 图的初始化操作batch_size = 128graph = tf.Graph()with graph.as_default(): # 输入数据，对于训练数据，我们使用一个占位符 # 在运行时，用一个批次 tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size)) tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels)) tf_valid_dataset = tf.constant(valid_dataset) tf_test_dataset = tf.constant(test_dataset) # 初始化变量 weights = tf.Variable( tf.truncated_normal([image_size * image_size, num_labels])) biases = tf.Variable(tf.zeros([num_labels])) # 训练计算 logits = tf.matmul(tf_train_dataset, weights) + biases loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) # 优化器还是 optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss) # 计算预测值 train_prediction = tf.nn.softmax(logits) valid_prediction = tf.nn.softmax( tf.matmul(tf_valid_dataset, weights) + biases) test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases) 12345678910111213141516171819202122232425262728num_steps = 3001with tf.Session(graph=graph) as session: tf.global_variables_initializer().run() print(\"Initialized\") starttime = time.clock() print('start time:',time.asctime( time.localtime(time.time()) )) for step in range(num_steps): # 用偏移量来选择数据，10一批次， 101个数据，当取到12批次，offset = 110%101 = 9,可以保证批次有效 # Note:我们可以用更好的随机策略选择批次 offset = (step * batch_size) % (train_labels.shape[0] - batch_size) # 创建一个批次. batch_data = train_dataset[offset:(offset + batch_size), :] batch_labels = train_labels[offset:(offset + batch_size), :] # 准备一个字典，来存放批数据，以便于 placeholder 来放数据 # 数据类型 numpy 的数组 feed_dict = &#123;tf_train_dataset : batch_data, tf_train_labels : batch_labels&#125; _, l, predictions = session.run( [optimizer, loss, train_prediction], feed_dict=feed_dict) if (step % 500 == 0): print(\"Minibatch loss at step %d: %f\" % (step, l)) print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)) print(\"Validation accuracy: %.1f%%\" % accuracy( valid_prediction.eval(), valid_labels)) print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels)) print('end time:',time.asctime( time.localtime(time.time()) )) elapsed = (time.clock() - starttime) print(\"Time used:\",elapsed) Initialized start time: Fri Aug 31 18:31:58 2018 Minibatch loss at step 0: 17.747850 Minibatch accuracy: 14.1% Validation accuracy: 14.1% Minibatch loss at step 500: 1.452345 Minibatch accuracy: 76.6% Validation accuracy: 76.6% Minibatch loss at step 1000: 1.574866 Minibatch accuracy: 76.6% Validation accuracy: 77.6% Minibatch loss at step 1500: 0.657050 Minibatch accuracy: 84.4% Validation accuracy: 77.0% Minibatch loss at step 2000: 0.710017 Minibatch accuracy: 83.6% Validation accuracy: 78.5% Minibatch loss at step 2500: 1.029955 Minibatch accuracy: 77.3% Validation accuracy: 78.4% Minibatch loss at step 3000: 0.693648 Minibatch accuracy: 84.4% Validation accuracy: 78.4% Test accuracy: 84.5% end time: Fri Aug 31 18:32:01 2018 Time used: 2.942584584146971","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://yoursite.com/tags/TensorFlow/"},{"name":"SGD","slug":"SGD","permalink":"http://yoursite.com/tags/SGD/"},{"name":"GD","slug":"GD","permalink":"http://yoursite.com/tags/GD/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}]},{"title":"多层神经网络","slug":"1.多层神经网络TensorFlow ReLUs","date":"2018-04-03T12:19:32.000Z","updated":"2018-08-30T09:58:08.940Z","comments":true,"path":"2018/04/03/1.多层神经网络TensorFlow ReLUs/","link":"","permalink":"http://yoursite.com/2018/04/03/1.多层神经网络TensorFlow ReLUs/","excerpt":"","text":"深度神经网络 神经神经网络如何对非线性模型进行预测？ 通过激活函数，激活函数可以使使神经网络适应非线性的拟合，以解决更复杂的问题 介绍个非常常用的激活函数——RELU (rectified linear unit）懒惰工程师最常用的函数 代码下面用 ReLU 函数把一个线性单层网络转变成非线性多层网络 这个代码很简单，过程如下图所示： 这段模拟了一个预测的过程 初始化好权重，偏差，以及输入特征，输入特征是个3*4矩阵 经过计算后还是一个3*3 矩阵然后经过relu 处理 relu 处理后还是个3*3的矩阵，然后经过第二次权重和偏差得到输出 1import tensorflow as tf 12C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters 12345678910111213# 定义输出层output = None# 假设隐藏层权重 4*3 的矩阵，输入4个节点，就是4个特征，输出3个节点hidden_layer_weights = [ [0.1, 0.2, 0.4], [0.4, 0.6, 0.6], [0.5, 0.9, 0.1], [0.8, 0.2, 0.8]]# 假设输出权重 3*2 的矩阵，输入3个节点，输出2个节点out_weights = [ [0.1, 0.6], [0.2, 0.1], [0.7, 0.9]] 1234567# Weights and biasesweights = [ tf.Variable(hidden_layer_weights), tf.Variable(out_weights)]biases = [ tf.Variable(tf.zeros(3)), tf.Variable(tf.zeros(2))] 12# Input 输入特征 3*4 的矩阵features = tf.Variable([[1.0, 2.0, 3.0, 4.0], [-1.0, -2.0, -3.0, -4.0], [11.0, 12.0, 13.0, 14.0]]) 12345# TODO: Create Modelhidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])hidden_layer = tf.nn.relu(hidden_layer)output = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1]) 12345# TODO: Print session results# 提个问题，为什么一定要通过sess.run 的形式？with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(sess.run(output)) 123[[ 5.11 8.440001] [ 0. 0. ] [24.010002 38.239998]]","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"TensorFlow","slug":"深度学习/TensorFlow","permalink":"http://yoursite.com/categories/深度学习/TensorFlow/"},{"name":"RELU","slug":"深度学习/TensorFlow/RELU","permalink":"http://yoursite.com/categories/深度学习/TensorFlow/RELU/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"多层神经网络","slug":"多层神经网络","permalink":"http://yoursite.com/tags/多层神经网络/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://yoursite.com/tags/TensorFlow/"},{"name":"RELU","slug":"RELU","permalink":"http://yoursite.com/tags/RELU/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"TensorFlow","slug":"深度学习/TensorFlow","permalink":"http://yoursite.com/categories/深度学习/TensorFlow/"},{"name":"RELU","slug":"深度学习/TensorFlow/RELU","permalink":"http://yoursite.com/categories/深度学习/TensorFlow/RELU/"}]},{"title":"深度神经网络的基本概念——高方差03","slug":"3.深度神经网络的基本概念——高方差问题","date":"2018-04-03T12:19:32.000Z","updated":"2018-08-31T01:41:21.377Z","comments":true,"path":"2018/04/03/3.深度神经网络的基本概念——高方差问题/","link":"","permalink":"http://yoursite.com/2018/04/03/3.深度神经网络的基本概念——高方差问题/","excerpt":"","text":"要点深度神经网络兴起的先决条件 历史久远：深度学习框架可以追溯到1980年福岛邦彦提出的新认知机 大量的训练数据：以前获取大量的数据很难，现在变得越来越容易 硬件条件：超高速计算机的诞生使得需要大量计算的深度神经网络的训练时间大大降低 紧身裤问题 如果你有一条非常合身的紧身裤，那么这个裤子就很难穿上——紧身裤用来比喻模型的模型的过拟合 解决方案常见的几种： 早停法——观察误差曲线，当出现过拟合情况时就停止训练 正则化——给参数加上一些束缚，而且不改变模型的结构 L1 正则化——降低 |权重|，使得有些接近0的参数变成0 L2 正则化——降低权重^2，把权重趋向于0，但不是0 Dropout——随机失活 Jeffery Hinton 提出的，把层与层之间的传递值，选择一定的概率把这些值置为0 所以神经网络就必须学会冗余备份各种学到的信息 这个训练过程感觉起来很低效，但是它把模型变得像多个神经网络组合成的，反正更加坚固 如果Dropout都对你的神经网络没有什么效果，那你就该换个更大的网络了 前向传播计算时，失活的值是0，但是未失活的值要除以失活概率（试想一下概率是1的情况），才可以正确计算预测值","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"过拟合，正则化","slug":"过拟合，正则化","permalink":"http://yoursite.com/tags/过拟合，正则化/"},{"name":"误差曲线，L1","slug":"误差曲线，L1","permalink":"http://yoursite.com/tags/误差曲线，L1/"},{"name":"L2","slug":"L2","permalink":"http://yoursite.com/tags/L2/"},{"name":"Dropout","slug":"Dropout","permalink":"http://yoursite.com/tags/Dropout/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}]},{"title":"模型的性能评估","slug":"20 型性能评估","date":"2018-04-02T12:19:32.000Z","updated":"2018-08-30T01:18:04.409Z","comments":true,"path":"2018/04/02/20 型性能评估/","link":"","permalink":"http://yoursite.com/2018/04/02/20 型性能评估/","excerpt":"","text":"模型性能评估 如何衡量评估你的模型的表现，问题就解决了一半了 为什么需要训练集，验证集，测试集？ 模型即使在训练集上达到了百分百的准确率，但是我们要使用的是其泛化的能力，即：预测新事物的能力 如何判断其泛化能力，我们要使用模型未记忆（训练）的数据，但是经过一次次的训练测试，测试集达到了很好的效果，但是在放入生产预测时效果又不好了，why？. 因为经过一次次的预测，模型会对测试集产生一些记忆，虽然只有一点点，但是次数多了，就会产生很大的影响. 解决方法，就是在测试集里面再划分出一个集合，用于最后的模型评估 介绍一个平台——Kaggle Challenge，机器学习的竞赛平台. 这里的数据分成三个部分，一个是训练集，一个公开的验证集，一个不公开的测试集. 有些人公开的验证集上做的很好，但是测试集上的效果不佳，很有可能是验证集测试了很多遍； 好的做法是，将公开的验证集划分出一块作为测试集 如何划分 训练集，验证集，测试集 我一般在数据集不多的情况下划分为 6:2:2 如何判断验证集有效呢，要超过30个样本（经验法则）的改变才足以说明 总共有3000个样本，效果从80%-&gt;81% 总共有3000个样本，效果从80%-&gt;80.5% 0.01*3000 = 30 说明有效 0.005 * 3000 = 15 说明有可能是噪声引起的 验证集的大小 一般要求&gt;30000个样本，那么30个有效变化，占了（30/30000 = 0.001 = 0.1%）,这样可以肉眼可见有效的变化 比起交叉验证，我更加倾向于获取更多的数据","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"机器学习","slug":"深度学习/机器学习","permalink":"http://yoursite.com/categories/深度学习/机器学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"Kaggle","slug":"Kaggle","permalink":"http://yoursite.com/tags/Kaggle/"},{"name":"模型评估","slug":"模型评估","permalink":"http://yoursite.com/tags/模型评估/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"机器学习","slug":"深度学习/机器学习","permalink":"http://yoursite.com/categories/深度学习/机器学习/"}]},{"title":"sklearn的模型保存08","slug":"test_09_保存模型_save","date":"2018-03-12T12:19:32.000Z","updated":"2018-08-29T08:58:11.142Z","comments":true,"path":"2018/03/12/test_09_保存模型_save/","link":"","permalink":"http://yoursite.com/2018/03/12/test_09_保存模型_save/","excerpt":"","text":"12from sklearn import svmfrom sklearn import datasets 标题1234clf = svm.SVC()iris = datasets.load_iris()X, y = iris.data, iris.targetclf.fit(X,y) SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto&#39;, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) 12345# method 1:pickleimport pickle# 保存模型with open('save/clf.pickle','wb') as f: pickle.dump(clf,f) 1234# 载入模型with open('save/clf.pickle','rb') as f: clf2 = pickle.load(f)clf2.predict(X[0:4]) 1234# method 2:joblib 第二种方法 from sklearn.externals import joblib# save 更加快，相对于原生的python的picklejoblib.dump(clf,'save/clf.pkl') 123# restore 载入模型clf3 = joblib.load('save/clf.pkl')clf3.predict(X[0:4]) array([0, 0, 0, 0])","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"模型保存","slug":"模型保存","permalink":"http://yoursite.com/tags/模型保存/"},{"name":"pickle","slug":"pickle","permalink":"http://yoursite.com/tags/pickle/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}]},{"title":"sklearn的学习曲线_过拟合欠拟合大杀器07","slug":"test-07-sklearn_学习曲线_过拟合欠拟合大杀器07","date":"2018-03-10T12:19:32.000Z","updated":"2018-08-24T10:59:42.940Z","comments":true,"path":"2018/03/10/test-07-sklearn_学习曲线_过拟合欠拟合大杀器07/","link":"","permalink":"http://yoursite.com/2018/03/10/test-07-sklearn_学习曲线_过拟合欠拟合大杀器07/","excerpt":"","text":"要点 正常的训练过程会出现过拟合以及欠拟合等情况，所以要识别这些情况，以下图为例： 左图欠拟合，中间正好，右图过拟合 一句话说明学习曲线learning_curve——通过增加训练数据，来观察模型的获益效果 一般学习曲线长什么样? 朴素贝叶斯大致收敛到一个较低的分数 支持向量机（SVM）是样本越多越好 我们使用手写数字数据集 分别用朴素贝叶斯和SVM 来训练模型 参考文档 验证曲线: 绘制分数以评估模型 代码123456from sklearn.model_selection import learning_curve #学习曲线模块from sklearn.datasets import load_digits #digits数据集from sklearn.svm import SVC #Support Vector Classifierimport matplotlib.pyplot as plt #可视化模块from sklearn.model_selection import ShuffleSplit # 专业的数据集分割包import numpy as np 1234# 手写数字，0-9，共1797样本，每个样本由64个特征组合（8*8），每个特征值用0-16表示digits = load_digits()X = digits.datay = digits.target 1234567# 迭代次数100次，交叉验证集分成100个，测试集比例0.2cv = ShuffleSplit(n_splits=100, test_size=0.2)#平均每一轮所得到的平均方差(共5轮，分别为样本10%、25%、50%、75%、100%),采用均方差度量损失，train_sizes, train_scores, test_scores= learning_curve( # SVM 中的参数，一个C 大间距，一个gammam, gamma 不是高斯半径 是 1/(2*sigma^2),都是越大越容易过拟合 SVC(gamma=0.0001), X, y, cv=cv, train_sizes=np.linspace(.1, 1.0, 5),n_jobs=5) 123# axis=1 对行进行操作，由于上面的cv=分成了20份，所以 train_scores 是一个20个元素的数组train_scores_mean = np.mean(train_scores, axis=1)test_scores_mean = np.mean(test_scores, axis=1) 123456789101112# train_sizes 横坐标，train_loss_mean 纵坐标，形状‘o-’,红色plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training\")plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation\")plt.xlabel(\"Training examples\")plt.ylabel(\"Loss\")# 将图标签——图例放到最佳的位置plt.legend(loc=\"best\")title = \"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"plt.title(title) 1Text(0.5,1,&apos;Learning Curves (SVM, RBF kernel, $\\\\gamma=0.001$)&apos;) 123456789# 接下来用朴素贝叶斯分类器from sklearn.naive_bayes import GaussianNB# 迭代次数100次，交叉验证集分成100个，测试集比例0.2cv = ShuffleSplit(n_splits=100, test_size=0.2)#平均每一轮所得到的平均方差(共5轮，分别为样本10%、25%、50%、75%、100%),采用均方差度量损失，train_sizes, train_scores, test_scores= learning_curve( # SVM 中的参数，一个C 大间距，一个gammam, gamma 不是高斯半径 是 1/(2*sigma^2),都是越大越容易过拟合 GaussianNB(), X, y, cv=cv, train_sizes=np.linspace(.1, 1.0, 5),n_jobs=2) 123# axis=1 对行进行操作，由于上面的cv=分成了20份，所以 train_scores 是一个20个元素的数组train_scores_mean = np.mean(train_scores, axis=1)test_scores_mean = np.mean(test_scores, axis=1) 123456789101112# train_sizes 横坐标，train_loss_mean 纵坐标，形状‘o-’,红色plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training\")plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation\")plt.xlabel(\"Training examples\")plt.ylabel(\"Loss\")# 将图标签——图例放到最佳的位置plt.legend(loc=\"best\")title = \"Learning Curves (Naive Bayes)\"plt.title(title) 1Text(0.5,1,&apos;Learning Curves (Naive Bayes)&apos;)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"分类","slug":"机器学习/sklearn/分类","permalink":"http://yoursite.com/categories/机器学习/sklearn/分类/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"学习曲线","slug":"学习曲线","permalink":"http://yoursite.com/tags/学习曲线/"},{"name":"过拟合","slug":"过拟合","permalink":"http://yoursite.com/tags/过拟合/"},{"name":"欠拟合","slug":"欠拟合","permalink":"http://yoursite.com/tags/欠拟合/"},{"name":"手写数字","slug":"手写数字","permalink":"http://yoursite.com/tags/手写数字/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"分类","slug":"机器学习/sklearn/分类","permalink":"http://yoursite.com/categories/机器学习/sklearn/分类/"}]},{"title":"sklearn的cross_validation交叉验证06","slug":"test-06-sklearn_cross_validation_的交叉验证06","date":"2018-03-09T12:19:32.000Z","updated":"2018-08-24T09:44:16.777Z","comments":true,"path":"2018/03/09/test-06-sklearn_cross_validation_的交叉验证06/","link":"","permalink":"http://yoursite.com/2018/03/09/test-06-sklearn_cross_validation_的交叉验证06/","excerpt":"","text":"要点 正常的训练过程会出现过拟合以及欠拟合等情况，所以要识别这些情况，首先你要懂得是交叉验证 本文我们用到的是利用 scikit-learn 包中的 train_test_split 可以快速划分数据集 我们使用鸢尾花数据集，首先采用0.25的划分来看看训练结果 然后我们 交叉验证（CV 缩写）)，即是设置不同（”hyperparameters(超参数)”）K近邻中的K来使模型达到最佳状态 我最常用的方法调用 cross_val_score，通过不同的划分集合来判断最好的参数取值 这里用的评估指标是accuracy以及neg_mean_squared_error更多指标 如果你有多个模型要链接成一个，把特征选择、归一化和分类合并成一个过程，那么要用 Pipeline（管道）和 FeatureUnion（特征联合）: 合并的评估器. 参考文档 3.1. 交叉验证：评估估算器的表现 代码123from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.neighbors import KNeighborsClassifier 123iris = load_iris()X = iris.dataY = iris.target 123# random_state 用来设置seed 的，seed 是确保相同划分的一种设置，比如可以这么写random_state = 3 # test_size 是测试集的划分大小，默认是0.25X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.25) 1234# K 近邻算法，常见分类算法，这里不是邻居越多越好，越少过拟合，越多欠拟合knn = KNeighborsClassifier(n_neighbors=6)knn.fit(X_train,Y_train)print(\"&gt; 没有使用交叉验证的得分： \",knn.score(X_test,Y_test)) 1&gt; 没有使用交叉验证的得分： 0.9210526315789473 12# K折交叉验证模块from sklearn.model_selection import cross_val_score 1234knn = KNeighborsClassifier(n_neighbors=6)# 把数据集分成5组，每组都是训练集合测试集scores = cross_val_score(knn,X,Y,cv = 5,scoring=\"accuracy\")print(\"&gt; 使用交叉验证的准确度： \",scores) 1&gt; 使用交叉验证的准确度： [0.96666667 1. 0.96666667 0.96666667 1. ] 12import numpy as npprint(\"&gt; 测试集的划分大小是： \",np.shape(X_test)[0]/(np.shape(X_train)[0]+np.shape(X_test)[0])) 1&gt; 测试集的划分大小是： 0.25333333333333335 12# 求下平均值print(\"&gt; 使用交叉验证的准确度平均值： \",scores.mean()) 1&gt; 使用交叉验证的准确度平均值： 0.9800000000000001 1234567891011# 现在切入正题，如何调整超参数 ，以n_neighbors为例k_scores = []k_losses = []for i in range(1,31): knn = KNeighborsClassifier(n_neighbors=i) # 把数据集分成10组，每组都是训练集合测试集 scores = cross_val_score(knn,X,Y,cv = 10,scoring=\"accuracy\") # 如果是回归问题使用neg_mean_squared_error,就是均方差，但是是负的，所以要加- loss = -cross_val_score(knn,X,Y,cv = 10,scoring=\"neg_mean_squared_error\") k_scores.append(scores.mean()) k_losses.append(loss.mean()) 12345# 画图画出来import matplotlib.pyplot as pltplt.plot(range(1,31),k_scores)plt.xlabel(\"Value of K for KNN\")plt.ylabel(\"cross-validated Accuracy\") 1Text(0,0.5,&apos;cross-validated Accuracy&apos;) 123plt.plot(range(1,31),k_losses)plt.xlabel(\"Value of K for KNN\")plt.ylabel(\"cross-validated loss\") 1Text(0,0.5,&apos;cross-validated loss&apos;) 12345678910111213# 如果你有归一化的过程，那么怎么使用交叉验证呢，你需要的pipelinefrom sklearn.pipeline import make_pipelinefrom sklearn import preprocessingk_scores = []k_losses = []for i in range(1,31): knn = make_pipeline(preprocessing.StandardScaler(), KNeighborsClassifier(n_neighbors=i)) # 把数据集分成10组，每组都是训练集合测试集 scores = cross_val_score(knn,X,Y,cv = 10,scoring=\"accuracy\") # 如果是回归问题使用neg_mean_squared_error,就是均方差，但是是负的，所以要加- loss = -cross_val_score(knn,X,Y,cv = 10,scoring=\"neg_mean_squared_error\") k_scores.append(scores.mean()) k_losses.append(loss.mean()) 12345# 画图画出来import matplotlib.pyplot as pltplt.plot(range(1,31),k_scores)plt.xlabel(\"Value of K for KNN\")plt.ylabel(\"cross-validated Accuracy\") 1Text(0,0.5,&apos;cross-validated Accuracy&apos;)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"分类","slug":"机器学习/sklearn/分类","permalink":"http://yoursite.com/categories/机器学习/sklearn/分类/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"分类","slug":"分类","permalink":"http://yoursite.com/tags/分类/"},{"name":"交叉验证","slug":"交叉验证","permalink":"http://yoursite.com/tags/交叉验证/"},{"name":"cross_validation","slug":"cross-validation","permalink":"http://yoursite.com/tags/cross-validation/"},{"name":"鸢尾花","slug":"鸢尾花","permalink":"http://yoursite.com/tags/鸢尾花/"},{"name":"K近邻","slug":"K近邻","permalink":"http://yoursite.com/tags/K近邻/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"分类","slug":"机器学习/sklearn/分类","permalink":"http://yoursite.com/categories/机器学习/sklearn/分类/"}]},{"title":"sklearn的归一化05","slug":"test-05-sklearn的归一化05","date":"2018-03-08T12:19:32.000Z","updated":"2018-08-24T09:13:57.085Z","comments":true,"path":"2018/03/08/test-05-sklearn的归一化05/","link":"","permalink":"http://yoursite.com/2018/03/08/test-05-sklearn的归一化05/","excerpt":"","text":"要点 我们通过make_classification创建每个类都是正态分布的多类数据集；如果你要聚类数据集，请使用 make_blobs(blobs 斑点的意思)；如果创建多标签分类器使用make_multilabel_classification 通过3D 图可视化了数据集的分布 将数据进行归一化 函数 scale 为数组形状的数据集的标准化提供了一个快捷实现 将特征缩放至特定范围内, 可以分别使用 MinMaxScaler 范围 [0,1] 和 MaxAbsScaler 范围 [-1,1]实现 通过SVC 模型分类 SVC的优势 高纬度空间有效 数据维度n ,样本数量时m, n&gt;&gt;m 时有效 使用训练集的子集，高效利用内存 使用不同的核函数，我常用的是线性核 以及高斯核svm.SVC(kernel=&#39;linear&#39;, C=1).fit(X_train, y_train) 比较使用归一化和没有使用归一化后的效果 代码12345678910111213141516# 标准化数据模块from sklearn import preprocessing import numpy as np# 将资料分割成train与test的模块from sklearn.model_selection import train_test_split# 生成适合做classification资料的模块from sklearn.datasets.samples_generator import make_classification # Support Vector Machine中的Support Vector Classifierfrom sklearn.svm import SVC # 可视化数据的模块import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D 1234567train_X,train_Y = make_classification(n_samples=300, n_features=3, n_redundant=0, n_informative=2, random_state=22, n_clusters_per_class=1, scale=100) 123456fig = plt.figure()# 创建一个三维的绘图工程ax = fig.add_subplot(111, projection='3d')ax.scatter(train_X[:,0],train_X[:,1],train_X[:,2],c=train_Y) 1&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x5299630&gt; 12# 分类的数据，如果只展示2维，所以可以把标签当成颜色分类出来plt.scatter(train_X[:,0],train_X[:,1],c=train_Y) 1&lt;matplotlib.collections.PathCollection at 0x13865278&gt; 12# minmax_scale 特征基于最大最小值进行缩放，范围[0,1] ,修改范围使用 minmax_scale(train_X,feature_range=(min, max))normalise_X = preprocessing.minmax_scale(train_X) 1234# 均值，对列求均值print(\"均值：\" ,normalise_X.mean(axis = 0))# 方差，对列求方差print(\"方差：\" ,normalise_X.std(axis = 0)) 12均值： [0.43318427 0.53885262 0.40536198]方差： [0.16968632 0.14949669 0.22150377] 123456# scale 最常见的归一化sclaed_X = preprocessing.scale(train_X)# 均值，对列求均值print(\"均值：\" ,sclaed_X.mean(axis = 0))# 方差，对列求方差print(\"方差：\" ,sclaed_X.std(axis = 0)) 12均值： [-4.14483263e-17 -2.82366723e-16 2.44249065e-17]方差： [1. 1. 1.] 1234567# 使用没有归一化后的数据X_train, X_test, Y_train, Y_test = train_test_split( train_X, train_Y, test_size=0.3)clf = SVC()clf.fit(X_train,Y_train)print(\"没有使用归一化后的得分：\",clf.score(X_test,Y_test)) 1没有使用归一化后的得分： 0.4666666666666667 1234567# 使用归一化后的数据X_train, X_test, Y_train, Y_test = train_test_split( normalise_X, train_Y, test_size=0.3)# 采用SVM 的分类器clf = SVC()clf.fit(X_train,Y_train)print(\"使用minmax_scale归一化后的得分：\",clf.score(X_test,Y_test)) 1使用minmax_scale归一化后的得分： 0.9333333333333333 1234567# 使用归一化后的数据X_train, X_test, Y_train, Y_test = train_test_split( sclaed_X, train_Y, test_size=0.3)# 采用SVM 的分类器clf = SVC()clf.fit(X_train,Y_train)print(\"使用scale归一化后的得分：\",clf.score(X_test,Y_test)) 1使用scale归一化后的得分： 0.9444444444444444","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"回归","slug":"机器学习/sklearn/回归","permalink":"http://yoursite.com/categories/机器学习/sklearn/回归/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"归一化","slug":"归一化","permalink":"http://yoursite.com/tags/归一化/"},{"name":"scale","slug":"scale","permalink":"http://yoursite.com/tags/scale/"},{"name":"minmax_scale","slug":"minmax-scale","permalink":"http://yoursite.com/tags/minmax-scale/"},{"name":"SVC","slug":"SVC","permalink":"http://yoursite.com/tags/SVC/"},{"name":"分类","slug":"分类","permalink":"http://yoursite.com/tags/分类/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"回归","slug":"机器学习/sklearn/回归","permalink":"http://yoursite.com/categories/机器学习/sklearn/回归/"}]},{"title":"sklearn的LinearRegression线性回归04","slug":"test_04_模型参数","date":"2018-03-06T12:19:32.000Z","updated":"2018-08-29T08:14:07.627Z","comments":true,"path":"2018/03/06/test_04_模型参数/","link":"","permalink":"http://yoursite.com/2018/03/06/test_04_模型参数/","excerpt":"","text":"参考英文API 我没有找到中文的API，如果哪位找到了，请告诉我 要点 linearmodel中的 model 中有很多常用的属性，我以LinearRegression为例子，这个model中的属性有`coef（斜率）以及intercept_（截距） 还有get_params()` （模型中所有的参数） ; 代码12from sklearn import datasetsfrom sklearn.linear_model import LinearRegression 12# 读入波士顿房价的数据boston_data = datasets.load_boston() 1234# 读入数据特征data_X = boston_data.data# 读入标签data_Y = boston_data.target 1234# 定义模型model = LinearRegression()# 训练模型model.fit(data_X,data_Y) 1LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False) 123# 线性回归模型常用参数 coef_ (斜率) intercept_(截距)# 模型的斜率model.coef_ 1234array([-1.07170557e-01, 4.63952195e-02, 2.08602395e-02, 2.68856140e+00, -1.77957587e+01, 3.80475246e+00, 7.51061703e-04, -1.47575880e+00, 3.05655038e-01, -1.23293463e-02, -9.53463555e-01, 9.39251272e-03, -5.25466633e-01]) 12# 模型的截距model.intercept_ 136.49110328036133 12# 输出模型中所有参数model.get_params() 1&#123;&apos;copy_X&apos;: True, &apos;fit_intercept&apos;: True, &apos;n_jobs&apos;: 1, &apos;normalize&apos;: False&#125; 12# 模型预测model.predict(data_X[0:2,]) 1array([30.00821269, 25.0298606 ]) 1model.score(data_X,data_Y) 10.7406077428649427","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"回归","slug":"机器学习/sklearn/回归","permalink":"http://yoursite.com/categories/机器学习/sklearn/回归/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"LinearRegression","slug":"LinearRegression","permalink":"http://yoursite.com/tags/LinearRegression/"},{"name":"linear_model","slug":"linear-model","permalink":"http://yoursite.com/tags/linear-model/"},{"name":"线性回归","slug":"线性回归","permalink":"http://yoursite.com/tags/线性回归/"},{"name":"参数","slug":"参数","permalink":"http://yoursite.com/tags/参数/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"回归","slug":"机器学习/sklearn/回归","permalink":"http://yoursite.com/categories/机器学习/sklearn/回归/"}]},{"title":"sklearn的可视化自己的数据集03","slug":"test_03_可视化_自己创建数据集","date":"2018-03-01T12:19:32.000Z","updated":"2018-08-29T08:14:33.374Z","comments":true,"path":"2018/03/01/test_03_可视化_自己创建数据集/","link":"","permalink":"http://yoursite.com/2018/03/01/test_03_可视化_自己创建数据集/","excerpt":"","text":"引入包 sklearn的数据集包，这次自己创建数据集 sklearn线性回归包 matplotlib画图包 3D 画图包 要点我们自己通过make_regression 构造数据集 构造数据集，样本个数100个，每个特征3维，标签维度1，噪音1度 sklearn 数据集 代码1234from sklearn import datasetsfrom sklearn.linear_model import LinearRegressionimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3D 1X,Y = datasets.make_regression(n_samples=100,n_features=2,n_targets=1,noise=1) 123456789fig = plt.figure()# 创建一个三维的绘图工程ax = fig.add_subplot(111, projection='3d')# 用散点图scatter 把3维数据放进去ax.scatter(X[:,0],X[:,1],X[:,2],c=Y)ax.set_xlabel('X Label')ax.set_ylabel('Y Label')ax.set_zlabel('Z Label') 12 Text(0.0937963,0.0125663,&#39;Z Label&#39;) 1plt.show() 12345678910111213# 如果数据超过3维，如何可视化呢# 通过PCA 降维 或者 LDA 就可以了# LDA 的图形模型是一个三层贝叶斯模型# 以鸢尾花数据集为例，特征是4维的# 引入PCA 包 以及 LDA(隐 Dirichlet 分配)from sklearn.decomposition import PCAfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysisiris = datasets.load_iris()X = iris.datay = iris.targettarget_names = iris.target_names 1target_names array([&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;], dtype=&#39;&lt;U10&#39;) 123# PCA将数据降低为2维pca = PCA(n_components=2)X_r = pca.fit(X).transform(X) 123# LDA 将数据降低为2维lda = LinearDiscriminantAnalysis(n_components=2)X_r2 = lda.fit(X, y).transform(X) 12print('explained variance ratio (first two components): %s' % str(pca.explained_variance_ratio_)) explained variance ratio (first two components): [0.92461621 0.05301557] 123456789plt.figure()colors = ['y', 'r', 'b']lw = 2for color, i, target_name in zip(colors, [0, 1, 2], target_names): plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, lw=lw, label=target_name)plt.legend(loc='best')plt.title('PCA of IRIS dataset') Text(0.5,1,&#39;PCA of IRIS dataset&#39;) 123456789plt.figure()for color, i, target_name in zip(colors, [0, 1, 2], target_names): plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=.8, color=color, label=target_name)plt.legend(loc='best')plt.title('LDA of IRIS dataset')plt.show()","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"数据集","slug":"数据集","permalink":"http://yoursite.com/tags/数据集/"},{"name":"可视化","slug":"可视化","permalink":"http://yoursite.com/tags/可视化/"},{"name":"3D","slug":"3D","permalink":"http://yoursite.com/tags/3D/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}]},{"title":"sklearn的波士顿房价数据集01","slug":"test_02_database_波士顿房价","date":"2018-02-01T12:19:32.000Z","updated":"2018-08-29T08:58:29.771Z","comments":true,"path":"2018/02/01/test_02_database_波士顿房价/","link":"","permalink":"http://yoursite.com/2018/02/01/test_02_database_波士顿房价/","excerpt":"","text":"引入包这里我们用到两个包 一个是sklearn 的自带的数据集合 一个是sklearn 的线性回归 12from sklearn import datasetsfrom sklearn.linear_model import LinearRegression 格式非常的固定 只要是自带的数据集，都是load_XXX() 数据特征都是.data;标签都是.target 定义模型，一般一句话超级简单 训练模型，都是XX.fit(特征，标签) 预测结果xx.predict(特征) 评价模型.score(特征，标签) 大小[0,1]越接近1越好 评价指标有很多种 sklearn API 12# 读入波士顿房价的数据boston_data = datasets.load_boston() 1234# 读入数据特征data_X = boston_data.data# 读入标签data_Y = boston_data.target 12# 定义模型model = LinearRegression() 12# 训练模型model.fit(data_X,data_Y) 输出： LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False) 1model.predict(data_X[0:4,]) 输出： array([30.00821269, 25.0298606 , 30.5702317 , 28.60814055]) 1data_Y[:4,] 输出： array([24. , 21.6, 34.7, 33.4]) 1model.score(data_X[0:4,],data_Y[:4])","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"波士顿房价","slug":"波士顿房价","permalink":"http://yoursite.com/tags/波士顿房价/"},{"name":"数据集","slug":"数据集","permalink":"http://yoursite.com/tags/数据集/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}]},{"title":"sklearn的鸢尾花瓣数据集02","slug":"test_01_花瓣_通用模型","date":"2018-02-01T12:19:32.000Z","updated":"2018-08-29T08:43:45.783Z","comments":true,"path":"2018/02/01/test_01_花瓣_通用模型/","link":"","permalink":"http://yoursite.com/2018/02/01/test_01_花瓣_通用模型/","excerpt":"","text":"鸢尾花瓣数据集引入包这里我们用到两个包，一个是sklearn 的自带的数据集合一个是sklearn 的数据集分割工具一个是k近邻分类器KNeighborsClassifierdas 格式非常的固定 只要是自带的数据集，都是load_XXX() 数据特征都是.data;标签都是.target 定义模型，一般一句话超级简单 训练模型，都是XX.fit(特征，标签) 预测结果xx.predict(特征) 评价模型.score(特征，标签) 大小[0,1]越接近1越好 评价指标有很多种 sklearn API 12345import numpy as np# 数据库可以用于TensorFlowfrom sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.neighbors import KNeighborsClassifier C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20. &quot;This module will be removed in 0.20.&quot;, DeprecationWarning) 123iris = datasets.load_iris()iris_X = iris.datairis_Y = iris.target 12# 读入花瓣数据，花瓣数据的特征是4维的，# 标签是3类的 0， 1 ， 2 1iris_X[:2,:] array([[5.1, 3.5, 1.4, 0.2], [4.9, 3. , 1.4, 0.2]]) 1iris_Y array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) 12# 将数据分成训练集 和测试集，采用train_test_split,数据会被打乱,比例就是后面的0.3X_train,X_test, Y_train,Y_test = train_test_split(iris_X,iris_Y,test_size=0.3) 1Y_train array([1, 1, 2, 1, 2, 0, 1, 0, 1, 0, 1, 2, 1, 1, 1, 0, 0, 2, 2, 0, 2, 0, 1, 1, 1, 1, 1, 0, 2, 2, 2, 1, 0, 1, 2, 2, 0, 1, 0, 2, 0, 1, 1, 1, 0, 0, 2, 2, 0, 1, 2, 1, 0, 0, 1, 2, 0, 0, 0, 0, 2, 1, 2, 0, 2, 0, 0, 2, 0, 1, 0, 1, 1, 1, 2, 2, 0, 1, 0, 2, 2, 2, 2, 0, 2, 2, 2, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 1, 0, 2, 2, 0, 2, 1, 2]) 123# 定义分类器knn = KNeighborsClassifier()knn.fit(X_train,Y_train) KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=1, n_neighbors=5, p=2, weights=&#39;uniform&#39;) 1knn.predict(X_test) array([0, 2, 1, 2, 0, 0, 2, 1, 2, 0, 2, 0, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 0, 2, 0, 1, 1, 1, 0, 2, 0, 1, 2, 1, 1, 0, 2]) 1Y_test array([0, 2, 2, 2, 0, 0, 2, 1, 2, 0, 2, 0, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 0, 2, 0, 1, 1, 1, 0, 2, 0, 1, 2, 1, 1, 0, 2]) 1knn.score(X_test,Y_test) 0.9777777777777777 一句话说明K近邻就是找到K个和目标最近的训练集中的点(最常用的是欧式距离)，用少数服从多数来预测目标。k 不是越小越好，也不是越大越好；k越小,过拟合，k越大欠拟合，所以后面要引入交叉验证来调整K实例与每一个训练点的距离（这里的复杂度为0(n)比较大，所以要使用kd树等结构kd树基本思想是，若 A 点距离 B 点非常远，B 点距离 C 点非常近， 可知 A 点与C点很遥远，不需要明确计算它们的距离。参考资料一文搞懂k近邻（k-NN）算法sklearn API","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"分类","slug":"分类","permalink":"http://yoursite.com/tags/分类/"},{"name":"鸢尾","slug":"鸢尾","permalink":"http://yoursite.com/tags/鸢尾/"},{"name":"花瓣识别花","slug":"花瓣识别花","permalink":"http://yoursite.com/tags/花瓣识别花/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}]},{"title":"notminist数据预处理示范LogisticRegression训练","slug":"notminist数据预处理示范LogisticRegression训练","date":"2018-01-06T12:19:32.000Z","updated":"2018-08-29T08:55:46.308Z","comments":true,"path":"2018/01/06/notminist数据预处理示范LogisticRegression训练/","link":"","permalink":"http://yoursite.com/2018/01/06/notminist数据预处理示范LogisticRegression训练/","excerpt":"","text":"[TOC] 要点我们这里采用notMNIST数据集，重点是对数据进行预处理，涉及内容包括： 图像展示 图像转化为训练数据 数据拆分成训练数据以及验证数据 检查数据的平衡性，shuffle后的数据是否可信 大量数据时如何快速检查重叠程度——矩阵操作 采用传统的机器学习训练一个简单的分类器 绘制学习曲线 notMNIST 入门123456789101112131415# 作业目的:用简单线性分类训练图像分类器# 准备工作:下载数据集# These are all the modules we'll be using later. Make sure you can import them# before proceeding further.from __future__ import print_functionimport imageioimport matplotlib.pyplot as pltimport numpy as npimport osimport sysimport tarfilefrom IPython.display import display, Imagefrom sklearn.linear_model import LogisticRegressionfrom six.moves.urllib.request import urlretrievefrom six.moves import cPickle as pickle 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 数据集采用的是notminist 数据集，这个更加像真实的数据集，不如mnist 数据集干净，更加有难度# 数据集有训练集 500K ，测试集 19000，这个规模在PC机上跑很快# 标签为A 到 J (10个类别)# 每个图像特征 28*28 像素# 原数据地址 http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html# 代码中的链接貌似下不了了\"\"\"\"\"\"\"\"\"\"\"\"\"将数据下载到本地\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"url = 'https://commondatastorage.googleapis.com/books1000/'last_percent_reported = Nonedata_root = '.' # Change me to store data elsewhere# 这个是用来汇报下载过程的，下载了多少def download_progress_hook(count, blockSize, totalSize): \"\"\"A hook to report the progress of a download. This is mostly intended for users with slow internet connections. Reports every 5% change in download progress. \"\"\" global last_percent_reported percent = int(count * blockSize * 100 / totalSize) if last_percent_reported != percent: if percent % 5 == 0: sys.stdout.write(\"%s%%\" % percent) sys.stdout.flush() else: sys.stdout.write(\".\") sys.stdout.flush() last_percent_reported = percent#这个是根据url 来下载文件def maybe_download(filename, expected_bytes, force=False): \"\"\"Download a file if not present, and make sure it's the right size.\"\"\" dest_filename = os.path.join(data_root, filename) if force or not os.path.exists(dest_filename): print('Attempting to download:', filename) filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook) print('\\nDownload Complete!') statinfo = os.stat(dest_filename) if statinfo.st_size == expected_bytes: print('Found and verified', dest_filename) else: raise Exception( 'Failed to verify ' + dest_filename + '. Can you get to it with a browser?') return dest_filename# 如果需要下载启用下面代码# train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)# test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"下载结束\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" 1&apos;下载结束&apos; 12train_filename = 'notMNIST_large.tar.gz'test_filename = \"notMNIST_small.tar.gz\" 12345678910111213141516171819202122232425262728293031323334\"\"\"\"\"\"\"\"\"\"\"\"\" 接下来解压这个文件，里面有目录，标记着A 到J\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"# 使用上面train_filename test_filename 来解压# 解压非常的耗时间# 解压后输出目录 train_folders test_foldersnum_classes = 10np.random.seed(133)# 解压tar.gz 文件def maybe_extract(filename, force=False): root = os.path.splitext(os.path.splitext(filename)[0])[0] # remove .tar.gz if os.path.isdir(root) and not force: # You may override by setting force=True. print('%s already present - Skipping extraction of %s.' % (root, filename)) else: print('Extracting data for %s. This may take a while. Please wait.' % root) tar = tarfile.open(filename) sys.stdout.flush() tar.extractall(data_root) tar.close() data_folders = [ os.path.join(root, d) for d in sorted(os.listdir(root)) if os.path.isdir(os.path.join(root, d))] if len(data_folders) != num_classes: raise Exception( 'Expected %d folders, one per class. Found %d instead.' % ( num_classes, len(data_folders))) print(data_folders) return data_folders# train_folders = maybe_extract(train_filename)# test_folders = maybe_extract(test_filename)\"\"\" 这个解压部分就结束了 \"\"\" 1&apos; 这个解压部分就结束了 &apos; 12345# 解压结束后可以得到这个目录列表from typing import Listtrain_folders: List[str] = ['notMNIST_large/A', 'notMNIST_large/B', 'notMNIST_large/C', 'notMNIST_large/D', 'notMNIST_large/E', 'notMNIST_large/F', 'notMNIST_large/G', 'notMNIST_large/H', 'notMNIST_large/I', 'notMNIST_large/J']test_folders = ['notMNIST_small/A', 'notMNIST_small/B', 'notMNIST_small/C', 'notMNIST_small/D', 'notMNIST_small/E', 'notMNIST_small/F', 'notMNIST_small/G', 'notMNIST_small/H', 'notMNIST_small/I', 'notMNIST_small/J'] 问题 1: 展示一些图像12345678910111213141516171819202122232425\"\"\"第一个作业 显示图像\"\"\"# 可以使用 IPython.display.# TODO:第一个作业 通过 IPython.display 展示图像# display(Image(\"notMNIST_large/A/a2F6b28udHRm.png\"))# 从每个类中随机选取几张图片展示def showimages(num_per_class:int,train_folders): plot_images = [] # 展示的图片 plt.figure() # 定义画图 index = 1 # 定义小图的索引 for _ in range(num_per_class): # 拿到list 中的元素 for folder in train_folders: # 列出所有子文件夹和子文件 image_files = os.listdir(folder) # 分成num_per_class 行，len(train_folders)列 plt.subplot(num_per_class,len(train_folders),index) img = plt.imread(os.path.join(folder, image_files[np.random.randint(len(image_files))])) plt.imshow(img,cmap='gray') index = index +1 plt.axis('off') plt.show() showimages(5,train_folders) 1234\"\"\"第二种用pillow\"\"\"from PIL import Imageim = Image.open(\"notMNIST_large/A/a2F6b28udHRm.png\")im 123\"\"\"第三种用MATLAB 的plt\"\"\"img = plt.imread(\"notMNIST_large/A/a2F6b28udHRm.png\")plt.imshow(img) 1&lt;matplotlib.image.AxesImage at 0x11e5e4a8&gt; 图像数据转换成 3D 数组12345\"\"\"\" 这里开始预处理数据集 \"\"\"\"\"# 1. 分割数据：由于数据没有办法全部放入内存，将数据集按照每个类分别放到磁盘中# 2. 组合训练数据：将每个类的数据再组合成计算机内存可放的集合# 3. 把图片处理成3维矩阵：我们把整个数值转化成 float 3维数组 (image index, x, y) 具体代码如下：dataset[num_images, :, :] = image_data# 4. 归一化：把数据处理成 均值为0 ，方差是 0.5 更好 1&apos;&quot; 这里开始预处理数据集 &apos; 12image_size = 28 # Pixel width and height.pixel_depth = 255.0 # Number of levels per pixel. 1234567891011121314151617181920212223242526272829303132333435363738394041# 1. 把每个标签的图像通过imageio.imread(image_file).astype(float)变成二维矩阵载入内存中# 2. 进行归一化处理(x-128)/255 【0,255】 -&gt;[-128,128] -&gt;[-0.5,0.5]# 3. 然后打包成[num_images, :, :]# folder = A....C..Fdef load_letter(folder, min_num_images): \"\"\"Load the data for a single letter label.\"\"\" image_files = os.listdir(folder) dataset = np.ndarray(shape=(len(image_files), image_size, image_size), dtype=np.float32) print(folder) num_images = 0 for image in image_files: image_file = os.path.join(folder, image) try: # 这部分就是进行归一化处理了 (x-128)/255 那么结果就在 【0,255】 -&gt;[-128,128] -&gt;[-0.5,0.5] image_data = (imageio.imread(image_file).astype(float) - pixel_depth / 2) / pixel_depth # 判断分辨率，大小不一致报错 if image_data.shape != (image_size, image_size): # 触发异常后，后面的代码就不会再执行 raise Exception('Unexpected image shape: %s' % str(image_data.shape)) # 这个就是最关键的代码，把图像数据加入到dataset中 dataset[num_images, :, :] = image_data num_images = num_images + 1 except (IOError, ValueError) as e: print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.') dataset = dataset[0:num_images, :, :] # 检查下是不是所有的图像都已经放好了 if num_images &lt; min_num_images: raise Exception('Many fewer images than expected: %d &lt; %d' % (num_images, min_num_images)) print('Full dataset tensor:', dataset.shape) # 计算均值 print('Mean:', np.mean(dataset)) # 计算标准差 print('Standard deviation:', np.std(dataset)) return dataset 1234567891011121314151617181920212223242526272829303132# 持久化每个标签的数据集# 重命名成\" folder.pickle\"# 使用方法为 pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)def maybe_pickle(data_folders, min_num_images_per_class, force=False): dataset_names = [] for folder in data_folders: # 把每个文件夹命名成A.pickle set_filename = folder + '.pickle' # 把数据集名称加入dataset_names dataset_names.append(set_filename) # 避免多次执行的重复操作 if os.path.exists(set_filename) and not force: # You may override by setting force=True. print('%s already present - Skipping pickling.' % set_filename) else: print('Pickling %s.' % set_filename) dataset = load_letter(folder, min_num_images_per_class) try: with open(set_filename, 'wb') as f: # 保存数据集pickle.dump # 将 obj 持久化保存到文件 tmp.txt 中 # pickle.dump(obj, open(\"tmp.txt\", \"w\")) # obj: 要持久化保存的对象； # file: 一个拥有 write() 方法的对象，并且这个 write() 方法能接收一个字符串作为参数。这个对象可以是一个以写模式打开的文件对象或者一个 # StringIO 对象，或者其他自定义的满足条件的对象。 # protocol: 这是一个可选的参数，默认为 3 ，如果设置为 4 ,则要求python 版本是3以上 # 有压缩功能 pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL) except Exception as e: print('Unable to save data to', set_filename, ':', e) return dataset_names 123# 调用方法处理数据处理,放心不会重复执行train_datasets = maybe_pickle(train_folders, 45000)test_datasets = maybe_pickle(test_folders, 1800) 1234567891011121314151617181920notMNIST_large/A.pickle already present - Skipping pickling.notMNIST_large/B.pickle already present - Skipping pickling.notMNIST_large/C.pickle already present - Skipping pickling.notMNIST_large/D.pickle already present - Skipping pickling.notMNIST_large/E.pickle already present - Skipping pickling.notMNIST_large/F.pickle already present - Skipping pickling.notMNIST_large/G.pickle already present - Skipping pickling.notMNIST_large/H.pickle already present - Skipping pickling.notMNIST_large/I.pickle already present - Skipping pickling.notMNIST_large/J.pickle already present - Skipping pickling.notMNIST_small/A.pickle already present - Skipping pickling.notMNIST_small/B.pickle already present - Skipping pickling.notMNIST_small/C.pickle already present - Skipping pickling.notMNIST_small/D.pickle already present - Skipping pickling.notMNIST_small/E.pickle already present - Skipping pickling.notMNIST_small/F.pickle already present - Skipping pickling.notMNIST_small/G.pickle already present - Skipping pickling.notMNIST_small/H.pickle already present - Skipping pickling.notMNIST_small/I.pickle already present - Skipping pickling.notMNIST_small/J.pickle already present - Skipping pickling. 问题 2: 验证归一化的图像1234567# TODO:第二个作业 通过展示归一化后的图像以及标签 提示：你可以使用matplotlib.pyplot来展示图像# 如何把pickle 载入内存中pickle_file_path = \"notMNIST_large/B.pickle\"f = open(pickle_file_path, 'rb')letter_set = pickle.load(f)index = np.random.randint(0,45000)plt.imshow(letter_set[index,:,:]) 1&lt;matplotlib.image.AxesImage at 0xf86ae10&gt; 12for pickle_file in train_datasets: print(pickle_file) 12345678910notMNIST_large/A.picklenotMNIST_large/B.picklenotMNIST_large/C.picklenotMNIST_large/D.picklenotMNIST_large/E.picklenotMNIST_large/F.picklenotMNIST_large/G.picklenotMNIST_large/H.picklenotMNIST_large/I.picklenotMNIST_large/J.pickle 12345678910111213141516171819202122232425# 多展示一些归一化后的pickle图像,# \"\"\"：num_per_class:int 每个类展示几个图像train_datasets 就是pickle 的名称的list\"\"\"def showimages_pickle(num_per_class:int,train_datasets): plot_images = [] # 展示的图片 plt.figure() # 定义画图 index = 1 # 定义小图的索引 for _ in range(num_per_class): # 拿到list 中的元素 for pickle_file in train_datasets: # 载入pickle 的文件 f = open(pickle_file, 'rb') image_data_per_class = pickle.load(f) # 分成num_per_class 行，len(train_folders)列 plt.subplot(num_per_class,len(train_datasets),index) plt.imshow( image_data_per_class[np.random.randint(np.shape(image_data_per_class)[0])],cmap='gray') index = index +1 plt.axis('off') plt.show() showimages_pickle(5,train_datasets) 123# 第二种 通过IPython.display展示index = np.random.randint(0,45000)# 但是IPython.display，我不知道如何载入图像，不知道谁懂 问题 3: 验证数据平衡123456789101112131415161718192021# TODO:第三个作业 检查各个类之间的数据是否平衡# 我认为平衡就是每个类的数据量是否一致# 可能理解错误了？# 把所有的数据集依次读出来，然后建立用柱状图表示出来labels = []num_of_labels = []# enumerate 枚举 可返回两个参数，一个是索引，一个是文件for label, pickle_file in enumerate(train_datasets): try: with open(pickle_file, 'rb') as f: letter_set = pickle.load(f) num = np.shape(letter_set)[0] num_of_labels.append(num) labels.append(label) except Exception as e: print('Unable to process data from', pickle_file, ':', e) raiseplt.bar(range(len(num_of_labels)), num_of_labels,color='rgb',tick_label=labels) 1&lt;BarContainer object of 10 artists&gt; 拆分数据集成批12345678910# 由于计算的的配置没有办法装下所有的配置，所以只能部分拿出来，组合成自己想要的数据集大小，并且要根据需要调整数据集的大小# 初始化变量 dataset (3维数组)，labels(1维数组)用于存放数据，def make_arrays(nb_rows, img_size): if nb_rows: # ndarray 创建3维数组 dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32) labels = np.ndarray(nb_rows, dtype=np.int32) else: dataset, labels = None, None return dataset, labels 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 第一个参数用于 A.pickle B.pickle# 第二个参数用于 设置训练集的大小# 比如总共选择 train_size = 200000 条数据，那么就是每个类 选择 （200000/ 10类）条数据，组合集合def merge_datasets(pickle_files, train_size, valid_size=0): num_classes = len(pickle_files) # 如果valid_size = 0，那么就是生成valid_dataset =None valid_dataset, valid_labels = make_arrays(valid_size, image_size) train_dataset, train_labels = make_arrays(train_size, image_size) # 测试集的大小 ，注意除法是 // # 验证集的大小 vsize_per_class = valid_size // num_classes tsize_per_class = train_size // num_classes start_v, start_t = 0, 0 # end_v, end_t = vsize_per_class, tsize_per_class end_l = vsize_per_class + tsize_per_class # enumerate 枚举 可返回两个参数，一个是索引，一个是文件 for label, pickle_file in enumerate(pickle_files): try: with open(pickle_file, 'rb') as f: letter_set = pickle.load(f) # let's shuffle the letters to have random validation and training set # 随机洗牌 np.random.shuffle(letter_set) # 把数据分成两部分，第一部分给 验证集valid 第二部分 测试集 train，每个都选一部分，组成最终数据集 if valid_dataset is not None: valid_letter = letter_set[:vsize_per_class, :, :] valid_dataset[start_v:end_v, :, :] = valid_letter valid_labels[start_v:end_v] = label start_v += vsize_per_class end_v += vsize_per_class train_letter = letter_set[vsize_per_class:end_l, :, :] train_dataset[start_t:end_t, :, :] = train_letter train_labels[start_t:end_t] = label start_t += tsize_per_class end_t += tsize_per_class except Exception as e: print('Unable to process data from', pickle_file, ':', e) raise return valid_dataset, valid_labels, train_dataset, train_labels 123456789101112131415161718192021222324252627from typing import Optionalfrom numpy.core.multiarray import ndarraytrain_size = 20000valid_size = 1000test_size = 1000valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets( train_datasets, train_size, valid_size)_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)print('Training:', train_dataset.shape, train_labels.shape)print('Validation:', valid_dataset.shape, valid_labels.shape)print('Testing:', test_dataset.shape, test_labels.shape)# 下面就是对做好的数据集进行洗牌def randomize(dataset, labels): # 获取label的排列 permutation = np.random.permutation(labels.shape[0]) # 用这个排列数来排序dataset 以及 label shuffled_dataset = dataset[permutation,:,:] shuffled_labels = labels[permutation] return shuffled_dataset, shuffled_labelstrain_dataset, train_labels = randomize(train_dataset, train_labels)test_dataset, test_labels = randomize(test_dataset, test_labels)valid_dataset, valid_labels = randomize(valid_dataset, valid_labels) 123Training: (20000, 28, 28) (20000,)Validation: (1000, 28, 28) (1000,)Testing: (1000, 28, 28) (1000,) 问题 4: 样本乱序与验证12345678910111213141516# Problem 4# TODO:第四个作业 说服自己洗牌后的数据很好# 接下来验证下数据平衡性，然后柱状图展示# 思路获取标签中的每一类的个数，然后画出柱状图# a = np.where(valid_labels==3) 过滤def showbalance(labels): num_of_labels = [] for label in range(10): # 这里我有个疑问，为什么label_per_class.shape 不能用？？？！！！ label_per_class = np.where(labels==label) num_of_labels.append(np.shape(label_per_class)[1]) plt.bar(range(len(num_of_labels)), num_of_labels,color='rgbcy',tick_label=range(len(num_of_labels))) return num_of_labelsshowbalance(valid_labels) 1[100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 12345678910111213141516# TODO:第四个作业 数据是否已经随机打乱，打乱后的标签是否正确# 思路随机选择num_start个起始点，然后连续选择num_pic张图片和标签然后展示def show_pic_and_label(dataset,labels,num_start,num_pic): plt.figure() index = 1; for i in range(num_start): start = np.random.randint(len(labels)-num_pic) for j in range(num_pic): plt.subplot(num_start,num_pic,index) plt.imshow(dataset[start+j,:,:],cmap='gray') plt.title(labels[start+j]) index = index +1 plt.axis('off') plt.show() show_pic_and_label(train_dataset, train_labels,5,20) 1234567891011121314151617181920212223242526# 把数据存储起来准备使用# 合成路径 os.path.join# 1.open(pickle_file, 'wb')# 2.把数据集组合成键值对# 3.pickle.dump(save, f, pickle.HIGHEST_PROTOCOL) 存放数据pickle_file = os.path.join(data_root, 'notMNIST.pickle')try: f = open(pickle_file, 'wb') save = &#123; 'train_dataset': train_dataset, 'train_labels': train_labels, 'valid_dataset': valid_dataset, 'valid_labels': valid_labels, 'test_dataset': test_dataset, 'test_labels': test_labels, &#125; pickle.dump(save, f, pickle.HIGHEST_PROTOCOL) f.close()except Exception as e: print('Unable to save data to', pickle_file, ':', e) raise# 操作系统获取文件声明，可以拿到文件大小statinfo = os.stat(pickle_file)print('Compressed pickle size:', statinfo.st_size) 1Compressed pickle size: 69080502 123import timelocaltime = time.asctime( time.localtime(time.time()) )print( time.asctime( time.localtime(time.time()) )) 1Tue Aug 28 15:21:49 2018 问题 5: 寻找重叠样本1234567891011121314151617181920212223242526272829303132333435# TODO:作业五 测量训练数据 与 交叉验证数据 间的重合程度# 重合度高容易导致过拟合问题# 计算向量间的欧式距离，判断图像是否相似，# dist = numpy.sqrt(numpy.sum(numpy.square(vec1 - vec2)))# dist = numpy.linalg.norm(vec1 - vec2)# 余弦相似性 cosine_sim = np.inner(X, Y) / np.inner(np.abs(X), np.abs(Y))def overlap(train_dataset,valid_dataset,threshold=0,num_duplicate_to_show = 5): print(\"开始---&gt;\",) num_overlap = 0 # 覆盖个数 print_pic_num = 0; # 打印图片个数 index = 1; plt.figure() for i in range(train_dataset.shape[0]): for j in range(valid_dataset.shape[0]): dist = np.linalg.norm(train_dataset[i,:,:]-valid_dataset[j,:,:]) if dist &lt;= threshold: num_overlap = num_overlap + 1 if(print_pic_num&lt;=num_duplicate_to_show): # 画train_dataset第一个图 plt.subplot(num_duplicate_to_show,2,index) plt.title(\"train_dataset\") plt.imshow(train_dataset[i,:,:],cmap='gray') index = index +1 # 画valid_dataset的一样图 plt.subplot(num_duplicate_to_show,2,index) plt.title(\"valid_dataset\") plt.imshow(valid_dataset[j,:,:],cmap='gray') print_pic_num = print_pic_num + 1 index = index +1 print(\"重合数据个数是：\",num_overlap) print(\"train_dataset重合率是：\",num_overlap/train_dataset.shape[0]) print(\"valid_dataset重合率是：\",num_overlap/valid_dataset.shape[0]) return num_overlapoverlap(train_dataset,valid_dataset,threshold=0,num_duplicate_to_show = 5) 1开始---&gt; 12C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py:107: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance. In a future version, a new instance will always be created and returned. Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance. warnings.warn(message, mplDeprecation, stacklevel=1) 123重合数据个数是： 213540train_dataset重合率是： 1.0677valid_dataset重合率是： 21.354 1213540 123456789101112131415161718192021222324252627282930313233343536373839404142def overlap_cos_matrix(source_dataset,target_dataset,threshold=1,num_duplicate_to_show = 5,selfcheck = False): print(\"开始时间：\",time.asctime( time.localtime(time.time()) )) X = np.reshape(source_dataset,(source_dataset.shape[0],-1)) Y = np.reshape(target_dataset,(target_dataset.shape[0],-1)) assert (X.shape[1] == Y.shape[1]) # 计算余弦相似度，注意这里是矩阵相乘 cosine_sim = np.dot(X, Y.T) / np.inner(np.abs(X), np.abs(Y)) assert (cosine_sim.shape == (X.shape[0],Y.shape[0])) print(cosine_sim.shape) # 判断cosine_sim中的元素是否有等于threshold的，如果有就是相等 num_source = 0 num_target = 0 print_pic_num = 1; plt.figure() print(\"矩阵计算结束：\",time.asctime( time.localtime(time.time()) )) for i in range(X.shape[0]): # 第i 行中所有重复的图片索引 dup_indices = np.where(cosine_sim[i,:] &gt;= threshold) for j in dup_indices[0]: if i==j and selfcheck: continue else: num_source = num_source +1 if(print_pic_num&lt;=num_duplicate_to_show): # 画train_dataset第一个图 plt.subplot(num_duplicate_to_show,2,print_pic_num) plt.imshow(source_dataset[i,:,:],cmap='gray') print_pic_num = print_pic_num +1 # 画valid_dataset的一样图 plt.subplot(num_duplicate_to_show,2,print_pic_num) plt.imshow(target_dataset[j,:,:],cmap='gray') print_pic_num = print_pic_num + 1 break print(\"结束时间：\",time.asctime( time.localtime(time.time()) )) print(\"重合数据个数是：\",num_source) print(\"train_dataset重合率是：\",num_source/source_dataset.shape[0]) # print(\"valid_dataset重合率是：\",num_overlap/valid_dataset.shape[0]) plt.axis('off') plt.show() return num_source overlap_cos_matrix(valid_dataset,train_dataset,threshold=1,num_duplicate_to_show = 10,selfcheck = False) 123456开始时间： Tue Aug 28 15:55:19 2018(1000, 20000)矩阵计算结束： Tue Aug 28 15:55:20 2018结束时间： Tue Aug 28 15:55:20 2018重合数据个数是： 47train_dataset重合率是： 0.047 147 问题 6: 训练一个简单的机器学习模型1234# TODO:作业六 验证集自己内部的重复图片有多少？res = overlap_cos_matrix(valid_dataset,valid_dataset,threshold=1,num_duplicate_to_show = 10,selfcheck = True)# 每个图像都和自己重叠res = res - valid_dataset.shape[0] 123456开始时间： Tue Aug 28 15:55:23 2018(1000, 1000)矩阵计算结束： Tue Aug 28 15:55:23 2018结束时间： Tue Aug 28 15:55:23 2018重合数据个数是： 16train_dataset重合率是： 0.016 12345678910111213141516# 扩展一下如何判断两个图是否相似呢，可以通过下面的代码——直方图相似度def difference(hist1,hist2): sum1 = 0 for i in range(len(hist1)): if (hist1[i] == hist2[i]): sum1 += 1 else: sum1 += 1 - float(abs(hist1[i] - hist2[i]))/ max(hist1[i], hist2[i]) return sum1/len(hist1)def similary_calculate(img1, img2): img1_reshape = np.reshape(img1, (28 * 28, 1)) img1_reshape = np.reshape(img2, (28 * 28, 1)) hist1 = list(img1_reshape.getdata()) hist2 = list(img1_reshape.getdata()) return difference(hist1, hist2) 1234567891011# TODO:作业七 创建一个消过毒的测试以及验证集 比较你们在后续作业的准确性# 思路一: 在目前基础上做 # 1. 首先trainset的pickle检查重复的，如果重复的删除# 2. validateset的pickle检查重复的，如果重复的删除# 3. test的pickle 检查重复的，重复的删除# 4. train 和 valid 中重复的，把valid 删除# 5. train 和test 中重复的，把test 删除# 思路二：从头做# 1. 把图像数据按类查找重复的，每个图像和其他所有图像匹配，重复就删除# 2. 重新执行 转换3D数据，归一化，拆分成组合成新批次 1234567891011121314# TODO:作业八 using 50, 100, 1000 and 5000 training samples，用sklearn 训练一个模型# 提示使用sklearn 的 linear_model.LogisticRegression()# 思路一：直接用学习曲线，不用上面分好的验证集# from sklearn.model_selection import learning_curve #学习曲线模块# from sklearn.linear_model import LogisticRegression# from sklearn.model_selection import ShuffleSplit # 专业的数据集分割包# # X = train_dataset.reshape(train_dataset.shape[0],-1)# y = train_labels# # train_sizes, train_scores, test_scores= learning_curve(# model, X, y,cv=5 train_sizes=[50,100,1000,5000],n_jobs=5)# # train_scores 123456789101112131415161718192021222324252627282930# 思路二：自己做预测，自己做学习曲线，用上面分好的验证集from sklearn.model_selection import learning_curve #学习曲线模块from sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import ShuffleSplit # 专业的数据集分割包# train_dataset, train_labels = randomize(train_dataset, train_labels)# test_dataset, test_labels = randomize(test_dataset, test_labels)# valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)train_size=[50,100,1000,5000,8000,15000,20000]train_scores = []valid_scores = []test_scores = []for size in train_size: # 每次循环都要新建模型，solver适合数据较多的情况，比较快 model = LogisticRegression(solver= 'saga',multi_class='multinomial') # 由于train_dataset中数据已经打乱了，所以按顺序拿 X = train_dataset[0:size,:].reshape(size,-1) y = train_labels[0:size] X_valid = valid_dataset.reshape(valid_dataset.shape[0],-1) y_valid = valid_labels X_test = test_dataset.reshape(test_dataset.shape[0],-1) y_test = test_labels model.fit(X,y) train_scores.append(model.score(X,y)) valid_scores.append(model.score(X_valid,y_valid)) test_scores.append(model.score(X_test,y_test)) 12C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge &quot;the coef_ did not converge&quot;, ConvergenceWarning) 123456789101112131415161718# axis=1 对行进行操作，由于上面的cv=分成了20份，所以 train_scores 是一个20个元素的数组# train_scores_mean = np.mean(train_scores, axis=1)# valid_scores_mean = np.mean(valid_scores, axis=1)# test_scores_mean = np.mean(test_scores, axis=1)# train_sizes 横坐标，train_loss_mean 纵坐标，形状‘o-’,红色plt.plot(train_size, train_scores, 'o-', color=\"r\", label=\"Training\")plt.plot(train_size, valid_scores, 'o-', color=\"b\", label=\"Cross-validation\")plt.plot(train_size, test_scores, 'o-', color=\"g\", label=\"test\")plt.xlabel(\"Training examples\")plt.ylabel(\"Loss\")# 将图标签——图例放到最佳的位置plt.legend(loc=\"best\")title = \"Learning Curves\"plt.title(title) 1Text(0.5,1,&apos;Learning Curves&apos;)","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"机器学习","slug":"深度学习/机器学习","permalink":"http://yoursite.com/categories/深度学习/机器学习/"},{"name":"notminist","slug":"深度学习/机器学习/notminist","permalink":"http://yoursite.com/categories/深度学习/机器学习/notminist/"},{"name":"分类","slug":"深度学习/机器学习/notminist/分类","permalink":"http://yoursite.com/categories/深度学习/机器学习/notminist/分类/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"notminist","slug":"notminist","permalink":"http://yoursite.com/tags/notminist/"},{"name":"数据清洗，数据预处理","slug":"数据清洗，数据预处理","permalink":"http://yoursite.com/tags/数据清洗，数据预处理/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"学习曲线","slug":"学习曲线","permalink":"http://yoursite.com/tags/学习曲线/"},{"name":"LogisticRegression","slug":"LogisticRegression","permalink":"http://yoursite.com/tags/LogisticRegression/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"机器学习","slug":"深度学习/机器学习","permalink":"http://yoursite.com/categories/深度学习/机器学习/"},{"name":"notminist","slug":"深度学习/机器学习/notminist","permalink":"http://yoursite.com/categories/深度学习/机器学习/notminist/"},{"name":"分类","slug":"深度学习/机器学习/notminist/分类","permalink":"http://yoursite.com/categories/深度学习/机器学习/notminist/分类/"}]},{"title":"搭建免费博客HEXO+GitHub","slug":"搭建免费博客HEXO+GitHub","date":"2017-08-21T12:19:32.000Z","updated":"2018-08-29T08:49:46.693Z","comments":true,"path":"2017/08/21/搭建免费博客HEXO+GitHub/","link":"","permalink":"http://yoursite.com/2017/08/21/搭建免费博客HEXO+GitHub/","excerpt":"","text":"原料 Node.js ——简单的说就是运行在服务端的 JavaScript, 所以这个构建后端服务的 . Nexo —— 一款基于Node.js的静态博客框架，这个是台湾人创建的 GitHub Pages —— GitHub全球最大的Gay站，我们用的是GitHub中的仓库，因为它是免费的.. 步骤创建Github仓库 创建Github仓库安装Git 安装Git 可以直接安装GitHub Desktop创建SSH秘钥 创建SSH秘钥 配置git的用户名和邮箱 右键打开gitBash,12git config --global user.name &quot;你的GitHub用户名&quot;git config --global user.email &quot;你的GitHub注册邮箱&quot; 看本地有秘钥没 1cd ~/. ssh 本地创建秘钥 1ssh-keygen -t rsa -C &quot;your_email@example.com&quot; 中间会提示你是否需要设置密码，可输可不输 上传到GitHub复制公钥到系统粘贴板中 1clip &lt; ~/.ssh/id_rsa.pub +测试 1ssh -T git@github.com 如果提示你 yes /no? 那就是yes 安装Node.js 安装Node.js 下载地址:官网 安装Hexo 安装Hexo 安装nexo 1npm install hexo-cli -g 安装部署工具 1npm install hexo-deployer-git --save 初始化 1hexo init 启动 12345hexo generatehexo server可以一句话hexo g -d 常用命令现在来介绍常用的Hexo 命令 1234567891011121314151617npm install hexo -g #安装Hexonpm update hexo -g #升级 hexo init #初始化博客命令简写hexo n &quot;我的博客&quot; == hexo new &quot;我的博客&quot; #新建文章hexo g == hexo generate #生成hexo s == hexo server #启动服务预览hexo d == hexo deploy #部署hexo server #Hexo会监视文件变动并自动更新，无须重启服务器hexo server -s #静态模式hexo server -p 5000 #更改端口hexo server -i 192.168.1.1 #自定义 IPhexo clean #清除缓存，若是网页正常情况下可以忽略这条命令刚刚的三个命令依次是新建一篇博客文章、生成网页、在本地预览的操作。 浏览器访问地址http://localhost:4000 上传到Github 上传到Github 配置根目录下 _config.yml 1234deploy:type: gitrepository: git@github.com:username/username.github.io.gitbranch: master 上传github 123hexo clean hexo g hexo d 最后一条命令是部署到github访问 http://xxxx.github.io 绑定域名 绑定域名 更换主题 更换主题 Themes 官网 如果你不喜欢Hexo默认的主题，可以更换不同的主题，主题传送门：Themes 我自己使用的是Next主题，可以在blog目录中的themes文件夹中查看你自己主题是什么。现在把默认主题更改成Next主题，在blog目录中（就是命令行的位置处于blog目录）打开命令行输入：1git clone https://github.com/iissnan/hexo-theme-next themes/next 发布文章 发布文章 命令行 1hexo n &quot;博客名字&quot; 直接做好markdown 文件，放在nexo的source_posts目录下 1source\\_posts OSS服务器 OSS服务器 为啥要用对象存储服务（Object Storage Service，简称OSS）？ 1.费用很低，甚至免费 2.图片加载快 我用的是阿里云的OSS如果你不会写markdown 如果你不会写markdownAPI 参考","categories":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/categories/其他/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"},{"name":"搭建博客","slug":"搭建博客","permalink":"http://yoursite.com/tags/搭建博客/"},{"name":"免费","slug":"免费","permalink":"http://yoursite.com/tags/免费/"}],"keywords":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/categories/其他/"}]}]}