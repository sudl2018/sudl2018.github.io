{"meta":{"title":"the blog of sudoli","subtitle":null,"description":null,"author":"sudoli","url":"http://yoursite.com"},"pages":[],"posts":[{"title":"所有可能的完整二叉树","slug":"329.找出所有可能的完整二叉树","date":"2018-08-26T06:06:32.000Z","updated":"2018-08-27T00:57:45.319Z","comments":true,"path":"2018/08/26/329.找出所有可能的完整二叉树/","link":"","permalink":"http://yoursite.com/2018/08/26/329.找出所有可能的完整二叉树/","excerpt":"","text":"问题找出所有可能的完整二叉树All Possible Full Binary Trees 完整二叉树是一类二叉树，其中每个结点恰好有 0 或 2 个子结点。 返回包含 N 个结点的所有可能完整二叉树的列表。 答案的每个元素都是一个可能树的根结点。 答案中每个树的每个结点都必须有 node.val=0。 你可以按任何顺序返回树的最终列表。 注意完整二叉树不是完全二叉树 思路一（深度优先搜索尝试所有可能节点，深拷贝二叉树） 找出所有的树，那必须有很多根节点，返回是个根节点的list 我不知道结果有多少颗树，所以不能一下子列出所有根节点，所以我首先考虑： 从根节点构造树的所有合理可能 dfs——从根节点开始，每次递归 都是把左右子节点放入树中的某个节点，所有可能节点都要尝试（for 循环），但是使用for 循环要考虑重复访问的问题 把每一种可能进行深拷贝后添加到结果中 代码一12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; List&lt;TreeNode&gt; res = new LinkedList&lt;&gt;(); int N = 0; public List&lt;TreeNode&gt; allPossibleFBT(int N) &#123; if(N%2==0)&#123; return res; &#125; this.N = N; TreeNode root = new TreeNode(0); LinkedList&lt;TreeNode&gt; nodes = new LinkedList&lt;&gt;(); nodes.add(root); helper(1,nodes,root); return res; &#125; public void helper(int count,LinkedList&lt;TreeNode&gt; nodes,TreeNode root)&#123; if(count == N)&#123; if(root != null)&#123; // 要用拷贝，因为原来root 会被我不停的改变 TreeNode new_root = new TreeNode(0); copyTree(root,new_root); res.add(new_root); &#125; return; &#125; if(count &gt; N)&#123; return; &#125; //这里一定要用poll ,因为链表中的元素已经被使用过的节点，如果再次被访问就会出现重复访问 //如果节点一旦被访问过，那就要移除 while(nodes.size()!=0)&#123; TreeNode node = nodes.poll(); node.left = new TreeNode(0); node.right = new TreeNode(0); // 将剩余没有访问的节点都放入list中，以便于下次访问 LinkedList&lt;TreeNode&gt; nodelist = new LinkedList&lt;&gt;(nodes); nodelist.add(node.left); nodelist.add(node.right); helper(count+2,nodelist,root); node.left = null; node.right = null; &#125; &#125; //深拷贝一颗树 public void copyTree(TreeNode node,TreeNode new_node)&#123; if(node == null)&#123; return; &#125; if(node.left != null)&#123; new_node.left = new TreeNode(0); copyTree(node.left,new_node.left); &#125; if(node.right != null)&#123; new_node.right = new TreeNode(0); copyTree(node.right,new_node.right); &#125; &#125; &#125; 思路二（动态规划） 完整二叉树的左右子树也都是完整二叉树 左子树的所有可能和右子树的所有可能的组合就是答案 从节点数是1开始构造，一直构造到N 代码一1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public List&lt;TreeNode&gt; allPossibleFBT(int N) &#123; // 构造存储记忆，存放所有的完整二叉树 Map&lt;Integer,List&lt;TreeNode&gt;&gt; mem = new HashMap&lt;&gt;(); // 初始化，从0开始到N，可以不需要从0开始，为什么不需要从0开始，下面解释 for(int i=0;i&lt;=N;i++)&#123; mem.put(i,new LinkedList&lt;TreeNode&gt;()); &#125; // 初始化N=0没有节点，N=1一个根节点 mem.get(1).add(new TreeNode(0)); // 从N=2开始推算 for(int i=2;i&lt;=N;i++)&#123; // 左子树可能节点个数,但是由于是完整二叉树，左子树节点数不可能是0个节点，所以mem不加入N==0也可以,左子树节点个数范围[0,i-1] for(int l=0;l&lt;i;l++)&#123; List&lt;TreeNode&gt; lefts = mem.get(l); List&lt;TreeNode&gt; rights = mem.get(i-l-1);//总共i个节点，左边l个，一个根节点1，那么右边i-l-1 // 左右子树的组合就是新树,for 循环确保了左子树，右子树不可能是空，；例如 N == 0，for就不执行了 for(TreeNode left:lefts)&#123; for(TreeNode right:rights)&#123; TreeNode root = new TreeNode(0); // 完整二叉树——左右子树都是完整二叉树 root.left = left; root.right = right; mem.get(i).add(root); &#125; &#125; &#125; &#125; return mem.get(N); &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"},{"name":"二叉树","slug":"算法/二叉树","permalink":"http://yoursite.com/categories/算法/二叉树/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/tags/算法/"},{"name":"二叉树","slug":"二叉树","permalink":"http://yoursite.com/tags/二叉树/"},{"name":"完整二叉树","slug":"完整二叉树","permalink":"http://yoursite.com/tags/完整二叉树/"},{"name":"medium","slug":"medium","permalink":"http://yoursite.com/tags/medium/"},{"name":"894","slug":"894","permalink":"http://yoursite.com/tags/894/"}],"keywords":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"},{"name":"二叉树","slug":"算法/二叉树","permalink":"http://yoursite.com/categories/算法/二叉树/"}]},{"title":"深度学习的学习速率曲线","slug":"22深度学习的学习速率曲线","date":"2018-04-10T12:19:32.000Z","updated":"2018-08-29T08:57:12.190Z","comments":true,"path":"2018/04/10/22深度学习的学习速率曲线/","link":"","permalink":"http://yoursite.com/2018/04/10/22深度学习的学习速率曲线/","excerpt":"","text":"深度学习的学习速率 问个问题，学习速率越高，越能快速训练好模型对不对？ 显然是错的，如果你降低学习速率，有时反而能更快的收敛模型，有时候是适得其反，留在鞍点上 可以观察损失曲线 1.你的损失函数下降的快与慢与你的模型好坏无关 2.较低的学习曲线反而能更快收敛 最后非常重要的一点，如果你的模型出现问题，首先降低学习速率","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"学习速率","slug":"学习速率","permalink":"http://yoursite.com/tags/学习速率/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}]},{"title":"深度学习的优化函数","slug":"21深度学习的优化函数","date":"2018-04-08T12:19:32.000Z","updated":"2018-08-30T09:44:16.063Z","comments":true,"path":"2018/04/08/21深度学习的优化函数/","link":"","permalink":"http://yoursite.com/2018/04/08/21深度学习的优化函数/","excerpt":"","text":"MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]} }); 深度学习的优化函数 深度学习产生的一个重要原因就是数据计算的庞大规模 梯度下降GD是一种很好的优化损失函数的策略，运行效果如下图蓝线所示 如此庞大的数据集如果运行梯度下降来最优化损失函数，将会非常的耗时 所以我们这里将会使用估计的方法，实话说这是个相当糟糕的估计，很差很差 我们随机从数据集中抽取很小的一部分数据[1,1000]的平均损失; 这里强调下随机，这个我认为是这个方法有效的核心——随机梯度下降（SGD）上图的紫色线 SGD的每次都不大准确，但是它很快所以要执行更多次数来弥补它的不精确性 针对他的精确性提高有动量梯度下降法以及ADAM 算法 动量梯度下降 12V_dw=0.9*V_dw+0.1*dWV_db=0.9*V_db+0.1*db 其实就是把前面梯度的移动平均值计算了出来，用平均值代替前一批数据的梯度 这个动量梯度下降非常有用，也能帮助模型收敛 学习速率衰减，越接近目标损失时，越是要用较小的学习速率 第一种 设置衰减率，decayrate称为衰减率，epochnum为代数，a0为初始学习率） $$ \\alpha=\\frac{1}{1+decayrateepochnum} $$ 第二种是设置指数 $$ \\alpha=0.95^{epochnum}\\alpha0 $$ 还有手动减少，离散衰减等 AdaGrad——首选优化算法 它是SGD的优化版本 使用了动量防止过拟合 使用了学习速率衰减（自动衰减） 准确率比使用动量的SGD低一点","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"机器学习","slug":"深度学习/机器学习","permalink":"http://yoursite.com/categories/深度学习/机器学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"优化函数","slug":"优化函数","permalink":"http://yoursite.com/tags/优化函数/"},{"name":"SGD，GD","slug":"SGD，GD","permalink":"http://yoursite.com/tags/SGD，GD/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"机器学习","slug":"深度学习/机器学习","permalink":"http://yoursite.com/categories/深度学习/机器学习/"}]},{"title":"深度神经网络的基本概念","slug":"2.深度神经网络的基本概念","date":"2018-04-03T12:19:32.000Z","updated":"2018-08-30T09:58:46.379Z","comments":true,"path":"2018/04/03/2.深度神经网络的基本概念/","link":"","permalink":"http://yoursite.com/2018/04/03/2.深度神经网络的基本概念/","excerpt":"","text":"深度神经网络线性模型的局限性 假设你有28x28的图片,经过一个简单的线性变换 只有一层W和b,然后预测10类，那么你的参数个数就有28x28x10+10 = 7850个参数 GPU就是专门用来应对大型矩阵相乘，成本低，计算速度快 线性模型可以很好表示特征之间相加的关系，但是不能表示特征之间相乘的关系 线性运行非常的稳定：y=wx ,x的微小变化不会引起y的巨大变化，所以模型中使用线性计算使模型保持稳定，还要加入非线性计算，使模型能够预测非线性的情况 链式法则 深度神经网络之间能够传递导数的秘诀就是——链式法则 1g(f(x))&apos; = g&apos;(f(x))*f(x)&apos; 反向传播 预测的过程是前向传播 模型调整参数的过程是反向传播； 核心就是用了链式法则，这里不展开讲反向传播了； 但是要记住的是反正传播相对于前向传播需要2倍的计算和空间，当你要改变模型的大小并放入内存中的时候就要考虑这个问题了 神经网络的训练过程 我们可以增加很多隐藏层来学习复杂的特征 例如图片，第一层就是线边点，第二层是一些鼻子，眼睛的轮廓…..第N层出现了人脸","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"链式法则","slug":"链式法则","permalink":"http://yoursite.com/tags/链式法则/"},{"name":"反向传播","slug":"反向传播","permalink":"http://yoursite.com/tags/反向传播/"},{"name":"线性模型优势","slug":"线性模型优势","permalink":"http://yoursite.com/tags/线性模型优势/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}]},{"title":"多层神经网络","slug":"1.多层神经网络TensorFlow ReLUs","date":"2018-04-03T12:19:32.000Z","updated":"2018-08-30T09:58:08.940Z","comments":true,"path":"2018/04/03/1.多层神经网络TensorFlow ReLUs/","link":"","permalink":"http://yoursite.com/2018/04/03/1.多层神经网络TensorFlow ReLUs/","excerpt":"","text":"深度神经网络 神经神经网络如何对非线性模型进行预测？ 通过激活函数，激活函数可以使使神经网络适应非线性的拟合，以解决更复杂的问题 介绍个非常常用的激活函数——RELU (rectified linear unit）懒惰工程师最常用的函数 代码下面用 ReLU 函数把一个线性单层网络转变成非线性多层网络 这个代码很简单，过程如下图所示： 这段模拟了一个预测的过程 初始化好权重，偏差，以及输入特征，输入特征是个3*4矩阵 经过计算后还是一个3*3 矩阵然后经过relu 处理 relu 处理后还是个3*3的矩阵，然后经过第二次权重和偏差得到输出 1import tensorflow as tf 12C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters 12345678910111213# 定义输出层output = None# 假设隐藏层权重 4*3 的矩阵，输入4个节点，就是4个特征，输出3个节点hidden_layer_weights = [ [0.1, 0.2, 0.4], [0.4, 0.6, 0.6], [0.5, 0.9, 0.1], [0.8, 0.2, 0.8]]# 假设输出权重 3*2 的矩阵，输入3个节点，输出2个节点out_weights = [ [0.1, 0.6], [0.2, 0.1], [0.7, 0.9]] 1234567# Weights and biasesweights = [ tf.Variable(hidden_layer_weights), tf.Variable(out_weights)]biases = [ tf.Variable(tf.zeros(3)), tf.Variable(tf.zeros(2))] 12# Input 输入特征 3*4 的矩阵features = tf.Variable([[1.0, 2.0, 3.0, 4.0], [-1.0, -2.0, -3.0, -4.0], [11.0, 12.0, 13.0, 14.0]]) 12345# TODO: Create Modelhidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])hidden_layer = tf.nn.relu(hidden_layer)output = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1]) 12345# TODO: Print session results# 提个问题，为什么一定要通过sess.run 的形式？with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(sess.run(output)) 123[[ 5.11 8.440001] [ 0. 0. ] [24.010002 38.239998]]","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"TensorFlow","slug":"深度学习/TensorFlow","permalink":"http://yoursite.com/categories/深度学习/TensorFlow/"},{"name":"RELU","slug":"深度学习/TensorFlow/RELU","permalink":"http://yoursite.com/categories/深度学习/TensorFlow/RELU/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"多层神经网络","slug":"多层神经网络","permalink":"http://yoursite.com/tags/多层神经网络/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://yoursite.com/tags/TensorFlow/"},{"name":"RELU","slug":"RELU","permalink":"http://yoursite.com/tags/RELU/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"TensorFlow","slug":"深度学习/TensorFlow","permalink":"http://yoursite.com/categories/深度学习/TensorFlow/"},{"name":"RELU","slug":"深度学习/TensorFlow/RELU","permalink":"http://yoursite.com/categories/深度学习/TensorFlow/RELU/"}]},{"title":"模型的性能评估","slug":"20 型性能评估","date":"2018-04-02T12:19:32.000Z","updated":"2018-08-30T01:18:04.409Z","comments":true,"path":"2018/04/02/20 型性能评估/","link":"","permalink":"http://yoursite.com/2018/04/02/20 型性能评估/","excerpt":"","text":"模型性能评估 如何衡量评估你的模型的表现，问题就解决了一半了 为什么需要训练集，验证集，测试集？ 模型即使在训练集上达到了百分百的准确率，但是我们要使用的是其泛化的能力，即：预测新事物的能力 如何判断其泛化能力，我们要使用模型未记忆（训练）的数据，但是经过一次次的训练测试，测试集达到了很好的效果，但是在放入生产预测时效果又不好了，why？. 因为经过一次次的预测，模型会对测试集产生一些记忆，虽然只有一点点，但是次数多了，就会产生很大的影响. 解决方法，就是在测试集里面再划分出一个集合，用于最后的模型评估 介绍一个平台——Kaggle Challenge，机器学习的竞赛平台. 这里的数据分成三个部分，一个是训练集，一个公开的验证集，一个不公开的测试集. 有些人公开的验证集上做的很好，但是测试集上的效果不佳，很有可能是验证集测试了很多遍； 好的做法是，将公开的验证集划分出一块作为测试集 如何划分 训练集，验证集，测试集 我一般在数据集不多的情况下划分为 6:2:2 如何判断验证集有效呢，要超过30个样本（经验法则）的改变才足以说明 总共有3000个样本，效果从80%-&gt;81% 总共有3000个样本，效果从80%-&gt;80.5% 0.01*3000 = 30 说明有效 0.005 * 3000 = 15 说明有可能是噪声引起的 验证集的大小 一般要求&gt;30000个样本，那么30个有效变化，占了（30/30000 = 0.001 = 0.1%）,这样可以肉眼可见有效的变化 比起交叉验证，我更加倾向于获取更多的数据","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"机器学习","slug":"深度学习/机器学习","permalink":"http://yoursite.com/categories/深度学习/机器学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"Kaggle","slug":"Kaggle","permalink":"http://yoursite.com/tags/Kaggle/"},{"name":"模型评估","slug":"模型评估","permalink":"http://yoursite.com/tags/模型评估/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"机器学习","slug":"深度学习/机器学习","permalink":"http://yoursite.com/categories/深度学习/机器学习/"}]},{"title":"sklearn的模型保存08","slug":"test_09_保存模型_save","date":"2018-03-12T12:19:32.000Z","updated":"2018-08-29T08:58:11.142Z","comments":true,"path":"2018/03/12/test_09_保存模型_save/","link":"","permalink":"http://yoursite.com/2018/03/12/test_09_保存模型_save/","excerpt":"","text":"12from sklearn import svmfrom sklearn import datasets 标题1234clf = svm.SVC()iris = datasets.load_iris()X, y = iris.data, iris.targetclf.fit(X,y) SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&apos;ovr&apos;, degree=3, gamma=&apos;auto&apos;, kernel=&apos;rbf&apos;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) 12345# method 1:pickleimport pickle# 保存模型with open('save/clf.pickle','wb') as f: pickle.dump(clf,f) 1234# 载入模型with open('save/clf.pickle','rb') as f: clf2 = pickle.load(f)clf2.predict(X[0:4]) 1234# method 2:joblib 第二种方法 from sklearn.externals import joblib# save 更加快，相对于原生的python的picklejoblib.dump(clf,'save/clf.pkl') 123# restore 载入模型clf3 = joblib.load('save/clf.pkl')clf3.predict(X[0:4]) array([0, 0, 0, 0])","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"模型保存","slug":"模型保存","permalink":"http://yoursite.com/tags/模型保存/"},{"name":"pickle","slug":"pickle","permalink":"http://yoursite.com/tags/pickle/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}]},{"title":"sklearn的学习曲线_过拟合欠拟合大杀器07","slug":"test-07-sklearn_学习曲线_过拟合欠拟合大杀器07","date":"2018-03-10T12:19:32.000Z","updated":"2018-08-24T10:59:42.940Z","comments":true,"path":"2018/03/10/test-07-sklearn_学习曲线_过拟合欠拟合大杀器07/","link":"","permalink":"http://yoursite.com/2018/03/10/test-07-sklearn_学习曲线_过拟合欠拟合大杀器07/","excerpt":"","text":"要点 正常的训练过程会出现过拟合以及欠拟合等情况，所以要识别这些情况，以下图为例： 左图欠拟合，中间正好，右图过拟合 一句话说明学习曲线learning_curve——通过增加训练数据，来观察模型的获益效果 一般学习曲线长什么样? 朴素贝叶斯大致收敛到一个较低的分数 支持向量机（SVM）是样本越多越好 我们使用手写数字数据集 分别用朴素贝叶斯和SVM 来训练模型 参考文档 验证曲线: 绘制分数以评估模型 代码123456from sklearn.model_selection import learning_curve #学习曲线模块from sklearn.datasets import load_digits #digits数据集from sklearn.svm import SVC #Support Vector Classifierimport matplotlib.pyplot as plt #可视化模块from sklearn.model_selection import ShuffleSplit # 专业的数据集分割包import numpy as np 1234# 手写数字，0-9，共1797样本，每个样本由64个特征组合（8*8），每个特征值用0-16表示digits = load_digits()X = digits.datay = digits.target 1234567# 迭代次数100次，交叉验证集分成100个，测试集比例0.2cv = ShuffleSplit(n_splits=100, test_size=0.2)#平均每一轮所得到的平均方差(共5轮，分别为样本10%、25%、50%、75%、100%),采用均方差度量损失，train_sizes, train_scores, test_scores= learning_curve( # SVM 中的参数，一个C 大间距，一个gammam, gamma 不是高斯半径 是 1/(2*sigma^2),都是越大越容易过拟合 SVC(gamma=0.0001), X, y, cv=cv, train_sizes=np.linspace(.1, 1.0, 5),n_jobs=5) 123# axis=1 对行进行操作，由于上面的cv=分成了20份，所以 train_scores 是一个20个元素的数组train_scores_mean = np.mean(train_scores, axis=1)test_scores_mean = np.mean(test_scores, axis=1) 123456789101112# train_sizes 横坐标，train_loss_mean 纵坐标，形状‘o-’,红色plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training\")plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation\")plt.xlabel(\"Training examples\")plt.ylabel(\"Loss\")# 将图标签——图例放到最佳的位置plt.legend(loc=\"best\")title = \"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"plt.title(title) 1Text(0.5,1,&apos;Learning Curves (SVM, RBF kernel, $\\\\gamma=0.001$)&apos;) 123456789# 接下来用朴素贝叶斯分类器from sklearn.naive_bayes import GaussianNB# 迭代次数100次，交叉验证集分成100个，测试集比例0.2cv = ShuffleSplit(n_splits=100, test_size=0.2)#平均每一轮所得到的平均方差(共5轮，分别为样本10%、25%、50%、75%、100%),采用均方差度量损失，train_sizes, train_scores, test_scores= learning_curve( # SVM 中的参数，一个C 大间距，一个gammam, gamma 不是高斯半径 是 1/(2*sigma^2),都是越大越容易过拟合 GaussianNB(), X, y, cv=cv, train_sizes=np.linspace(.1, 1.0, 5),n_jobs=2) 123# axis=1 对行进行操作，由于上面的cv=分成了20份，所以 train_scores 是一个20个元素的数组train_scores_mean = np.mean(train_scores, axis=1)test_scores_mean = np.mean(test_scores, axis=1) 123456789101112# train_sizes 横坐标，train_loss_mean 纵坐标，形状‘o-’,红色plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training\")plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation\")plt.xlabel(\"Training examples\")plt.ylabel(\"Loss\")# 将图标签——图例放到最佳的位置plt.legend(loc=\"best\")title = \"Learning Curves (Naive Bayes)\"plt.title(title) 1Text(0.5,1,&apos;Learning Curves (Naive Bayes)&apos;)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"分类","slug":"机器学习/sklearn/分类","permalink":"http://yoursite.com/categories/机器学习/sklearn/分类/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"学习曲线","slug":"学习曲线","permalink":"http://yoursite.com/tags/学习曲线/"},{"name":"过拟合","slug":"过拟合","permalink":"http://yoursite.com/tags/过拟合/"},{"name":"欠拟合","slug":"欠拟合","permalink":"http://yoursite.com/tags/欠拟合/"},{"name":"手写数字","slug":"手写数字","permalink":"http://yoursite.com/tags/手写数字/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"分类","slug":"机器学习/sklearn/分类","permalink":"http://yoursite.com/categories/机器学习/sklearn/分类/"}]},{"title":"sklearn的cross_validation交叉验证06","slug":"test-06-sklearn_cross_validation_的交叉验证06","date":"2018-03-09T12:19:32.000Z","updated":"2018-08-24T09:44:16.777Z","comments":true,"path":"2018/03/09/test-06-sklearn_cross_validation_的交叉验证06/","link":"","permalink":"http://yoursite.com/2018/03/09/test-06-sklearn_cross_validation_的交叉验证06/","excerpt":"","text":"要点 正常的训练过程会出现过拟合以及欠拟合等情况，所以要识别这些情况，首先你要懂得是交叉验证 本文我们用到的是利用 scikit-learn 包中的 train_test_split 可以快速划分数据集 我们使用鸢尾花数据集，首先采用0.25的划分来看看训练结果 然后我们 交叉验证（CV 缩写）)，即是设置不同（”hyperparameters(超参数)”）K近邻中的K来使模型达到最佳状态 我最常用的方法调用 cross_val_score，通过不同的划分集合来判断最好的参数取值 这里用的评估指标是accuracy以及neg_mean_squared_error更多指标 如果你有多个模型要链接成一个，把特征选择、归一化和分类合并成一个过程，那么要用 Pipeline（管道）和 FeatureUnion（特征联合）: 合并的评估器. 参考文档 3.1. 交叉验证：评估估算器的表现 代码123from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.neighbors import KNeighborsClassifier 123iris = load_iris()X = iris.dataY = iris.target 123# random_state 用来设置seed 的，seed 是确保相同划分的一种设置，比如可以这么写random_state = 3 # test_size 是测试集的划分大小，默认是0.25X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.25) 1234# K 近邻算法，常见分类算法，这里不是邻居越多越好，越少过拟合，越多欠拟合knn = KNeighborsClassifier(n_neighbors=6)knn.fit(X_train,Y_train)print(\"&gt; 没有使用交叉验证的得分： \",knn.score(X_test,Y_test)) 1&gt; 没有使用交叉验证的得分： 0.9210526315789473 12# K折交叉验证模块from sklearn.model_selection import cross_val_score 1234knn = KNeighborsClassifier(n_neighbors=6)# 把数据集分成5组，每组都是训练集合测试集scores = cross_val_score(knn,X,Y,cv = 5,scoring=\"accuracy\")print(\"&gt; 使用交叉验证的准确度： \",scores) 1&gt; 使用交叉验证的准确度： [0.96666667 1. 0.96666667 0.96666667 1. ] 12import numpy as npprint(\"&gt; 测试集的划分大小是： \",np.shape(X_test)[0]/(np.shape(X_train)[0]+np.shape(X_test)[0])) 1&gt; 测试集的划分大小是： 0.25333333333333335 12# 求下平均值print(\"&gt; 使用交叉验证的准确度平均值： \",scores.mean()) 1&gt; 使用交叉验证的准确度平均值： 0.9800000000000001 1234567891011# 现在切入正题，如何调整超参数 ，以n_neighbors为例k_scores = []k_losses = []for i in range(1,31): knn = KNeighborsClassifier(n_neighbors=i) # 把数据集分成10组，每组都是训练集合测试集 scores = cross_val_score(knn,X,Y,cv = 10,scoring=\"accuracy\") # 如果是回归问题使用neg_mean_squared_error,就是均方差，但是是负的，所以要加- loss = -cross_val_score(knn,X,Y,cv = 10,scoring=\"neg_mean_squared_error\") k_scores.append(scores.mean()) k_losses.append(loss.mean()) 12345# 画图画出来import matplotlib.pyplot as pltplt.plot(range(1,31),k_scores)plt.xlabel(\"Value of K for KNN\")plt.ylabel(\"cross-validated Accuracy\") 1Text(0,0.5,&apos;cross-validated Accuracy&apos;) 123plt.plot(range(1,31),k_losses)plt.xlabel(\"Value of K for KNN\")plt.ylabel(\"cross-validated loss\") 1Text(0,0.5,&apos;cross-validated loss&apos;) 12345678910111213# 如果你有归一化的过程，那么怎么使用交叉验证呢，你需要的pipelinefrom sklearn.pipeline import make_pipelinefrom sklearn import preprocessingk_scores = []k_losses = []for i in range(1,31): knn = make_pipeline(preprocessing.StandardScaler(), KNeighborsClassifier(n_neighbors=i)) # 把数据集分成10组，每组都是训练集合测试集 scores = cross_val_score(knn,X,Y,cv = 10,scoring=\"accuracy\") # 如果是回归问题使用neg_mean_squared_error,就是均方差，但是是负的，所以要加- loss = -cross_val_score(knn,X,Y,cv = 10,scoring=\"neg_mean_squared_error\") k_scores.append(scores.mean()) k_losses.append(loss.mean()) 12345# 画图画出来import matplotlib.pyplot as pltplt.plot(range(1,31),k_scores)plt.xlabel(\"Value of K for KNN\")plt.ylabel(\"cross-validated Accuracy\") 1Text(0,0.5,&apos;cross-validated Accuracy&apos;)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"分类","slug":"机器学习/sklearn/分类","permalink":"http://yoursite.com/categories/机器学习/sklearn/分类/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"分类","slug":"分类","permalink":"http://yoursite.com/tags/分类/"},{"name":"交叉验证","slug":"交叉验证","permalink":"http://yoursite.com/tags/交叉验证/"},{"name":"cross_validation","slug":"cross-validation","permalink":"http://yoursite.com/tags/cross-validation/"},{"name":"鸢尾花","slug":"鸢尾花","permalink":"http://yoursite.com/tags/鸢尾花/"},{"name":"K近邻","slug":"K近邻","permalink":"http://yoursite.com/tags/K近邻/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"分类","slug":"机器学习/sklearn/分类","permalink":"http://yoursite.com/categories/机器学习/sklearn/分类/"}]},{"title":"sklearn的归一化05","slug":"test-05-sklearn的归一化05","date":"2018-03-08T12:19:32.000Z","updated":"2018-08-24T09:13:57.085Z","comments":true,"path":"2018/03/08/test-05-sklearn的归一化05/","link":"","permalink":"http://yoursite.com/2018/03/08/test-05-sklearn的归一化05/","excerpt":"","text":"要点 我们通过make_classification创建每个类都是正态分布的多类数据集；如果你要聚类数据集，请使用 make_blobs(blobs 斑点的意思)；如果创建多标签分类器使用make_multilabel_classification 通过3D 图可视化了数据集的分布 将数据进行归一化 函数 scale 为数组形状的数据集的标准化提供了一个快捷实现 将特征缩放至特定范围内, 可以分别使用 MinMaxScaler 范围 [0,1] 和 MaxAbsScaler 范围 [-1,1]实现 通过SVC 模型分类 SVC的优势 高纬度空间有效 数据维度n ,样本数量时m, n&gt;&gt;m 时有效 使用训练集的子集，高效利用内存 使用不同的核函数，我常用的是线性核 以及高斯核svm.SVC(kernel=&#39;linear&#39;, C=1).fit(X_train, y_train) 比较使用归一化和没有使用归一化后的效果 代码12345678910111213141516# 标准化数据模块from sklearn import preprocessing import numpy as np# 将资料分割成train与test的模块from sklearn.model_selection import train_test_split# 生成适合做classification资料的模块from sklearn.datasets.samples_generator import make_classification # Support Vector Machine中的Support Vector Classifierfrom sklearn.svm import SVC # 可视化数据的模块import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D 1234567train_X,train_Y = make_classification(n_samples=300, n_features=3, n_redundant=0, n_informative=2, random_state=22, n_clusters_per_class=1, scale=100) 123456fig = plt.figure()# 创建一个三维的绘图工程ax = fig.add_subplot(111, projection='3d')ax.scatter(train_X[:,0],train_X[:,1],train_X[:,2],c=train_Y) 1&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x5299630&gt; 12# 分类的数据，如果只展示2维，所以可以把标签当成颜色分类出来plt.scatter(train_X[:,0],train_X[:,1],c=train_Y) 1&lt;matplotlib.collections.PathCollection at 0x13865278&gt; 12# minmax_scale 特征基于最大最小值进行缩放，范围[0,1] ,修改范围使用 minmax_scale(train_X,feature_range=(min, max))normalise_X = preprocessing.minmax_scale(train_X) 1234# 均值，对列求均值print(\"均值：\" ,normalise_X.mean(axis = 0))# 方差，对列求方差print(\"方差：\" ,normalise_X.std(axis = 0)) 12均值： [0.43318427 0.53885262 0.40536198]方差： [0.16968632 0.14949669 0.22150377] 123456# scale 最常见的归一化sclaed_X = preprocessing.scale(train_X)# 均值，对列求均值print(\"均值：\" ,sclaed_X.mean(axis = 0))# 方差，对列求方差print(\"方差：\" ,sclaed_X.std(axis = 0)) 12均值： [-4.14483263e-17 -2.82366723e-16 2.44249065e-17]方差： [1. 1. 1.] 1234567# 使用没有归一化后的数据X_train, X_test, Y_train, Y_test = train_test_split( train_X, train_Y, test_size=0.3)clf = SVC()clf.fit(X_train,Y_train)print(\"没有使用归一化后的得分：\",clf.score(X_test,Y_test)) 1没有使用归一化后的得分： 0.4666666666666667 1234567# 使用归一化后的数据X_train, X_test, Y_train, Y_test = train_test_split( normalise_X, train_Y, test_size=0.3)# 采用SVM 的分类器clf = SVC()clf.fit(X_train,Y_train)print(\"使用minmax_scale归一化后的得分：\",clf.score(X_test,Y_test)) 1使用minmax_scale归一化后的得分： 0.9333333333333333 1234567# 使用归一化后的数据X_train, X_test, Y_train, Y_test = train_test_split( sclaed_X, train_Y, test_size=0.3)# 采用SVM 的分类器clf = SVC()clf.fit(X_train,Y_train)print(\"使用scale归一化后的得分：\",clf.score(X_test,Y_test)) 1使用scale归一化后的得分： 0.9444444444444444","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"回归","slug":"机器学习/sklearn/回归","permalink":"http://yoursite.com/categories/机器学习/sklearn/回归/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"归一化","slug":"归一化","permalink":"http://yoursite.com/tags/归一化/"},{"name":"scale","slug":"scale","permalink":"http://yoursite.com/tags/scale/"},{"name":"minmax_scale","slug":"minmax-scale","permalink":"http://yoursite.com/tags/minmax-scale/"},{"name":"SVC","slug":"SVC","permalink":"http://yoursite.com/tags/SVC/"},{"name":"分类","slug":"分类","permalink":"http://yoursite.com/tags/分类/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"回归","slug":"机器学习/sklearn/回归","permalink":"http://yoursite.com/categories/机器学习/sklearn/回归/"}]},{"title":"sklearn的LinearRegression线性回归04","slug":"test_04_模型参数","date":"2018-03-06T12:19:32.000Z","updated":"2018-08-29T08:14:07.627Z","comments":true,"path":"2018/03/06/test_04_模型参数/","link":"","permalink":"http://yoursite.com/2018/03/06/test_04_模型参数/","excerpt":"","text":"参考英文API 我没有找到中文的API，如果哪位找到了，请告诉我 要点 linear_model中的 model 中有很多常用的属性，我以LinearRegression为例子，这个model中的属性有coef_ （斜率）以及 intercept_（截距） 还有get_params() （模型中所有的参数） ; 代码12from sklearn import datasetsfrom sklearn.linear_model import LinearRegression 12# 读入波士顿房价的数据boston_data = datasets.load_boston() 1234# 读入数据特征data_X = boston_data.data# 读入标签data_Y = boston_data.target 1234# 定义模型model = LinearRegression()# 训练模型model.fit(data_X,data_Y) 1LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False) 123# 线性回归模型常用参数 coef_ (斜率) intercept_(截距)# 模型的斜率model.coef_ 1234array([-1.07170557e-01, 4.63952195e-02, 2.08602395e-02, 2.68856140e+00, -1.77957587e+01, 3.80475246e+00, 7.51061703e-04, -1.47575880e+00, 3.05655038e-01, -1.23293463e-02, -9.53463555e-01, 9.39251272e-03, -5.25466633e-01]) 12# 模型的截距model.intercept_ 136.49110328036133 12# 输出模型中所有参数model.get_params() 1&#123;&apos;copy_X&apos;: True, &apos;fit_intercept&apos;: True, &apos;n_jobs&apos;: 1, &apos;normalize&apos;: False&#125; 12# 模型预测model.predict(data_X[0:2,]) 1array([30.00821269, 25.0298606 ]) 1model.score(data_X,data_Y) 10.7406077428649427","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"回归","slug":"机器学习/sklearn/回归","permalink":"http://yoursite.com/categories/机器学习/sklearn/回归/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"LinearRegression","slug":"LinearRegression","permalink":"http://yoursite.com/tags/LinearRegression/"},{"name":"linear_model","slug":"linear-model","permalink":"http://yoursite.com/tags/linear-model/"},{"name":"线性回归","slug":"线性回归","permalink":"http://yoursite.com/tags/线性回归/"},{"name":"参数","slug":"参数","permalink":"http://yoursite.com/tags/参数/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"回归","slug":"机器学习/sklearn/回归","permalink":"http://yoursite.com/categories/机器学习/sklearn/回归/"}]},{"title":"sklearn的可视化自己的数据集03","slug":"test_03_可视化_自己创建数据集","date":"2018-03-01T12:19:32.000Z","updated":"2018-08-29T08:14:33.374Z","comments":true,"path":"2018/03/01/test_03_可视化_自己创建数据集/","link":"","permalink":"http://yoursite.com/2018/03/01/test_03_可视化_自己创建数据集/","excerpt":"","text":"引入包 sklearn的数据集包，这次自己创建数据集 sklearn线性回归包 matplotlib画图包 3D 画图包 要点我们自己通过make_regression 构造数据集 构造数据集，样本个数100个，每个特征3维，标签维度1，噪音1度 sklearn 数据集 代码1234from sklearn import datasetsfrom sklearn.linear_model import LinearRegressionimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3D 1X,Y = datasets.make_regression(n_samples=100,n_features=2,n_targets=1,noise=1) 123456789fig = plt.figure()# 创建一个三维的绘图工程ax = fig.add_subplot(111, projection='3d')# 用散点图scatter 把3维数据放进去ax.scatter(X[:,0],X[:,1],X[:,2],c=Y)ax.set_xlabel('X Label')ax.set_ylabel('Y Label')ax.set_zlabel('Z Label') 12 Text(0.0937963,0.0125663,&apos;Z Label&apos;) 1plt.show() 12345678910111213# 如果数据超过3维，如何可视化呢# 通过PCA 降维 或者 LDA 就可以了# LDA 的图形模型是一个三层贝叶斯模型# 以鸢尾花数据集为例，特征是4维的# 引入PCA 包 以及 LDA(隐 Dirichlet 分配)from sklearn.decomposition import PCAfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysisiris = datasets.load_iris()X = iris.datay = iris.targettarget_names = iris.target_names 1target_names array([&apos;setosa&apos;, &apos;versicolor&apos;, &apos;virginica&apos;], dtype=&apos;&lt;U10&apos;) 123# PCA将数据降低为2维pca = PCA(n_components=2)X_r = pca.fit(X).transform(X) 123# LDA 将数据降低为2维lda = LinearDiscriminantAnalysis(n_components=2)X_r2 = lda.fit(X, y).transform(X) 12print('explained variance ratio (first two components): %s' % str(pca.explained_variance_ratio_)) explained variance ratio (first two components): [0.92461621 0.05301557] 123456789plt.figure()colors = ['y', 'r', 'b']lw = 2for color, i, target_name in zip(colors, [0, 1, 2], target_names): plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, lw=lw, label=target_name)plt.legend(loc='best')plt.title('PCA of IRIS dataset') Text(0.5,1,&apos;PCA of IRIS dataset&apos;) 123456789plt.figure()for color, i, target_name in zip(colors, [0, 1, 2], target_names): plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=.8, color=color, label=target_name)plt.legend(loc='best')plt.title('LDA of IRIS dataset')plt.show()","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"数据集","slug":"数据集","permalink":"http://yoursite.com/tags/数据集/"},{"name":"可视化","slug":"可视化","permalink":"http://yoursite.com/tags/可视化/"},{"name":"3D","slug":"3D","permalink":"http://yoursite.com/tags/3D/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}]},{"title":"sklearn的鸢尾花瓣数据集02","slug":"test_01_花瓣_通用模型","date":"2018-02-01T12:19:32.000Z","updated":"2018-08-29T08:43:45.783Z","comments":true,"path":"2018/02/01/test_01_花瓣_通用模型/","link":"","permalink":"http://yoursite.com/2018/02/01/test_01_花瓣_通用模型/","excerpt":"","text":"鸢尾花瓣数据集引入包这里我们用到两个包，一个是sklearn 的自带的数据集合一个是sklearn 的数据集分割工具一个是k近邻分类器KNeighborsClassifierdas 格式非常的固定 只要是自带的数据集，都是load_XXX() 数据特征都是.data;标签都是.target 定义模型，一般一句话超级简单 训练模型，都是XX.fit(特征，标签) 预测结果xx.predict(特征) 评价模型.score(特征，标签) 大小[0,1]越接近1越好 评价指标有很多种 sklearn API 12345import numpy as np# 数据库可以用于TensorFlowfrom sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.neighbors import KNeighborsClassifier C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20. &quot;This module will be removed in 0.20.&quot;, DeprecationWarning) 123iris = datasets.load_iris()iris_X = iris.datairis_Y = iris.target 12# 读入花瓣数据，花瓣数据的特征是4维的，# 标签是3类的 0， 1 ， 2 1iris_X[:2,:] array([[5.1, 3.5, 1.4, 0.2], [4.9, 3. , 1.4, 0.2]]) 1iris_Y array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) 12# 将数据分成训练集 和测试集，采用train_test_split,数据会被打乱,比例就是后面的0.3X_train,X_test, Y_train,Y_test = train_test_split(iris_X,iris_Y,test_size=0.3) 1Y_train array([1, 1, 2, 1, 2, 0, 1, 0, 1, 0, 1, 2, 1, 1, 1, 0, 0, 2, 2, 0, 2, 0, 1, 1, 1, 1, 1, 0, 2, 2, 2, 1, 0, 1, 2, 2, 0, 1, 0, 2, 0, 1, 1, 1, 0, 0, 2, 2, 0, 1, 2, 1, 0, 0, 1, 2, 0, 0, 0, 0, 2, 1, 2, 0, 2, 0, 0, 2, 0, 1, 0, 1, 1, 1, 2, 2, 0, 1, 0, 2, 2, 2, 2, 0, 2, 2, 2, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 1, 0, 2, 2, 0, 2, 1, 2]) 123# 定义分类器knn = KNeighborsClassifier()knn.fit(X_train,Y_train) KNeighborsClassifier(algorithm=&apos;auto&apos;, leaf_size=30, metric=&apos;minkowski&apos;, metric_params=None, n_jobs=1, n_neighbors=5, p=2, weights=&apos;uniform&apos;) 1knn.predict(X_test) array([0, 2, 1, 2, 0, 0, 2, 1, 2, 0, 2, 0, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 0, 2, 0, 1, 1, 1, 0, 2, 0, 1, 2, 1, 1, 0, 2]) 1Y_test array([0, 2, 2, 2, 0, 0, 2, 1, 2, 0, 2, 0, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 0, 2, 0, 1, 1, 1, 0, 2, 0, 1, 2, 1, 1, 0, 2]) 1knn.score(X_test,Y_test) 0.9777777777777777 一句话说明K近邻就是找到K个和目标最近的训练集中的点(最常用的是欧式距离)，用少数服从多数来预测目标。k 不是越小越好，也不是越大越好；k越小,过拟合，k越大欠拟合，所以后面要引入交叉验证来调整K实例与每一个训练点的距离（这里的复杂度为0(n)比较大，所以要使用kd树等结构kd树基本思想是，若 A 点距离 B 点非常远，B 点距离 C 点非常近， 可知 A 点与C点很遥远，不需要明确计算它们的距离。参考资料一文搞懂k近邻（k-NN）算法sklearn API","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"分类","slug":"分类","permalink":"http://yoursite.com/tags/分类/"},{"name":"鸢尾","slug":"鸢尾","permalink":"http://yoursite.com/tags/鸢尾/"},{"name":"花瓣识别花","slug":"花瓣识别花","permalink":"http://yoursite.com/tags/花瓣识别花/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}]},{"title":"sklearn的波士顿房价数据集01","slug":"test_02_database_波士顿房价","date":"2018-02-01T12:19:32.000Z","updated":"2018-08-29T08:58:29.771Z","comments":true,"path":"2018/02/01/test_02_database_波士顿房价/","link":"","permalink":"http://yoursite.com/2018/02/01/test_02_database_波士顿房价/","excerpt":"","text":"引入包这里我们用到两个包 一个是sklearn 的自带的数据集合 一个是sklearn 的线性回归 12from sklearn import datasetsfrom sklearn.linear_model import LinearRegression 格式非常的固定 只要是自带的数据集，都是load_XXX() 数据特征都是.data;标签都是.target 定义模型，一般一句话超级简单 训练模型，都是XX.fit(特征，标签) 预测结果xx.predict(特征) 评价模型.score(特征，标签) 大小[0,1]越接近1越好 评价指标有很多种 sklearn API 12# 读入波士顿房价的数据boston_data = datasets.load_boston() 1234# 读入数据特征data_X = boston_data.data# 读入标签data_Y = boston_data.target 12# 定义模型model = LinearRegression() 12# 训练模型model.fit(data_X,data_Y) 输出： LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False) 1model.predict(data_X[0:4,]) 输出： array([30.00821269, 25.0298606 , 30.5702317 , 28.60814055]) 1data_Y[:4,] 输出： array([24. , 21.6, 34.7, 33.4]) 1model.score(data_X[0:4,],data_Y[:4])","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"波士顿房价","slug":"波士顿房价","permalink":"http://yoursite.com/tags/波士顿房价/"},{"name":"数据集","slug":"数据集","permalink":"http://yoursite.com/tags/数据集/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}]},{"title":"notminist数据预处理示范LogisticRegression训练","slug":"notminist数据预处理示范LogisticRegression训练","date":"2018-01-06T12:19:32.000Z","updated":"2018-08-29T08:55:46.308Z","comments":true,"path":"2018/01/06/notminist数据预处理示范LogisticRegression训练/","link":"","permalink":"http://yoursite.com/2018/01/06/notminist数据预处理示范LogisticRegression训练/","excerpt":"","text":"[TOC] 要点我们这里采用notMNIST数据集，重点是对数据进行预处理，涉及内容包括： 图像展示 图像转化为训练数据 数据拆分成训练数据以及验证数据 检查数据的平衡性，shuffle后的数据是否可信 大量数据时如何快速检查重叠程度——矩阵操作 采用传统的机器学习训练一个简单的分类器 绘制学习曲线 notMNIST 入门123456789101112131415# 作业目的:用简单线性分类训练图像分类器# 准备工作:下载数据集# These are all the modules we'll be using later. Make sure you can import them# before proceeding further.from __future__ import print_functionimport imageioimport matplotlib.pyplot as pltimport numpy as npimport osimport sysimport tarfilefrom IPython.display import display, Imagefrom sklearn.linear_model import LogisticRegressionfrom six.moves.urllib.request import urlretrievefrom six.moves import cPickle as pickle 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 数据集采用的是notminist 数据集，这个更加像真实的数据集，不如mnist 数据集干净，更加有难度# 数据集有训练集 500K ，测试集 19000，这个规模在PC机上跑很快# 标签为A 到 J (10个类别)# 每个图像特征 28*28 像素# 原数据地址 http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html# 代码中的链接貌似下不了了\"\"\"\"\"\"\"\"\"\"\"\"\"将数据下载到本地\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"url = 'https://commondatastorage.googleapis.com/books1000/'last_percent_reported = Nonedata_root = '.' # Change me to store data elsewhere# 这个是用来汇报下载过程的，下载了多少def download_progress_hook(count, blockSize, totalSize): \"\"\"A hook to report the progress of a download. This is mostly intended for users with slow internet connections. Reports every 5% change in download progress. \"\"\" global last_percent_reported percent = int(count * blockSize * 100 / totalSize) if last_percent_reported != percent: if percent % 5 == 0: sys.stdout.write(\"%s%%\" % percent) sys.stdout.flush() else: sys.stdout.write(\".\") sys.stdout.flush() last_percent_reported = percent#这个是根据url 来下载文件def maybe_download(filename, expected_bytes, force=False): \"\"\"Download a file if not present, and make sure it's the right size.\"\"\" dest_filename = os.path.join(data_root, filename) if force or not os.path.exists(dest_filename): print('Attempting to download:', filename) filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook) print('\\nDownload Complete!') statinfo = os.stat(dest_filename) if statinfo.st_size == expected_bytes: print('Found and verified', dest_filename) else: raise Exception( 'Failed to verify ' + dest_filename + '. Can you get to it with a browser?') return dest_filename# 如果需要下载启用下面代码# train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)# test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"下载结束\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" 1&apos;下载结束&apos; 12train_filename = 'notMNIST_large.tar.gz'test_filename = \"notMNIST_small.tar.gz\" 12345678910111213141516171819202122232425262728293031323334\"\"\"\"\"\"\"\"\"\"\"\"\" 接下来解压这个文件，里面有目录，标记着A 到J\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"# 使用上面train_filename test_filename 来解压# 解压非常的耗时间# 解压后输出目录 train_folders test_foldersnum_classes = 10np.random.seed(133)# 解压tar.gz 文件def maybe_extract(filename, force=False): root = os.path.splitext(os.path.splitext(filename)[0])[0] # remove .tar.gz if os.path.isdir(root) and not force: # You may override by setting force=True. print('%s already present - Skipping extraction of %s.' % (root, filename)) else: print('Extracting data for %s. This may take a while. Please wait.' % root) tar = tarfile.open(filename) sys.stdout.flush() tar.extractall(data_root) tar.close() data_folders = [ os.path.join(root, d) for d in sorted(os.listdir(root)) if os.path.isdir(os.path.join(root, d))] if len(data_folders) != num_classes: raise Exception( 'Expected %d folders, one per class. Found %d instead.' % ( num_classes, len(data_folders))) print(data_folders) return data_folders# train_folders = maybe_extract(train_filename)# test_folders = maybe_extract(test_filename)\"\"\" 这个解压部分就结束了 \"\"\" 1&apos; 这个解压部分就结束了 &apos; 12345# 解压结束后可以得到这个目录列表from typing import Listtrain_folders: List[str] = ['notMNIST_large/A', 'notMNIST_large/B', 'notMNIST_large/C', 'notMNIST_large/D', 'notMNIST_large/E', 'notMNIST_large/F', 'notMNIST_large/G', 'notMNIST_large/H', 'notMNIST_large/I', 'notMNIST_large/J']test_folders = ['notMNIST_small/A', 'notMNIST_small/B', 'notMNIST_small/C', 'notMNIST_small/D', 'notMNIST_small/E', 'notMNIST_small/F', 'notMNIST_small/G', 'notMNIST_small/H', 'notMNIST_small/I', 'notMNIST_small/J'] 问题 1: 展示一些图像12345678910111213141516171819202122232425\"\"\"第一个作业 显示图像\"\"\"# 可以使用 IPython.display.# TODO:第一个作业 通过 IPython.display 展示图像# display(Image(\"notMNIST_large/A/a2F6b28udHRm.png\"))# 从每个类中随机选取几张图片展示def showimages(num_per_class:int,train_folders): plot_images = [] # 展示的图片 plt.figure() # 定义画图 index = 1 # 定义小图的索引 for _ in range(num_per_class): # 拿到list 中的元素 for folder in train_folders: # 列出所有子文件夹和子文件 image_files = os.listdir(folder) # 分成num_per_class 行，len(train_folders)列 plt.subplot(num_per_class,len(train_folders),index) img = plt.imread(os.path.join(folder, image_files[np.random.randint(len(image_files))])) plt.imshow(img,cmap='gray') index = index +1 plt.axis('off') plt.show() showimages(5,train_folders) 1234\"\"\"第二种用pillow\"\"\"from PIL import Imageim = Image.open(\"notMNIST_large/A/a2F6b28udHRm.png\")im 123\"\"\"第三种用MATLAB 的plt\"\"\"img = plt.imread(\"notMNIST_large/A/a2F6b28udHRm.png\")plt.imshow(img) 1&lt;matplotlib.image.AxesImage at 0x11e5e4a8&gt; 图像数据转换成 3D 数组12345\"\"\"\" 这里开始预处理数据集 \"\"\"\"\"# 1. 分割数据：由于数据没有办法全部放入内存，将数据集按照每个类分别放到磁盘中# 2. 组合训练数据：将每个类的数据再组合成计算机内存可放的集合# 3. 把图片处理成3维矩阵：我们把整个数值转化成 float 3维数组 (image index, x, y) 具体代码如下：dataset[num_images, :, :] = image_data# 4. 归一化：把数据处理成 均值为0 ，方差是 0.5 更好 1&apos;&quot; 这里开始预处理数据集 &apos; 12image_size = 28 # Pixel width and height.pixel_depth = 255.0 # Number of levels per pixel. 1234567891011121314151617181920212223242526272829303132333435363738394041# 1. 把每个标签的图像通过imageio.imread(image_file).astype(float)变成二维矩阵载入内存中# 2. 进行归一化处理(x-128)/255 【0,255】 -&gt;[-128,128] -&gt;[-0.5,0.5]# 3. 然后打包成[num_images, :, :]# folder = A....C..Fdef load_letter(folder, min_num_images): \"\"\"Load the data for a single letter label.\"\"\" image_files = os.listdir(folder) dataset = np.ndarray(shape=(len(image_files), image_size, image_size), dtype=np.float32) print(folder) num_images = 0 for image in image_files: image_file = os.path.join(folder, image) try: # 这部分就是进行归一化处理了 (x-128)/255 那么结果就在 【0,255】 -&gt;[-128,128] -&gt;[-0.5,0.5] image_data = (imageio.imread(image_file).astype(float) - pixel_depth / 2) / pixel_depth # 判断分辨率，大小不一致报错 if image_data.shape != (image_size, image_size): # 触发异常后，后面的代码就不会再执行 raise Exception('Unexpected image shape: %s' % str(image_data.shape)) # 这个就是最关键的代码，把图像数据加入到dataset中 dataset[num_images, :, :] = image_data num_images = num_images + 1 except (IOError, ValueError) as e: print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.') dataset = dataset[0:num_images, :, :] # 检查下是不是所有的图像都已经放好了 if num_images &lt; min_num_images: raise Exception('Many fewer images than expected: %d &lt; %d' % (num_images, min_num_images)) print('Full dataset tensor:', dataset.shape) # 计算均值 print('Mean:', np.mean(dataset)) # 计算标准差 print('Standard deviation:', np.std(dataset)) return dataset 1234567891011121314151617181920212223242526272829303132# 持久化每个标签的数据集# 重命名成\" folder.pickle\"# 使用方法为 pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)def maybe_pickle(data_folders, min_num_images_per_class, force=False): dataset_names = [] for folder in data_folders: # 把每个文件夹命名成A.pickle set_filename = folder + '.pickle' # 把数据集名称加入dataset_names dataset_names.append(set_filename) # 避免多次执行的重复操作 if os.path.exists(set_filename) and not force: # You may override by setting force=True. print('%s already present - Skipping pickling.' % set_filename) else: print('Pickling %s.' % set_filename) dataset = load_letter(folder, min_num_images_per_class) try: with open(set_filename, 'wb') as f: # 保存数据集pickle.dump # 将 obj 持久化保存到文件 tmp.txt 中 # pickle.dump(obj, open(\"tmp.txt\", \"w\")) # obj: 要持久化保存的对象； # file: 一个拥有 write() 方法的对象，并且这个 write() 方法能接收一个字符串作为参数。这个对象可以是一个以写模式打开的文件对象或者一个 # StringIO 对象，或者其他自定义的满足条件的对象。 # protocol: 这是一个可选的参数，默认为 3 ，如果设置为 4 ,则要求python 版本是3以上 # 有压缩功能 pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL) except Exception as e: print('Unable to save data to', set_filename, ':', e) return dataset_names 123# 调用方法处理数据处理,放心不会重复执行train_datasets = maybe_pickle(train_folders, 45000)test_datasets = maybe_pickle(test_folders, 1800) 1234567891011121314151617181920notMNIST_large/A.pickle already present - Skipping pickling.notMNIST_large/B.pickle already present - Skipping pickling.notMNIST_large/C.pickle already present - Skipping pickling.notMNIST_large/D.pickle already present - Skipping pickling.notMNIST_large/E.pickle already present - Skipping pickling.notMNIST_large/F.pickle already present - Skipping pickling.notMNIST_large/G.pickle already present - Skipping pickling.notMNIST_large/H.pickle already present - Skipping pickling.notMNIST_large/I.pickle already present - Skipping pickling.notMNIST_large/J.pickle already present - Skipping pickling.notMNIST_small/A.pickle already present - Skipping pickling.notMNIST_small/B.pickle already present - Skipping pickling.notMNIST_small/C.pickle already present - Skipping pickling.notMNIST_small/D.pickle already present - Skipping pickling.notMNIST_small/E.pickle already present - Skipping pickling.notMNIST_small/F.pickle already present - Skipping pickling.notMNIST_small/G.pickle already present - Skipping pickling.notMNIST_small/H.pickle already present - Skipping pickling.notMNIST_small/I.pickle already present - Skipping pickling.notMNIST_small/J.pickle already present - Skipping pickling. 问题 2: 验证归一化的图像1234567# TODO:第二个作业 通过展示归一化后的图像以及标签 提示：你可以使用matplotlib.pyplot来展示图像# 如何把pickle 载入内存中pickle_file_path = \"notMNIST_large/B.pickle\"f = open(pickle_file_path, 'rb')letter_set = pickle.load(f)index = np.random.randint(0,45000)plt.imshow(letter_set[index,:,:]) 1&lt;matplotlib.image.AxesImage at 0xf86ae10&gt; 12for pickle_file in train_datasets: print(pickle_file) 12345678910notMNIST_large/A.picklenotMNIST_large/B.picklenotMNIST_large/C.picklenotMNIST_large/D.picklenotMNIST_large/E.picklenotMNIST_large/F.picklenotMNIST_large/G.picklenotMNIST_large/H.picklenotMNIST_large/I.picklenotMNIST_large/J.pickle 12345678910111213141516171819202122232425# 多展示一些归一化后的pickle图像,# \"\"\"：num_per_class:int 每个类展示几个图像train_datasets 就是pickle 的名称的list\"\"\"def showimages_pickle(num_per_class:int,train_datasets): plot_images = [] # 展示的图片 plt.figure() # 定义画图 index = 1 # 定义小图的索引 for _ in range(num_per_class): # 拿到list 中的元素 for pickle_file in train_datasets: # 载入pickle 的文件 f = open(pickle_file, 'rb') image_data_per_class = pickle.load(f) # 分成num_per_class 行，len(train_folders)列 plt.subplot(num_per_class,len(train_datasets),index) plt.imshow( image_data_per_class[np.random.randint(np.shape(image_data_per_class)[0])],cmap='gray') index = index +1 plt.axis('off') plt.show() showimages_pickle(5,train_datasets) 123# 第二种 通过IPython.display展示index = np.random.randint(0,45000)# 但是IPython.display，我不知道如何载入图像，不知道谁懂 问题 3: 验证数据平衡123456789101112131415161718192021# TODO:第三个作业 检查各个类之间的数据是否平衡# 我认为平衡就是每个类的数据量是否一致# 可能理解错误了？# 把所有的数据集依次读出来，然后建立用柱状图表示出来labels = []num_of_labels = []# enumerate 枚举 可返回两个参数，一个是索引，一个是文件for label, pickle_file in enumerate(train_datasets): try: with open(pickle_file, 'rb') as f: letter_set = pickle.load(f) num = np.shape(letter_set)[0] num_of_labels.append(num) labels.append(label) except Exception as e: print('Unable to process data from', pickle_file, ':', e) raiseplt.bar(range(len(num_of_labels)), num_of_labels,color='rgb',tick_label=labels) 1&lt;BarContainer object of 10 artists&gt; 拆分数据集成批12345678910# 由于计算的的配置没有办法装下所有的配置，所以只能部分拿出来，组合成自己想要的数据集大小，并且要根据需要调整数据集的大小# 初始化变量 dataset (3维数组)，labels(1维数组)用于存放数据，def make_arrays(nb_rows, img_size): if nb_rows: # ndarray 创建3维数组 dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32) labels = np.ndarray(nb_rows, dtype=np.int32) else: dataset, labels = None, None return dataset, labels 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 第一个参数用于 A.pickle B.pickle# 第二个参数用于 设置训练集的大小# 比如总共选择 train_size = 200000 条数据，那么就是每个类 选择 （200000/ 10类）条数据，组合集合def merge_datasets(pickle_files, train_size, valid_size=0): num_classes = len(pickle_files) # 如果valid_size = 0，那么就是生成valid_dataset =None valid_dataset, valid_labels = make_arrays(valid_size, image_size) train_dataset, train_labels = make_arrays(train_size, image_size) # 测试集的大小 ，注意除法是 // # 验证集的大小 vsize_per_class = valid_size // num_classes tsize_per_class = train_size // num_classes start_v, start_t = 0, 0 # end_v, end_t = vsize_per_class, tsize_per_class end_l = vsize_per_class + tsize_per_class # enumerate 枚举 可返回两个参数，一个是索引，一个是文件 for label, pickle_file in enumerate(pickle_files): try: with open(pickle_file, 'rb') as f: letter_set = pickle.load(f) # let's shuffle the letters to have random validation and training set # 随机洗牌 np.random.shuffle(letter_set) # 把数据分成两部分，第一部分给 验证集valid 第二部分 测试集 train，每个都选一部分，组成最终数据集 if valid_dataset is not None: valid_letter = letter_set[:vsize_per_class, :, :] valid_dataset[start_v:end_v, :, :] = valid_letter valid_labels[start_v:end_v] = label start_v += vsize_per_class end_v += vsize_per_class train_letter = letter_set[vsize_per_class:end_l, :, :] train_dataset[start_t:end_t, :, :] = train_letter train_labels[start_t:end_t] = label start_t += tsize_per_class end_t += tsize_per_class except Exception as e: print('Unable to process data from', pickle_file, ':', e) raise return valid_dataset, valid_labels, train_dataset, train_labels 123456789101112131415161718192021222324252627from typing import Optionalfrom numpy.core.multiarray import ndarraytrain_size = 20000valid_size = 1000test_size = 1000valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets( train_datasets, train_size, valid_size)_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)print('Training:', train_dataset.shape, train_labels.shape)print('Validation:', valid_dataset.shape, valid_labels.shape)print('Testing:', test_dataset.shape, test_labels.shape)# 下面就是对做好的数据集进行洗牌def randomize(dataset, labels): # 获取label的排列 permutation = np.random.permutation(labels.shape[0]) # 用这个排列数来排序dataset 以及 label shuffled_dataset = dataset[permutation,:,:] shuffled_labels = labels[permutation] return shuffled_dataset, shuffled_labelstrain_dataset, train_labels = randomize(train_dataset, train_labels)test_dataset, test_labels = randomize(test_dataset, test_labels)valid_dataset, valid_labels = randomize(valid_dataset, valid_labels) 123Training: (20000, 28, 28) (20000,)Validation: (1000, 28, 28) (1000,)Testing: (1000, 28, 28) (1000,) 问题 4: 样本乱序与验证12345678910111213141516# Problem 4# TODO:第四个作业 说服自己洗牌后的数据很好# 接下来验证下数据平衡性，然后柱状图展示# 思路获取标签中的每一类的个数，然后画出柱状图# a = np.where(valid_labels==3) 过滤def showbalance(labels): num_of_labels = [] for label in range(10): # 这里我有个疑问，为什么label_per_class.shape 不能用？？？！！！ label_per_class = np.where(labels==label) num_of_labels.append(np.shape(label_per_class)[1]) plt.bar(range(len(num_of_labels)), num_of_labels,color='rgbcy',tick_label=range(len(num_of_labels))) return num_of_labelsshowbalance(valid_labels) 1[100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 12345678910111213141516# TODO:第四个作业 数据是否已经随机打乱，打乱后的标签是否正确# 思路随机选择num_start个起始点，然后连续选择num_pic张图片和标签然后展示def show_pic_and_label(dataset,labels,num_start,num_pic): plt.figure() index = 1; for i in range(num_start): start = np.random.randint(len(labels)-num_pic) for j in range(num_pic): plt.subplot(num_start,num_pic,index) plt.imshow(dataset[start+j,:,:],cmap='gray') plt.title(labels[start+j]) index = index +1 plt.axis('off') plt.show() show_pic_and_label(train_dataset, train_labels,5,20) 1234567891011121314151617181920212223242526# 把数据存储起来准备使用# 合成路径 os.path.join# 1.open(pickle_file, 'wb')# 2.把数据集组合成键值对# 3.pickle.dump(save, f, pickle.HIGHEST_PROTOCOL) 存放数据pickle_file = os.path.join(data_root, 'notMNIST.pickle')try: f = open(pickle_file, 'wb') save = &#123; 'train_dataset': train_dataset, 'train_labels': train_labels, 'valid_dataset': valid_dataset, 'valid_labels': valid_labels, 'test_dataset': test_dataset, 'test_labels': test_labels, &#125; pickle.dump(save, f, pickle.HIGHEST_PROTOCOL) f.close()except Exception as e: print('Unable to save data to', pickle_file, ':', e) raise# 操作系统获取文件声明，可以拿到文件大小statinfo = os.stat(pickle_file)print('Compressed pickle size:', statinfo.st_size) 1Compressed pickle size: 69080502 123import timelocaltime = time.asctime( time.localtime(time.time()) )print( time.asctime( time.localtime(time.time()) )) 1Tue Aug 28 15:21:49 2018 问题 5: 寻找重叠样本1234567891011121314151617181920212223242526272829303132333435# TODO:作业五 测量训练数据 与 交叉验证数据 间的重合程度# 重合度高容易导致过拟合问题# 计算向量间的欧式距离，判断图像是否相似，# dist = numpy.sqrt(numpy.sum(numpy.square(vec1 - vec2)))# dist = numpy.linalg.norm(vec1 - vec2)# 余弦相似性 cosine_sim = np.inner(X, Y) / np.inner(np.abs(X), np.abs(Y))def overlap(train_dataset,valid_dataset,threshold=0,num_duplicate_to_show = 5): print(\"开始---&gt;\",) num_overlap = 0 # 覆盖个数 print_pic_num = 0; # 打印图片个数 index = 1; plt.figure() for i in range(train_dataset.shape[0]): for j in range(valid_dataset.shape[0]): dist = np.linalg.norm(train_dataset[i,:,:]-valid_dataset[j,:,:]) if dist &lt;= threshold: num_overlap = num_overlap + 1 if(print_pic_num&lt;=num_duplicate_to_show): # 画train_dataset第一个图 plt.subplot(num_duplicate_to_show,2,index) plt.title(\"train_dataset\") plt.imshow(train_dataset[i,:,:],cmap='gray') index = index +1 # 画valid_dataset的一样图 plt.subplot(num_duplicate_to_show,2,index) plt.title(\"valid_dataset\") plt.imshow(valid_dataset[j,:,:],cmap='gray') print_pic_num = print_pic_num + 1 index = index +1 print(\"重合数据个数是：\",num_overlap) print(\"train_dataset重合率是：\",num_overlap/train_dataset.shape[0]) print(\"valid_dataset重合率是：\",num_overlap/valid_dataset.shape[0]) return num_overlapoverlap(train_dataset,valid_dataset,threshold=0,num_duplicate_to_show = 5) 1开始---&gt; 12C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py:107: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance. In a future version, a new instance will always be created and returned. Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance. warnings.warn(message, mplDeprecation, stacklevel=1) 123重合数据个数是： 213540train_dataset重合率是： 1.0677valid_dataset重合率是： 21.354 1213540 123456789101112131415161718192021222324252627282930313233343536373839404142def overlap_cos_matrix(source_dataset,target_dataset,threshold=1,num_duplicate_to_show = 5,selfcheck = False): print(\"开始时间：\",time.asctime( time.localtime(time.time()) )) X = np.reshape(source_dataset,(source_dataset.shape[0],-1)) Y = np.reshape(target_dataset,(target_dataset.shape[0],-1)) assert (X.shape[1] == Y.shape[1]) # 计算余弦相似度，注意这里是矩阵相乘 cosine_sim = np.dot(X, Y.T) / np.inner(np.abs(X), np.abs(Y)) assert (cosine_sim.shape == (X.shape[0],Y.shape[0])) print(cosine_sim.shape) # 判断cosine_sim中的元素是否有等于threshold的，如果有就是相等 num_source = 0 num_target = 0 print_pic_num = 1; plt.figure() print(\"矩阵计算结束：\",time.asctime( time.localtime(time.time()) )) for i in range(X.shape[0]): # 第i 行中所有重复的图片索引 dup_indices = np.where(cosine_sim[i,:] &gt;= threshold) for j in dup_indices[0]: if i==j and selfcheck: continue else: num_source = num_source +1 if(print_pic_num&lt;=num_duplicate_to_show): # 画train_dataset第一个图 plt.subplot(num_duplicate_to_show,2,print_pic_num) plt.imshow(source_dataset[i,:,:],cmap='gray') print_pic_num = print_pic_num +1 # 画valid_dataset的一样图 plt.subplot(num_duplicate_to_show,2,print_pic_num) plt.imshow(target_dataset[j,:,:],cmap='gray') print_pic_num = print_pic_num + 1 break print(\"结束时间：\",time.asctime( time.localtime(time.time()) )) print(\"重合数据个数是：\",num_source) print(\"train_dataset重合率是：\",num_source/source_dataset.shape[0]) # print(\"valid_dataset重合率是：\",num_overlap/valid_dataset.shape[0]) plt.axis('off') plt.show() return num_source overlap_cos_matrix(valid_dataset,train_dataset,threshold=1,num_duplicate_to_show = 10,selfcheck = False) 123456开始时间： Tue Aug 28 15:55:19 2018(1000, 20000)矩阵计算结束： Tue Aug 28 15:55:20 2018结束时间： Tue Aug 28 15:55:20 2018重合数据个数是： 47train_dataset重合率是： 0.047 147 问题 6: 训练一个简单的机器学习模型1234# TODO:作业六 验证集自己内部的重复图片有多少？res = overlap_cos_matrix(valid_dataset,valid_dataset,threshold=1,num_duplicate_to_show = 10,selfcheck = True)# 每个图像都和自己重叠res = res - valid_dataset.shape[0] 123456开始时间： Tue Aug 28 15:55:23 2018(1000, 1000)矩阵计算结束： Tue Aug 28 15:55:23 2018结束时间： Tue Aug 28 15:55:23 2018重合数据个数是： 16train_dataset重合率是： 0.016 12345678910111213141516# 扩展一下如何判断两个图是否相似呢，可以通过下面的代码——直方图相似度def difference(hist1,hist2): sum1 = 0 for i in range(len(hist1)): if (hist1[i] == hist2[i]): sum1 += 1 else: sum1 += 1 - float(abs(hist1[i] - hist2[i]))/ max(hist1[i], hist2[i]) return sum1/len(hist1)def similary_calculate(img1, img2): img1_reshape = np.reshape(img1, (28 * 28, 1)) img1_reshape = np.reshape(img2, (28 * 28, 1)) hist1 = list(img1_reshape.getdata()) hist2 = list(img1_reshape.getdata()) return difference(hist1, hist2) 1234567891011# TODO:作业七 创建一个消过毒的测试以及验证集 比较你们在后续作业的准确性# 思路一: 在目前基础上做 # 1. 首先trainset的pickle检查重复的，如果重复的删除# 2. validateset的pickle检查重复的，如果重复的删除# 3. test的pickle 检查重复的，重复的删除# 4. train 和 valid 中重复的，把valid 删除# 5. train 和test 中重复的，把test 删除# 思路二：从头做# 1. 把图像数据按类查找重复的，每个图像和其他所有图像匹配，重复就删除# 2. 重新执行 转换3D数据，归一化，拆分成组合成新批次 1234567891011121314# TODO:作业八 using 50, 100, 1000 and 5000 training samples，用sklearn 训练一个模型# 提示使用sklearn 的 linear_model.LogisticRegression()# 思路一：直接用学习曲线，不用上面分好的验证集# from sklearn.model_selection import learning_curve #学习曲线模块# from sklearn.linear_model import LogisticRegression# from sklearn.model_selection import ShuffleSplit # 专业的数据集分割包# # X = train_dataset.reshape(train_dataset.shape[0],-1)# y = train_labels# # train_sizes, train_scores, test_scores= learning_curve(# model, X, y,cv=5 train_sizes=[50,100,1000,5000],n_jobs=5)# # train_scores 123456789101112131415161718192021222324252627282930# 思路二：自己做预测，自己做学习曲线，用上面分好的验证集from sklearn.model_selection import learning_curve #学习曲线模块from sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import ShuffleSplit # 专业的数据集分割包# train_dataset, train_labels = randomize(train_dataset, train_labels)# test_dataset, test_labels = randomize(test_dataset, test_labels)# valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)train_size=[50,100,1000,5000,8000,15000,20000]train_scores = []valid_scores = []test_scores = []for size in train_size: # 每次循环都要新建模型，solver适合数据较多的情况，比较快 model = LogisticRegression(solver= 'saga',multi_class='multinomial') # 由于train_dataset中数据已经打乱了，所以按顺序拿 X = train_dataset[0:size,:].reshape(size,-1) y = train_labels[0:size] X_valid = valid_dataset.reshape(valid_dataset.shape[0],-1) y_valid = valid_labels X_test = test_dataset.reshape(test_dataset.shape[0],-1) y_test = test_labels model.fit(X,y) train_scores.append(model.score(X,y)) valid_scores.append(model.score(X_valid,y_valid)) test_scores.append(model.score(X_test,y_test)) 12C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge &quot;the coef_ did not converge&quot;, ConvergenceWarning) 123456789101112131415161718# axis=1 对行进行操作，由于上面的cv=分成了20份，所以 train_scores 是一个20个元素的数组# train_scores_mean = np.mean(train_scores, axis=1)# valid_scores_mean = np.mean(valid_scores, axis=1)# test_scores_mean = np.mean(test_scores, axis=1)# train_sizes 横坐标，train_loss_mean 纵坐标，形状‘o-’,红色plt.plot(train_size, train_scores, 'o-', color=\"r\", label=\"Training\")plt.plot(train_size, valid_scores, 'o-', color=\"b\", label=\"Cross-validation\")plt.plot(train_size, test_scores, 'o-', color=\"g\", label=\"test\")plt.xlabel(\"Training examples\")plt.ylabel(\"Loss\")# 将图标签——图例放到最佳的位置plt.legend(loc=\"best\")title = \"Learning Curves\"plt.title(title) 1Text(0.5,1,&apos;Learning Curves&apos;)","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"机器学习","slug":"深度学习/机器学习","permalink":"http://yoursite.com/categories/深度学习/机器学习/"},{"name":"notminist","slug":"深度学习/机器学习/notminist","permalink":"http://yoursite.com/categories/深度学习/机器学习/notminist/"},{"name":"分类","slug":"深度学习/机器学习/notminist/分类","permalink":"http://yoursite.com/categories/深度学习/机器学习/notminist/分类/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"notminist","slug":"notminist","permalink":"http://yoursite.com/tags/notminist/"},{"name":"数据清洗，数据预处理","slug":"数据清洗，数据预处理","permalink":"http://yoursite.com/tags/数据清洗，数据预处理/"},{"name":"学习曲线","slug":"学习曲线","permalink":"http://yoursite.com/tags/学习曲线/"},{"name":"LogisticRegression","slug":"LogisticRegression","permalink":"http://yoursite.com/tags/LogisticRegression/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"机器学习","slug":"深度学习/机器学习","permalink":"http://yoursite.com/categories/深度学习/机器学习/"},{"name":"notminist","slug":"深度学习/机器学习/notminist","permalink":"http://yoursite.com/categories/深度学习/机器学习/notminist/"},{"name":"分类","slug":"深度学习/机器学习/notminist/分类","permalink":"http://yoursite.com/categories/深度学习/机器学习/notminist/分类/"}]},{"title":"搭建免费博客HEXO+GitHub","slug":"搭建免费博客HEXO+GitHub","date":"2017-08-21T12:19:32.000Z","updated":"2018-08-29T08:49:46.693Z","comments":true,"path":"2017/08/21/搭建免费博客HEXO+GitHub/","link":"","permalink":"http://yoursite.com/2017/08/21/搭建免费博客HEXO+GitHub/","excerpt":"","text":"原料 Node.js ——简单的说就是运行在服务端的 JavaScript, 所以这个构建后端服务的 . Nexo —— 一款基于Node.js的静态博客框架，这个是台湾人创建的 GitHub Pages —— GitHub全球最大的Gay站，我们用的是GitHub中的仓库，因为它是免费的.. 步骤创建Github仓库 创建Github仓库安装Git 安装Git 可以直接安装GitHub Desktop创建SSH秘钥 创建SSH秘钥 配置git的用户名和邮箱 右键打开gitBash,12git config --global user.name &quot;你的GitHub用户名&quot;git config --global user.email &quot;你的GitHub注册邮箱&quot; 看本地有秘钥没 1cd ~/. ssh 本地创建秘钥 1ssh-keygen -t rsa -C &quot;your_email@example.com&quot; 中间会提示你是否需要设置密码，可输可不输 上传到GitHub复制公钥到系统粘贴板中 1clip &lt; ~/.ssh/id_rsa.pub +测试 1ssh -T git@github.com 如果提示你 yes /no? 那就是yes 安装Node.js 安装Node.js 下载地址:官网 安装Hexo 安装Hexo 安装nexo 1npm install hexo-cli -g 安装部署工具 1npm install hexo-deployer-git --save 初始化 1hexo init 启动 12345hexo generatehexo server可以一句话hexo g -d 常用命令现在来介绍常用的Hexo 命令 1234567891011121314151617npm install hexo -g #安装Hexonpm update hexo -g #升级 hexo init #初始化博客命令简写hexo n &quot;我的博客&quot; == hexo new &quot;我的博客&quot; #新建文章hexo g == hexo generate #生成hexo s == hexo server #启动服务预览hexo d == hexo deploy #部署hexo server #Hexo会监视文件变动并自动更新，无须重启服务器hexo server -s #静态模式hexo server -p 5000 #更改端口hexo server -i 192.168.1.1 #自定义 IPhexo clean #清除缓存，若是网页正常情况下可以忽略这条命令刚刚的三个命令依次是新建一篇博客文章、生成网页、在本地预览的操作。 浏览器访问地址http://localhost:4000 上传到Github 上传到Github 配置根目录下 _config.yml 1234deploy:type: gitrepository: git@github.com:username/username.github.io.gitbranch: master 上传github 123hexo clean hexo g hexo d 最后一条命令是部署到github访问 http://xxxx.github.io 绑定域名 绑定域名 更换主题 更换主题 Themes 官网 如果你不喜欢Hexo默认的主题，可以更换不同的主题，主题传送门：Themes 我自己使用的是Next主题，可以在blog目录中的themes文件夹中查看你自己主题是什么。现在把默认主题更改成Next主题，在blog目录中（就是命令行的位置处于blog目录）打开命令行输入：1git clone https://github.com/iissnan/hexo-theme-next themes/next 发布文章 发布文章 命令行 1hexo n &quot;博客名字&quot; 直接做好markdown 文件，放在nexo的source_posts目录下 1source\\_posts OSS服务器 OSS服务器 为啥要用对象存储服务（Object Storage Service，简称OSS）？ 1.费用很低，甚至免费 2.图片加载快 我用的是阿里云的OSS如果你不会写markdown 如果你不会写markdownAPI 参考","categories":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/categories/其他/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"},{"name":"搭建博客","slug":"搭建博客","permalink":"http://yoursite.com/tags/搭建博客/"},{"name":"免费","slug":"免费","permalink":"http://yoursite.com/tags/免费/"}],"keywords":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/categories/其他/"}]}]}