{"meta":{"title":"sudoli's blog","subtitle":null,"description":null,"author":"sudoli","url":"http://yoursite.com"},"pages":[],"posts":[{"title":"springcloud","slug":"七、zuul路由网关","date":"2019-01-22T12:19:32.000Z","updated":"2019-02-14T08:32:11.141Z","comments":true,"path":"2019/01/22/七、zuul路由网关/","link":"","permalink":"http://yoursite.com/2019/01/22/七、zuul路由网关/","excerpt":"","text":"七、zuul路由网关概述Zuul包含了对请求的路由和过滤两个最主要的功能：其中路由功能负责将外部请求转发到具体的微服务实例上，是实现外部访问统一入口的基础而过滤器功能则负责对请求的处理过程进行干预，是实现请求校验、服务聚合等功能的基础.Zuul和Eureka进行整合，将Zuul自身注册为Eureka服务治理下的应用，同时从Eureka中获得其他微服务的消息，也即以后的访问微服务都是通过Zuul跳转后获得。 1注意：Zuul服务最终还是会注册进Eureka ==提供=代理+路由+过滤三大功能== 官网资料 https://github.com/Netflix/zuul/wiki/Getting-Started 实施步骤 新建Module模块microservicecloud-zuul-gateway-9527 修改pom.xml 123456789&lt;!-- zuul路由网关 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zuul&lt;/artifactId&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt;&lt;/dependency&gt; 修改yml 123456789101112131415161718192021server: port: 9527 spring: application: name: microservicecloud-zuul-gateway eureka: client: service-url: defaultZone: http://eureka7001.com:7001/eureka,http://eureka7002.com:7002/eureka,http://eureka7003.com:7003/eureka instance: instance-id: gateway-9527.com prefer-ip-address: true info: app.name: atguigu-microcloud company.name: www.atguigu.com build.artifactId: $project.artifactId$ build.version: $project.version$ hosts修改 127.0.0.1 myzuul.com 修改主启动类加上标签 123456789@SpringBootApplication@EnableZuulProxypublic class Zuul_9527_StartSpringCloudApp&#123; public static void main(String[] args) &#123; SpringApplication.run(Zuul_9527_StartSpringCloudApp.class, args); &#125;&#125; 启动测试 启用路由 http://myzuul.com:9527/microservicecloud-dept/dept/get/2 但是这个路由还是可以访问的 http://localhost:8001/dept/get/2 自定义路由访问映射规则 修改yml beforehttp://myzuul.com:9527/microservicecloud-dept/dept/get/2 1234zuul: routes: mydept.serviceId: microservicecloud-dept mydept.path: /mydept/** afterhttp://myzuul.com:9527/mydept/dept/get/1 但是 再次修改yml 123456zuul: #ignored-services: microservicecloud-dept ignored-services: \"*\" routes: mydept.serviceId: microservicecloud-dept mydept.path: /mydept/** 添加公共前缀 prefix: /atguigu 123456zuul: prefix: /atguigu ignored-services: \"*\" routes: mydept.serviceId: microservicecloud-dept mydept.path: /mydept/** http://myzuul.com:9527/atguigu/mydept/dept/get/1","categories":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/tags/springcloud/"}],"keywords":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}]},{"title":"springcloud","slug":"六、（3）服务监控hystrixDashboard","date":"2019-01-21T12:19:32.000Z","updated":"2019-02-14T08:31:56.881Z","comments":true,"path":"2019/01/21/六、（3）服务监控hystrixDashboard/","link":"","permalink":"http://yoursite.com/2019/01/21/六、（3）服务监控hystrixDashboard/","excerpt":"","text":"服务监控hystrixDashboard除了隔离依赖服务的调用以外，Hystrix还提供了准实时的调用监控（Hystrix Dashboard），Hystrix会持续地记录所有通过Hystrix发起的请求的执行信息，并以统计报表和图形的形式展示给用户，包括每秒执行多少请求多少成功，多少失败等。Netflix通过hystrix-metrics-event-stream项目实现了对以上指标的监控。Spring Cloud也提供了Hystrix Dashboard的整合，对监控内容转化成可视化界面。 步骤1、新建工程microservicecloud-consumer-hystrix-dashboard 2、pom.xml 123456789&lt;!-- hystrix和 hystrix-dashboard相关--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix-dashboard&lt;/artifactId&gt; &lt;/dependency&gt; 3、yml 12server: port: 9001 4、修改主程序主启动类改名+新注解@EnableHystrixDashboard 123456789@SpringBootApplication@EnableHystrixDashboardpublic class DeptConsumer_DashBoard_App&#123; public static void main(String[] args) &#123; SpringApplication.run(DeptConsumer_DashBoard_App.class,args); &#125;&#125; 所有Provider微服务提供类(8001/8002/8003)都需要监控依赖配置 12345&lt;!-- actuator监控信息完善 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 启动 测试 http://localhost:9001/hystrix 启动microservicecloud-provider-dept-hystrix-8001 http://localhost:8001/dept/get/1 http://localhost:8001/hystrix.stream 怎么用 实心圆：共有两种含义。它通过颜色的变化代表了实例的健康程度，它的健康度从绿色&lt;黄色&lt;橙色&lt;红色递减。 该实心圆除了颜色的变化之外，它的大小也会根据实例的请求流量发生变化，流量越大该实心圆就越大。所以通过该实心圆的展示，就可以在大量的实例中快速的发现故障实例和高压力实例。 曲线：用来记录2分钟内流量的相对变化，可以通过它来观察到流量的上升和下降趋势。 整图说明 比较复杂的","categories":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/tags/springcloud/"}],"keywords":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}]},{"title":"springcloud","slug":"六、(2)Hystrix服务降级","date":"2019-01-20T12:19:32.000Z","updated":"2019-02-14T08:31:48.884Z","comments":true,"path":"2019/01/20/六、(2)Hystrix服务降级/","link":"","permalink":"http://yoursite.com/2019/01/20/六、(2)Hystrix服务降级/","excerpt":"","text":"服务降级整体资源快不够了，忍痛将某些服务先关掉，待渡过难关，再开启回来。 服务降级处理是在客户端实现完成的，与服务端没有关系 ==此时服务端provider已经down了，但是我们做了服务降级处理，让客户端在服务端不可用时也会获得提示信息而不会挂起耗死服务器== 1、修改microservicecloud-api工程，根据已经有的DeptClientService接口新建一个实现了FallbackFactory接口的类DeptClientServiceFallbackFactory 123456789101112131415161718192021222324252627282930313233343536373839package com.atguigu.springcloud.service; import java.util.List; import org.springframework.stereotype.Component; import com.atguigu.springcloud.entities.Dept; import feign.hystrix.FallbackFactory; @Component//不要忘记添加，不要忘记添加public class DeptClientServiceFallbackFactory implements FallbackFactory&lt;DeptClientService&gt;&#123; @Override public DeptClientService create(Throwable throwable) &#123; return new DeptClientService() &#123; @Override public Dept get(long id) &#123; return new Dept().setDeptno(id) .setDname(\"该ID：\"+id+\"没有没有对应的信息,Consumer客户端提供的降级信息,此刻服务Provider已经关闭\") .setDb_source(\"no this database in MySQL\"); &#125; @Override public List&lt;Dept&gt; list() &#123; return null; &#125; @Override public boolean add(Dept dept) &#123; return false; &#125; &#125;; &#125;&#125; 2、修改service接口 修改microservicecloud-api工程，DeptClientService接口在注解@FeignClient中添加fallbackFactory属性值 1234567891011121314151617181920212223package com.atguigu.springcloud.service; import java.util.List; import org.springframework.cloud.netflix.feign.FeignClient;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod; import com.atguigu.springcloud.entities.Dept; @FeignClient(value = \"MICROSERVICECLOUD-DEPT\",fallbackFactory=DeptClientServiceFallbackFactory.class)public interface DeptClientService&#123; @RequestMapping(value = \"/dept/get/&#123;id&#125;\",method = RequestMethod.GET) public Dept get(@PathVariable(\"id\") long id); @RequestMapping(value = \"/dept/list\",method = RequestMethod.GET) public List&lt;Dept&gt; list(); @RequestMapping(value = \"/dept/add\",method = RequestMethod.POST) public boolean add(Dept dept);&#125; 2.1 microservicecloud-api工程 mvn clean install 3、microservicecloud-consumer-dept-feign工程修改yml ==也就是说只要改Feign 的接口，让接口知道出现异常，找哪个异常类，在异常类中实现所有方法异常处理== 123456789101112server: port: 80 feign: hystrix: enabled: true eureka: client: register-with-eureka: false service-url: defaultZone: http://eureka7001.com:7001/eureka/,http://eureka7002.com:7002/eureka/,http://eureka7003.com:7003/eureka/ 4、测试","categories":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/tags/springcloud/"}],"keywords":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}]},{"title":"springcloud","slug":"六、(1)Hystrix服务熔断实施步骤","date":"2019-01-18T12:19:32.000Z","updated":"2019-02-14T08:31:37.342Z","comments":true,"path":"2019/01/18/六、(1)Hystrix服务熔断实施步骤/","link":"","permalink":"http://yoursite.com/2019/01/18/六、(1)Hystrix服务熔断实施步骤/","excerpt":"","text":"服务熔断实施步骤1、修改pom.xml，添加 12345&lt;!-- hystrix --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt;&lt;/dependency&gt; 2、修改yml 1234567eureka: client: #客户端注册进eureka服务列表内 service-url: defaultZone: http://eureka7001.com:7001/eureka/,http://eureka7002.com:7002/eureka/,http://eureka7003.com:7003/eureka/ instance: instance-id: microservicecloud-dept8001-hystrix #自定义服务名称信息 prefer-ip-address: true #访问路径可以显示IP地址 全局 12345678910111213141516171819202122232425262728293031323334353637server: port: 8001 mybatis: config-location: classpath:mybatis/mybatis.cfg.xml #mybatis所在路径 type-aliases-package: com.atguigu.springcloud.entities #entity别名类 mapper-locations: - classpath:mybatis/mapper/**/*.xml #mapper映射文件 spring: application: name: microservicecloud-dept datasource: type: com.alibaba.druid.pool.DruidDataSource driver-class-name: org.gjt.mm.mysql.Driver url: jdbc:mysql://localhost:3306/cloudDB01 username: root password: 123456 dbcp2: min-idle: 5 initial-size: 5 max-total: 5 max-wait-millis: 200 eureka: client: #客户端注册进eureka服务列表内 service-url: defaultZone: http://eureka7001.com:7001/eureka/,http://eureka7002.com:7002/eureka/,http://eureka7003.com:7003/eureka/ instance: instance-id: microservicecloud-dept8001-hystrix #自定义服务名称信息 prefer-ip-address: true #访问路径可以显示IP地址 info: app.name: atguigu-microservicecloud company.name: www.atguigu.com build.artifactId: $project.artifactId$ build.version: $project.version$ 3、修改DeptController @HystrixCommand报异常后如何处理 一旦调用服务方法失败并抛出了错误信息后，会自动调用@HystrixCommand标注好的fallbackMethod调用类中的指定方法 123456789101112131415161718192021222324252627 @RequestMapping(value = \"/dept/get/&#123;id&#125;\", method = RequestMethod.GET) //一旦调用服务方法失败并抛出了错误信息后，会自动调用@HystrixCommand标注好的fallbackMethod调用类中的指定方法 @HystrixCommand(fallbackMethod = \"processHystrix_Get\") public Dept get(@PathVariable(\"id\") Long id) &#123; Dept dept = this.service.get(id); if (null == dept) &#123; throw new RuntimeException(\"该ID：\" + id + \"没有没有对应的信息\"); &#125; return dept; &#125;@RequestMapping(value = \"/dept/get/&#123;id&#125;\", method = RequestMethod.GET)//一旦调用服务方法失败并抛出了错误信息后，会自动调用@HystrixCommand标注好的fallbackMethod调用类中的指定方法@HystrixCommand(fallbackMethod = \"processHystrix_Get\")public Dept get(@PathVariable(\"id\") Long id)&#123; Dept dept = this.service.get(id); if (null == dept) &#123; throw new RuntimeException(\"该ID：\" + id + \"没有没有对应的信息\"); &#125; return dept;&#125; 4、修改主启动类 修改主启动类DeptProvider8001_Hystrix_App并添加新注解@EnableCircuitBreaker 1234567891011@SpringBootApplication@EnableEurekaClient //本服务启动后会自动注册进eureka服务中@EnableDiscoveryClient //服务发现@EnableCircuitBreaker//对hystrixR熔断机制的支持public class DeptProvider8001_Hystrix_App&#123; public static void main(String[] args) &#123; SpringApplication.run(DeptProvider8001_Hystrix_App.class, args); &#125;&#125; 5、测试 如果对应的ID：112，数据库里面没有这个记录，我们报错后统一返回。","categories":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/tags/springcloud/"}],"keywords":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}]},{"title":"springcloud","slug":"六、Hystrix断路器","date":"2019-01-17T12:19:32.000Z","updated":"2019-02-14T08:31:29.118Z","comments":true,"path":"2019/01/17/六、Hystrix断路器/","link":"","permalink":"http://yoursite.com/2019/01/17/六、Hystrix断路器/","excerpt":"","text":"六、Hystrix断路器 分布式系统面临的问题分布式系统面临的问题复杂分布式体系结构中的应用程序有数十个依赖关系，每个依赖关系在某些时候将不可避免地失败。 服务雪崩多个微服务之间调用的时候，假设微服务A调用微服务B和微服务C，微服务B和微服务C又调用其它的微服务，这就是所谓的“扇出”。如果扇出的链路上某个微服务的调用响应时间过长或者不可用，对微服务A的调用就会占用越来越多的系统资源，进而引起系统崩溃，所谓的“雪崩效应”. 对于高流量的应用来说，单一的后端依赖可能会导致所有服务器上的所有资源都在几秒钟内饱和。比失败更糟糕的是，这些应用程序还可能导致服务之间的延迟增加，备份队列，线程和其他系统资源紧张，导致整个系统发生更多的级联故障。这些都表示需要对故障和延迟进行隔离和管理，以便单个依赖关系的失败，不能取消整个应用程序或系统。 备注：一般情况对于服务依赖的保护主要有3中解决方案： （1）熔断模式：这种模式主要是参考电路熔断，如果一条线路电压过高，保险丝会熔断，防止火灾。放到我们的系统中，如果某个目标服务调用慢或者有大量超时，此时，熔断该服务的调用，对于后续调用请求，不在继续调用目标服务，直接返回，快速释放资源。如果目标服务情况好转则恢复调用。 （2）隔离模式：这种模式就像对系统请求按类型划分成一个个小岛的一样，当某个小岛被火少光了，不会影响到其他的小岛。例如可以对不同类型的请求使用线程池来资源隔离，每种类型的请求互不影响，如果一种类型的请求线程资源耗尽，则对后续的该类型请求直接返回，不再调用后续资源。这种模式使用场景非常多，例如将一个服务拆开，对于重要的服务使用单独服务器来部署，再或者公司最近推广的多中心。 （3）限流模式：上述的熔断模式和隔离模式都属于出错后的容错处理机制，而限流模式则可以称为预防模式。限流模式主要是提前对各个类型的请求设置最高的QPS阈值，若高于设置的阈值则对该请求直接返回，不再调用后续资源。这种模式不能解决服务依赖的问题，只能解决系统整体资源分配问题，因为没有被限流的请求依然有可能造成雪崩效应。 Hystrix干嘛的 Hystrix是一个用于处理分布式==系统的延迟和容错的开源库==，在分布式系统里，许多依赖不可避免的会调用失败，比如超时、异常等，Hystrix能够保证在一个依赖出问题的情况下，==不会导致整体服务失败，避免级联故障，以提高分布式系统的弹性。== “断路器”本身是一种开关装置，当某个服务单元发生故障之后，==通过断路器的故障监控（类似熔断保险丝），向调用方返回一个符合预期的、可处理的备选响应（FallBack），而不是长时间的等待或者抛出调用方无法处理的异常==，这样就保证了服务调用方的线程不会被长时间、不必要地占用，从而避免了故障在分布式系统中的蔓延，乃至雪崩。 Hystrix 简介 服务熔断熔断机制是应对雪崩效应的一种微服务链路保护机制。当扇出链路的某个微服务不可用或者响应时间太长时，会进行服务的降级，==进而熔断该节点微服务的调用，快速返回”错误”的响应信息==。当检测到该节点微服务调用响应正常后恢复调用链路。==在SpringCloud框架里熔断机制通过Hystrix实现==。Hystrix会监控微服务间调用的状况，当失败的调用到一定阈值，==缺省是5秒内20次调用失败就会启动熔断机制==。熔断机制的注解是@HystrixCommand。","categories":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/tags/springcloud/"}],"keywords":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}]},{"title":"springcloud","slug":"五、Feign负载均衡","date":"2019-01-16T12:19:32.000Z","updated":"2019-02-14T08:31:21.476Z","comments":true,"path":"2019/01/16/五、Feign负载均衡/","link":"","permalink":"http://yoursite.com/2019/01/16/五、Feign负载均衡/","excerpt":"","text":"五、Feign负载均衡 概述官网解释：http://projects.spring.io/spring-cloud/spring-cloud.html#spring-cloud-feign ==Feign是一个声明式WebService客户端==。使用Feign能让编写Web Service客户端更加简单, 它的使用方法是定义一个接口，然后在上面添加注解，同时也支持JAX-RS标准的注解。Feign也支持可拔插式的编码器和解码器。Spring Cloud对Feign进行了封装，使其支持了Spring MVC标准注解和HttpMessageConverters。Feign可以与Eureka和Ribbon组合使用以支持负载均衡。 == ==Feign是一个声明式的Web服务客户端，使得编写Web服务客户端变得非常容易，====只需要创建一个接口，然后在上面添加注解即可。==参考官网：https://github.com/OpenFeign/feign== Feign能干什么Feign旨在使编写Java Http客户端变得更容易。前面在使用Ribbon+RestTemplate时，利用RestTemplate对http请求的封装处理，形成了一套模版化的调用方法。但是在实际开发中，由于对服务依赖的调用可能不止一处，往往一个接口会被多处调用，所以通常都会针对每个微服务自行封装一些客户端类来包装这些依赖服务的调用。==所以，Feign在此基础上做了进一步封装，由他来帮助我们定义和实现依赖服务接口的定义。在Feign的实现下，我们只需创建一个接口并使用注解的方式来配置它(以前是Dao接口上面标注Mapper注解,现在是一个微服务接口上面标注一个Feign注解即可)，即可完成对服务提供方的接口绑定，简化了使用Spring cloud Ribbon时，自动封装服务调用客户端的开发量。== ==Feign集成了Ribbon====利用Ribbon维护了MicroServiceCloud-Dept的服务列表信息==，并且通过轮询实现了客户端的负载均衡。而与Ribbon不同的是，通过feign只需要定义服务绑定接口且以声明式的方法，优雅而简单的实现了服务调用 使用步骤1、参考microservicecloud-consumer-dept-80 新建microservicecloud-consumer-dept-feign 2、修改启动类名字DeptConsumer80_Feign_App 3、pom.xml修改，主要添加对feign的支持 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-feign&lt;/artifactId&gt;&lt;/dependency&gt; 4、修改microservicecloud-api工程, 修改pom.xml,添加feign的支持 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-feign&lt;/artifactId&gt;&lt;/dependency&gt; 新建DeptClientService接口并新增注解@FeignClient 1234567891011121314151617181920212223package com.atguigu.springcloud.service; import java.util.List; import org.springframework.cloud.netflix.feign.FeignClient;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod; import com.atguigu.springcloud.entities.Dept; @FeignClient(value = \"MICROSERVICECLOUD-DEPT\")public interface DeptClientService&#123; @RequestMapping(value = \"/dept/get/&#123;id&#125;\",method = RequestMethod.GET) public Dept get(@PathVariable(\"id\") long id); @RequestMapping(value = \"/dept/list\",method = RequestMethod.GET) public List&lt;Dept&gt; list(); @RequestMapping(value = \"/dept/add\",method = RequestMethod.POST) public boolean add(Dept dept);&#125; mvn clean mvn install 这里就是讲要用的接口放入可以公共调用的api 中，然后用maven 包的形式调用 5、microservicecloud-consumer-dept-feign工程修改Controller，添加上一步新建的DeptClientService接口 123456789101112131415161718192021222324252627282930313233343536package com.atguigu.springcloud.controller; import java.util.List; import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController; import com.atguigu.springcloud.entities.Dept;import com.atguigu.springcloud.service.DeptClientService; @RestControllerpublic class DeptController_Feign&#123; @Autowired private DeptClientService service ; @RequestMapping(value = \"/consumer/dept/get/&#123;id&#125;\") public Dept get(@PathVariable(\"id\") Long id) &#123; return this.service.get(id); &#125; @RequestMapping(value = \"/consumer/dept/list\") public List&lt;Dept&gt; list() &#123; return this.service.list(); &#125; @RequestMapping(value = \"/consumer/dept/add\") public Object add(Dept dept) &#123; return this.service.add(dept); &#125;&#125; 6、microservicecloud-consumer-dept-feign工程修改主启动类 1234567891011@SpringBootApplication@EnableEurekaClient@EnableFeignClients(basePackages= &#123;\"com.atguigu.springcloud\"&#125;)@ComponentScan(\"com.atguigu.springcloud\")public class DeptConsumer80_Feign_App&#123; public static void main(String[] args) &#123; SpringApplication.run(DeptConsumer80_Feign_App.class, args); &#125;&#125; 8、接下来测试了 总结 ==Feign通过接口的方法调用Rest服务（之前是Ribbon+RestTemplate）==，该请求发送给Eureka服务器（http://MICROSERVICECLOUD-DEPT/dept/list）,通过Feign直接找到服务接口，由于在进行服务调用的时候融合了Ribbon技术，所以也支持负载均衡作用。 看看脑图","categories":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/tags/springcloud/"}],"keywords":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}]},{"title":"springcloud","slug":"四、（3）Ribbon核心组件IRule","date":"2019-01-15T12:19:32.000Z","updated":"2019-02-14T08:31:11.841Z","comments":true,"path":"2019/01/15/四、（3）Ribbon核心组件IRule/","link":"","permalink":"http://yoursite.com/2019/01/15/四、（3）Ribbon核心组件IRule/","excerpt":"","text":"Ribbon核心组件IRule自定义服务选择策略 1234567@Beanpublic IRule myRule()&#123; //return new RoundRobinRule(); //return new RandomRule();//达到的目的，用我们重新选择的随机算法替代默认的轮询。 return new RetryRule();&#125; IRule：根据特定算法中从服务列表中选取一个要访问的服务 RoundRobinRule 轮询 RandomRule 随机 AvailabilityFilteringRule 会先过滤掉由于多次访问故障而处于断路器跳闸状态的服务， 还有并发的连接数量超过阈值的服务，然后对剩余的服务列表按照轮询策略进行访问 WeightedResponseTimeRule 根据平均响应时间计算所有服务的权重，响应时间越快服务权重越大被选中的概率越高。 刚启动时如果统计信息不足，则使用RoundRobinRule策略，等统计信息足够， 会切换到WeightedResponseTimeRule RetryRule 先按照RoundRobinRule的策略获取服务，如果获取服务失败则在指定时间内会进行重试，获取可用的服务 BestAvailableRule 会先过滤掉由于多次访问故障而处于断路器跳闸状态的服务，然后选择一个并发量最小的服务 ZoneAvoidanceRule 默认规则,复合判断server所在区域的性能和server的可用性选择服务器","categories":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/tags/springcloud/"}],"keywords":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}]},{"title":"springcloud","slug":"四、(2) Ribbon负载均衡验证","date":"2019-01-13T12:19:32.000Z","updated":"2019-02-14T08:31:01.838Z","comments":true,"path":"2019/01/13/四、(2) Ribbon负载均衡验证/","link":"","permalink":"http://yoursite.com/2019/01/13/四、(2) Ribbon负载均衡验证/","excerpt":"","text":"Ribbon负载均衡验证架构说明 Ribbon在工作时分成两步==第一步先选择 EurekaServer ,它优先选择在同一个区域内负载较少的server.====第二步再根据用户指定的策略，==在从server取到的服务注册列表中选择一个地址。其中Ribbon提供了多种策略：比如轮询、随机和根据响应时间加权。 2、参考microservicecloud-provider-dept-8001，新建两份，分别命名为8002，8003 3、新建8002/8003数据库，各自微服务分别连各自的数据库 8002 123456789101112131415161718192021 DROP DATABASE IF EXISTS cloudDB02; CREATE DATABASE cloudDB02 CHARACTER SET UTF8; USE cloudDB02; CREATE TABLE dept( deptno BIGINT NOT NULL PRIMARY KEY AUTO_INCREMENT, dname VARCHAR(60), db_source VARCHAR(60)); INSERT INTO dept(dname,db_source) VALUES('开发部',DATABASE());INSERT INTO dept(dname,db_source) VALUES('人事部',DATABASE());INSERT INTO dept(dname,db_source) VALUES('财务部',DATABASE());INSERT INTO dept(dname,db_source) VALUES('市场部',DATABASE());INSERT INTO dept(dname,db_source) VALUES('运维部',DATABASE()); SELECT * FROM dept; 8003 123456789101112131415161718192021DROP DATABASE IF EXISTS cloudDB03; CREATE DATABASE cloudDB03 CHARACTER SET UTF8; USE cloudDB03; CREATE TABLE dept( deptno BIGINT NOT NULL PRIMARY KEY AUTO_INCREMENT, dname VARCHAR(60), db_source VARCHAR(60)); INSERT INTO dept(dname,db_source) VALUES('开发部',DATABASE());INSERT INTO dept(dname,db_source) VALUES('人事部',DATABASE());INSERT INTO dept(dname,db_source) VALUES('财务部',DATABASE());INSERT INTO dept(dname,db_source) VALUES('市场部',DATABASE());INSERT INTO dept(dname,db_source) VALUES('运维部',DATABASE()); SELECT * FROM dept; 4、修改8002/8003各自YML 5、启动3个eureka集群配置区 6、 7、启动microservicecloud-consumer-dept-80 8、http://localhost/consumer/dept/list 注意观察看到返回的数据库名字，各不相同，负载均衡实现，轮流访问数据库 9、总结：Ribbon其实就是一个软负载均衡的客户端组件，他可以和其他所需请求的客户端结合使用，和eureka结合只是其中的一个实例。","categories":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/tags/springcloud/"}],"keywords":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}]},{"title":"springcloud","slug":"四、(1) Ribbon配置初步","date":"2019-01-12T12:19:32.000Z","updated":"2019-02-14T08:30:54.634Z","comments":true,"path":"2019/01/12/四、(1) Ribbon配置初步/","link":"","permalink":"http://yoursite.com/2019/01/12/四、(1) Ribbon配置初步/","excerpt":"","text":"Ribbon配置初步修改microservicecloud-consumer-dept-80工程pom.xml 修改 12345678910111213&lt;!-- Ribbon相关 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-ribbon&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; 修改application.yml 追加eureka的服务注册地址 12345678server: port: 80 eureka: client: register-with-eureka: false service-url: defaultZone: http://eureka7001.com:7001/eureka/,http://eureka7002.com:7002/eureka/,http://eureka7003.com:7003/eureka/ ==在配置类的加上 @LoadBalanced==非常重要 1234567891011121314151617package com.atguigu.springcloud.cfgbeans; import org.springframework.cloud.client.loadbalancer.LoadBalanced;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.web.client.RestTemplate; @Configurationpublic class ConfigBean&#123; @Bean @LoadBalanced public RestTemplate getRestTemplate() &#123; return new RestTemplate(); &#125;&#125; 主启动类 加上@EnableEurekaClient 123456789@SpringBootApplication@EnableEurekaClientpublic class DeptConsumer80_App&#123; public static void main(String[] args) &#123; SpringApplication.run(DeptConsumer80_App.class, args); &#125;&#125; 修改DeptController_Consumer客户端访问类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.atguigu.springcloud.controller; import java.util.List; import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import org.springframework.web.client.RestTemplate; import com.atguigu.springcloud.entities.Dept; @RestControllerpublic class DeptController_Consumer&#123; //private static final String REST_URL_PREFIX = \"http://localhost:8001\"; private static final String REST_URL_PREFIX = \"http://MICROSERVICECLOUD-DEPT\"; @Autowired private RestTemplate restTemplate; @RequestMapping(value=\"/consumer/dept/add\") public boolean add(Dept dept) &#123; return restTemplate.postForObject(REST_URL_PREFIX+\"/dept/add\", dept, Boolean.class); &#125; @RequestMapping(value=\"/consumer/dept/get/&#123;id&#125;\") public Dept get(@PathVariable(\"id\") Long id) &#123; return restTemplate.getForObject(REST_URL_PREFIX+\"/dept/get/\"+id, Dept.class); &#125; @SuppressWarnings(\"unchecked\") @RequestMapping(value=\"/consumer/dept/list\") public List&lt;Dept&gt; list() &#123; return restTemplate.getForObject(REST_URL_PREFIX+\"/dept/list\", List.class); &#125; //测试@EnableDiscoveryClient,消费端可以调用服务发现 @RequestMapping(value=\"/consumer/dept/discovery\") public Object discovery() &#123; return restTemplate.getForObject(REST_URL_PREFIX+\"/dept/discovery\", Object.class); &#125; &#125; 先启动3个eureka集群后，再启动microservicecloud-provider-dept-8001并注册进eureka 启动microservicecloud-consumer-dept-80 测试 http://localhost/consumer/dept/get/1 http://localhost/consumer/dept/list http://localhost/consumer/dept/add?dname=大数据部 结论 ==Ribbon和Eureka整合后Consumer可以直接调用服务而不用再关心地址和端口号== 12345@RestControllerpublic class DeptController_Consumer&#123; //private static final String REST_URL_PREFIX = \"http://localhost:8001\"; private static final String REST_URL_PREFIX = \"http://MICROSERVICECLOUD-DEPT\";","categories":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/tags/springcloud/"}],"keywords":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}]},{"title":"springcloud","slug":"三、（5）Eureka比Zookeeper好在哪里","date":"2019-01-11T12:19:32.000Z","updated":"2019-02-14T08:30:29.203Z","comments":true,"path":"2019/01/11/三、（5）Eureka比Zookeeper好在哪里/","link":"","permalink":"http://yoursite.com/2019/01/11/三、（5）Eureka比Zookeeper好在哪里/","excerpt":"","text":"作为服务注册中心，Eureka比Zookeeper好在哪里著名的CAP理论指出，一==个分布式系统不可能同时满足C(一致性)、A(可用性)和P(分区容错性)。==由于分区容错性P在是分布式系统中必须要保证的，因此我们只能在A和C之间进行权衡。因此 ==Zookeeper保证的是CP,== ==Eureka则是AP。== 4.1 Zookeeper保证CP当向注册中心查询服务列表时，我们可以容忍注册中心返回的是几分钟以前的注册信息，但不能接受服务直接down掉不可用。也就是说，服务注册功能对可用性的要求要高于一致性。但是zk会出现这样一种情况，==当master节点因为网络故障与其他节点失去联系时，剩余节点会重新进行leader选举==。问题在于，选举leader的时间太长，==30 ~ 120s, 且选举期间整个zk集群都是不可用的==，这就导致在选举期间注册服务瘫痪。在云部署的环境下，==因网络问题使得zk集群失去master节点是较大概率会发生的事==，虽然服务能够最终恢复，但是漫长的选举时间导致的注册长期不可用是不能容忍的。 4.2 Eureka保证APEureka看明白了这一点，因此在设计时就优先保证可用性。==Eureka各个节点都是平等的，==几个节点挂掉不会影响正常节点的工作，剩余的节点依然可以提供注册和查询服务。而Eureka的客户端在向某个Eureka注册或时如果发现连接失败，则会自动切换至其它节点，==只要有一台Eureka还在，就能保证注册服务可用(保证可用性)，只不过查到的信息可能不是最新的(不保证强一致性)==。除此之外，Eureka还有一种自我保护机制，==如果在15分钟内超过85%的节点都没有正常的心跳，那么Eureka就认为客户端与注册中心出现了网络故障==，此时会出现以下几种情况： Eureka不再从注册列表中移除因为长时间没收到心跳而应该过期的服务 ==Eureka仍然能够接受新服务的注册和查询请求，但是不会被同步到其它节点上(即保证当前节点依然可用)== 当网络稳定时，当前实例新的注册信息会被同步到其它节点中 ==因此， Eureka可以很好的应对因网络故障导致部分节点失去联系的情况，而不会像zookeeper那样使整个注册服务瘫痪。==","categories":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/tags/springcloud/"}],"keywords":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}]},{"title":"springcloud","slug":"四、Ribbon负载均衡","date":"2019-01-10T12:19:32.000Z","updated":"2019-02-14T08:30:43.794Z","comments":true,"path":"2019/01/10/四、Ribbon负载均衡/","link":"","permalink":"http://yoursite.com/2019/01/10/四、Ribbon负载均衡/","excerpt":"","text":"四、Ribbon负载均衡 英 [‘r?b?n] 美 [‘r?b?n] n. 带；缎带；（勋章等的）绶带；带状物；勋表 vi. 形成带状 vt. 把…撕成条带；用缎带装饰 Ribbon 是什么Spring Cloud Ribbon是基于Netflix Ribbon实现的一套==客户端 负载均衡==的工具。 简单的说，Ribbon是Netflix发布的开源项目，主要功能是提供客户端的软件负载均衡算法，将Netflix的中间层服务连接在一起。Ribbon客户端组件提供一系列完善的配置项如连接超时，重试等。简单的说，就是在配置文件中列出Load Balancer（简称LB）后面所有的机器，==Ribbon会自动的帮助你基于某种规则（如简单轮询，随机连接等）去连接这些机器==。我们也很容易使用Ribbon实现自定义的负载均衡算法。 Load Balance干嘛的LB，即负载均衡(Load Balance)，在微服务或分布式集群中经常用的一种应用。======负载均衡简单的说就是将用户的请求平摊的分配到多个服务上，从而达到系统的HA==。常见的负载均衡有软件Nginx，LVS，硬件 F5等。相应的在中间件，例如：dubbo和SpringCloud中均给我们提供了负载均衡，==SpringCloud的负载均衡算法可以自定义==。 集中式LB即在服务的==消费方和提供方之间使用独立的LB设施(可以是硬件，如F5, 也可以是软件，如nginx)==, 由该设施负责把访问请求通过某种策略转发至服务的提供方； 进程内LB==将LB逻辑集成到消费方==，消费方从服务注册中心获知有哪些地址可用，然后自己再从这些地址中选择出一个合适的服务器。 ==Ribbon就属于进程内LB==，它只是一个类库，集成于消费方进程，消费方通过它来获取到服务提供方的地址。 这个是ribbon官网 https://github.com/Netflix/ribbon/wiki/Getting-Started","categories":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/tags/springcloud/"}],"keywords":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}]},{"title":"springcloud","slug":"三、（4）Eureka 的集群配置","date":"2019-01-09T12:19:32.000Z","updated":"2019-02-14T08:30:10.919Z","comments":true,"path":"2019/01/09/三、（4）Eureka 的集群配置/","link":"","permalink":"http://yoursite.com/2019/01/09/三、（4）Eureka 的集群配置/","excerpt":"","text":"集群配置 基本原理上图是来自eureka的官方架构图，这是基于集群配置的eureka； 处于不同节点的eureka通过Replicate进行数据同步 Application Service为服务提供者 Application Client为服务消费者 Make Remote Call完成一次服务调用 服务启动后向Eureka注册，Eureka Server会将注册信息向其他Eureka Server进行同步，当服务消费者要调用服务提供者，则向服务注册中心获取服务提供者地址，然后会将服务提供者地址缓存在本地，下次再调用时，则直接从本地缓存中取，完成一次调用。 当服务注册中心Eureka Server检测到服务提供者因为宕机、网络原因不可用时，则在服务注册中心将服务置为DOWN状态，并把当前服务提供者状态向订阅者发布，订阅过的服务消费者更新本地缓存。 服务提供者在启动后，周期性（默认30秒）向Eureka Server发送心跳，以证明当前服务是可用状态。Eureka Server在一定的时间（默认90秒）未收到客户端的心跳，则认为服务宕机，注销该实例。 配置步骤1、新建microservicecloud-eureka-7002/microservicecloud-eureka-7003 2、修改7002 与7003 的pom 1234567891011121314151617181920212223&lt;parent&gt; &lt;groupId&gt;com.atguigu.springcloud&lt;/groupId&gt; &lt;artifactId&gt;microservicecloud&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/parent&gt;&lt;artifactId&gt;microservicecloud-eureka-7002&lt;/artifactId&gt;&lt;dependencies&gt; &lt;!--eureka-server服务端 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 修改后立即生效，热部署 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;springloaded&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 3、修改7002和7003的主启动类 123456789@SpringBootApplication@EnableEurekaServer // EurekaServer服务器端启动类,接受其它微服务注册进来public class EurekaServer7002_App&#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaServer7002_App.class, args); &#125;&#125; 4、修改主机host，为了测试效果 5、修改7001 与7002 7003 的yml 123456789101112server: port: 7001 eureka: instance: hostname: eureka7001.com #eureka服务端的实例名称 client: register-with-eureka: false #false表示不向注册中心注册自己。 fetch-registry: false #false表示自己端就是注册中心，我的职责就是维护服务实例，并不需要去检索服务 service-url: #单机 defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/ #设置与Eureka Server交互的地址查询服务和注册服务都需要依赖这个地址（单机）。 defaultZone: http://eureka7002.com:7002/eureka/,http://eureka7003.com:7003/eureka/ 这里我很奇怪 为什么defaultZone 要这么设置 123456789101112server: port: 7003 eureka: instance: hostname: eureka7003.com #eureka服务端的实例名称 client: register-with-eureka: false #false表示不向注册中心注册自己。 fetch-registry: false #false表示自己端就是注册中心，我的职责就是维护服务实例，并不需要去检索服务 service-url: #defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/ #设置与Eureka Server交互的地址查询服务和注册服务都需要依赖这个地址。 defaultZone: http://eureka7001.com:7001/eureka/,http://eureka7002.com:7002/eureka/ 123456789101112server: port: 7002 eureka: instance: hostname: eureka7002.com #eureka服务端的实例名称 client: register-with-eureka: false #false表示不向注册中心注册自己。 fetch-registry: false #false表示自己端就是注册中心，我的职责就是维护服务实例，并不需要去检索服务 service-url: #defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/ #设置与Eureka Server交互的地址查询服务和注册服务都需要依赖这个地址。 defaultZone: http://eureka7001.com:7001/eureka/,http://eureka7003.com:7003/eureka/ 5、修改服务提供者的yml 1234eureka: client: #客户端注册进eureka服务列表内 service-url: defaultZone: http://eureka7001.com:7001/eureka/,http://eureka7002.com:7002/eureka/,http://eureka7003.com:7003/eureka/ 传统的ACID分别是什么 原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability） CAP原则又称CAP定理 指的是在一个分布式系统中，Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），三者不可兼得。 双11 的话选择AP 分布式系统 必选P","categories":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/tags/springcloud/"}],"keywords":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}]},{"title":"springcloud","slug":"三、（3）Eureka自我保护 与 Eureka 服务发现","date":"2019-01-05T12:19:32.000Z","updated":"2019-02-14T08:29:55.850Z","comments":true,"path":"2019/01/05/三、（3）Eureka自我保护 与 Eureka 服务发现/","link":"","permalink":"http://yoursite.com/2019/01/05/三、（3）Eureka自我保护 与 Eureka 服务发现/","excerpt":"","text":"Eureka自我保护 “好死不如赖活着” 一句话：某时刻某一个微服务不可用了，eureka不会立刻清理，依旧会对该微服务的信息进行保存 什么是自我保护模式？ 默认情况下，如果EurekaServer在一定时间内没有接收到某个微服务实例的心跳，EurekaServer将会注销该实例==（默认90秒）==。但是当网络分区故障发生时，微服务与EurekaServer之间无法正常通信，以上行为可能变得非常危险了——因为微服务本身其实是健康的，此时本不应该注销这个微服务。Eureka通过“自我保护模式”来解决这个问题——当EurekaServer节点在短时间内丢失过多客户端时（可能发生了网络分区故障），那么这个节点就会进入自我保护模式。一旦进入该模式，EurekaServer就会保护服务注册表中的信息，不再删除服务注册表中的数据（也就是不会注销任何微服务）。当网络故障恢复后，该Eureka Server节点会自动退出自我保护模式。 ==在自我保护模式中，Eureka Server会保护服务注册表中的信息，不再注销任何服务实例。当它收到的心跳数重新恢复到阈值以上时，该Eureka Server节点就会自动退出自我保护模式。它的设计哲学就是宁可保留错误的服务注册信息，也不盲目注销任何可能健康的服务实例。一句话讲解：好死不如赖活着== 综上，自我保护模式是一种应对网络异常的安全保护措施。它的架构哲学是宁可同时保留所有微服务（健康的微服务和不健康的微服务都会保留），也不盲目注销任何健康的微服务。使用自我保护模式，可以让Eureka集群更加的健壮、稳定。 ==在Spring Cloud中，可以使用eureka.server.enable-self-preservation = false 禁用自我保护模式。== Eureka 服务发现microservicecloud-provider-dept-8001服务发现Discovery 对于注册进eureka里面的微服务，可以通过服务发现来获得该服务的信息 1、在服务提供者的controller中添加 12345678910111213141516@Autowiredprivate DiscoveryClient client;@RequestMapping(value = \"/dept/discovery\", method = RequestMethod.GET)public Object discovery()&#123; List&lt;String&gt; list = client.getServices(); System.out.println(\"**********\" + list); List&lt;ServiceInstance&gt; srvList = client.getInstances(\"MICROSERVICECLOUD-DEPT\"); for (ServiceInstance element : srvList) &#123; System.out.println(element.getServiceId() + \"\\t\" + element.getHost() + \"\\t\" + element.getPort() + \"\\t\" + element.getUri()); &#125; return this.client;&#125; 2、在服务提供者的主启动类上添加 12345678910@SpringBootApplication@EnableEurekaClient //本服务启动后会自动注册进eureka服务中@EnableDiscoveryClient //服务发现public class DeptProvider8001_App&#123; public static void main(String[] args) &#123; SpringApplication.run(DeptProvider8001_App.class, args); &#125;&#125; 3、启动Euraka 服务 和 服务提供者 访问 http://localhost:8001/dept/discovery","categories":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/tags/springcloud/"}],"keywords":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}]},{"title":"springcloud","slug":"三、（2）actuator与注册微服务信息完善 & 访问信息有IP信息提示 & 超链接点击服务报告ErrorPage","date":"2019-01-02T12:19:32.000Z","updated":"2019-02-14T08:29:46.304Z","comments":true,"path":"2019/01/02/三、（2）actuator与注册微服务信息完善 & 访问信息有IP信息提示 & 超链接点击服务报告ErrorPage/","link":"","permalink":"http://yoursite.com/2019/01/02/三、（2）actuator与注册微服务信息完善 & 访问信息有IP信息提示 & 超链接点击服务报告ErrorPage/","excerpt":"","text":"actuator与注册微服务信息完善如何修改status 名称 修改client 的yml 123456eureka: client: #客户端注册进eureka服务列表内 service-url: defaultZone: http://localhost:7001/eureka instance: instance-id: microservicecloud-dept8001 效果 访问信息有IP信息提示修改yml 1234567eureka: client: #客户端注册进eureka服务列表内 service-url: defaultZone: http://localhost:7001/eureka instance: instance-id: microservicecloud-dept8001 #自定义服务名称信息 prefer-ip-address: true #访问路径可以显示IP地址 效果 超链接点击服务报告ErrorPage1、修改pom.xml 12345&lt;!-- actuator监控信息完善 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; 2、yml 12345info: app.name: atguigu-microservicecloud company.name: www.atguigu.com build.artifactId: $&#123;project.artifactId&#125; build.version: $&#123;project.version&#125; 3、修改==父工程==的pom.xml 用来解析,为什么在父工程，因为可以避免在每个工程添加 12build.artifactId: $project.artifactId$build.version: $project.version$ pom.xml 修改 用来解析 上面的值 1234567891011121314151617181920&lt;build&gt; &lt;finalName&gt;microservicecloud&lt;/finalName&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;delimiters&gt; &lt;delimit&gt;$&lt;/delimit&gt; &lt;/delimiters&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt;","categories":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/tags/springcloud/"}],"keywords":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}]},{"title":"springcloud","slug":"三、（1） 构建Euraka服务&&将已有的部门微服务注册进eureka服务中心","date":"2018-12-30T12:19:32.000Z","updated":"2019-02-14T08:29:36.765Z","comments":true,"path":"2018/12/30/三、（1） 构建Euraka服务&&将已有的部门微服务注册进eureka服务中心/","link":"","permalink":"http://yoursite.com/2018/12/30/三、（1） 构建Euraka服务&&将已有的部门微服务注册进eureka服务中心/","excerpt":"","text":"构建Euraka服务1、新建 microservicecloud-eureka-7001 的module 2、pom.xml 123456789101112131415161718192021222324252627282930&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;com.atguigu.springcloud&lt;/groupId&gt; &lt;artifactId&gt;microservicecloud&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;artifactId&gt;microservicecloud-eureka-7001&lt;/artifactId&gt; &lt;dependencies&gt; &lt;!--eureka-server服务端 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 修改后立即生效，热部署 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;springloaded&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 3、yml 1234567891011server: port: 7001 eureka: instance: hostname: localhost #eureka服务端的实例名称 client: register-with-eureka: false #false表示不向注册中心注册自己。 fetch-registry: false #false表示自己端就是注册中心，我的职责就是维护服务实例，并不需要去检索服务 service-url: defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/ #设置与Eureka Server交互的地址查询服务和注册服务都需要依赖这个地址。 4、在启动类上加上 @EnableEurekaServer 123456789101112131415package com.atguigu.springcloud;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;@SpringBootApplication@EnableEurekaServer // EurekaServer服务器端启动类,接受其它微服务注册进来public class EurekaServer7001_App&#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaServer7001_App.class, args); &#125;&#125; 5、启动主程序后访问 http://localhost:7001/ 出现以下 将已有的部门微服务注册进eureka服务中心microservicecloud-provider-dept-8001 1、修改pom.xml 123456789&lt;!-- 将微服务provider侧注册进eureka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; 完整的pom.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;com.atguigu.springcloud&lt;/groupId&gt; &lt;artifactId&gt;microservicecloud&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;artifactId&gt;microservicecloud-provider-dept-8001&lt;/artifactId&gt; &lt;dependencies&gt; &lt;!-- 引入自己定义的api通用包，可以使用Dept部门Entity --&gt; &lt;dependency&gt; &lt;groupId&gt;com.atguigu.springcloud&lt;/groupId&gt; &lt;artifactId&gt;microservicecloud-api&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 将微服务provider侧注册进eureka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-core&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jetty&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 修改后立即生效，热部署 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;springloaded&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 2、yml 1234eureka: client: #客户端注册进eureka服务列表内 service-url: defaultZone: http://localhost:7001/eureka 3、修改主启动类 @EnableEurekaClient //本服务启动后会自动注册进eureka服务中 12345678910@SpringBootApplication@EnableEurekaClient //本服务启动后会自动注册进eureka服务中//@EnableDiscoveryClient //服务发现public class DeptProvider8001_App&#123; public static void main(String[] args) &#123; SpringApplication.run(DeptProvider8001_App.class, args); &#125;&#125; 4、启动主启动类 访问 http://localhost:7001/ 显示","categories":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/tags/springcloud/"}],"keywords":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}]},{"title":"springcloud","slug":"三、Eureka服务注册与发现","date":"2018-12-28T12:19:32.000Z","updated":"2019-02-14T08:29:32.229Z","comments":true,"path":"2018/12/28/三、Eureka服务注册与发现/","link":"","permalink":"http://yoursite.com/2018/12/28/三、Eureka服务注册与发现/","excerpt":"","text":"三、Eureka服务注册与发现springcloud 的门神——Eureka Euraka发音 英 [,j?(?)’ri?k?] 美 [j?’rik?] n. (Eureka)人名；(西)欧雷卡 int. 有了！；找到了！ Eureka是什么Eureka是Netflix的一个子模块，也是核心模块之一。Eureka是一个基于REST的服务，用于定位服务，以实现云端中间层服务发现和故障转移。服务注册与发现对于微服务架构来说是非常重要的，有了服务发现与注册，==只需要使用服务的标识符，就可以访问到服务，而不需要修改服务调用的配置文件了==。==功能类似于dubbo的注册中心，比如Zookeeper。== Netflix在设计Eureka时遵守的就是AP原则CAP原则又称CAP定理，指的是在一个分布式系统中，Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），三者不可兼得 Euraka 原理Spring Cloud 封装了 Netflix 公司开发的 Eureka 模块来实现服务注册和发现(请对比Zookeeper)。 Eureka 采用了 ==C-S 的设计架构==。Eureka Server 作为服务注册功能的服务器，它是服务注册中心。 而系统中的其他微服务，使用 Eureka 的客户端连接到 Eureka Server并维持心跳连接。这样系统的维护人员就可以通过 Eureka Server 来监控系统中各个微服务是否正常运行。SpringCloud 的一些其他模块（比如Zuul）就可以通过 Eureka Server 来发现系统中的其他微服务，并执行相关的逻辑。请注意和Dubbo的架构对比 ? Eureka包含两个组件：==Eureka Server和Eureka Client====Eureka Server==提供服务注册服务各个节点启动后，会在EurekaServer中进行注册，这样EurekaServer中的服务注册表中将会存储所有可用服务节点的信息，服务节点的信息可以在界面中直观的看到 EurekaClient是一个Java客户端，用于简化Eureka Server的交互，==客户端同时也具备一个内置的、使用轮询(round-robin)负载算法的负载均衡器。==在应用启动后，将会向Eureka Server发送心跳(默认周期为30秒)。如果Eureka Server在==多个心跳周期内没有接收到某个节点的心跳，EurekaServer将会从服务注册表中把这个服务节点移除（默认90秒）==","categories":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/tags/springcloud/"}],"keywords":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}]},{"title":"springcloud","slug":"二、SpringCloud 是什么","date":"2018-12-26T12:19:32.000Z","updated":"2019-02-14T08:29:28.815Z","comments":true,"path":"2018/12/26/二、SpringCloud 是什么/","link":"","permalink":"http://yoursite.com/2018/12/26/二、SpringCloud 是什么/","excerpt":"","text":"二、SpringCloud 是什么 SpringCloud=分布式微服务架构下的一站式解决方案，是各个微服务架构落地技术的集合体，俗称==微服务全家桶== SpringCloud，基于SpringBoot提供了一套微服务解决方案，包括服务注册与发现，配置中心，全链路监控，服务网关，负载均衡，熔断器等组件，除了基于NetFlix的开源组件做高度抽象封装之外，还有一些选型中立的开源组件。 SpringCloud利用SpringBoot的开发便利性巧妙地简化了分布式系统基础设施的开发，SpringCloud为开发人员提供了快速构建分布式系统的一些工具，包括==配置管理、服务发现、断路器、路由、微代理、事件总线、全局锁、决策竞选、分布式会话==等等,它们都可以用SpringBoot的开发风格做到一键启动和部署。 SpringBoot并没有重复制造轮子，它只是将目前各家公司开发的比较成熟、经得起实际考验的服务框架组合起来，==通过SpringBoot风格进行再封装屏蔽掉了复杂的配置和实现原理，最终给开发者留出了一套简单易懂、易部署和易维护的分布式系统开发工具包== SpringCloud和SpringBoot是什么关系SpringBoot专注于快速方便的开发单个个体微服务。 SpringCloud是关注全局的微服务协调整理治理框架，它将SpringBoot开发的一个个单体微服务整合并管理起来，为各个微服务之间提供，配置管理、服务发现、断路器、路由、微代理、事件总线、全局锁、决策竞选、分布式会话等等集成服务 ==SpringBoot可以离开SpringCloud独立使用开发项目，但是SpringCloud离不开SpringBoot，属于依赖的关系==. SpringBoot==专注于快速、方便的开发单个微服务个体，SpringCloud关注全局的服务治理框架==。 国内那些公司在用SpringCloud","categories":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/tags/springcloud/"}],"keywords":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}]},{"title":"springcloud","slug":"一、微服务背景知识","date":"2018-12-25T12:19:32.000Z","updated":"2019-02-14T08:26:14.317Z","comments":true,"path":"2018/12/25/一、微服务背景知识/","link":"","permalink":"http://yoursite.com/2018/12/25/一、微服务背景知识/","excerpt":"","text":"一、微服务背景知识五大神兽Eureka(服务注册) 服务注册和发现，高可用服务注册中心 Config(配置中心) 常规的server 和client 使用Bus 结合RabbitMQ 实现自动刷新 Ribbon (服务通讯) RestTemplate Feign Ribbon通讯方式 Zuul(动态路由) Hystrix 熔断机制 容器编排：Docker + Rancher 常见的问题 什么是微服务 微服务之间时如何建立通讯的 springCloud 和Dubbo 有哪些区别 通讯方式不同： springcloud 是通常是基于HTTP的RESTful API; Dubbo 是采用RPC(远程过程调用) SpringBoot 和SpringCloud,请谈谈你对他们的理解 什么是服务熔断？什么是服务降级 微服务的优缺点分别是什么？说说你在项目开发中遇到的坑 你所知道的微服务技术栈有哪些？请举例一二 Eureka 和 ZooKeeper 都可以做服务注册与发现，说说两者区别？ 微服务概述业界大牛马丁.福勒（Martin Fowler） 指出：==微服务架构是一种架构模式或者说是一种架构风格，它提倡将单一应用程序划分成一组小的服务，==每个服务运行在其独立的自己的==进程==中，服务之间互相协调、互相配合，为用户提供最终价值。服务之间采用轻量级的通信机制互相沟通（通常是基于HTTP的RESTful API）。每个服务都围绕着具体业务进行构建，并且能够被独立地部署到生产环境、类生产环境等。另外，应尽量避免统一的、集中式的服务管理机制，对具体的一个服务而言，应根据业务上下文，选择合适的语言、工具对其进行构建，可以有一个非常轻量级的集中式管理来协调这些服务，可以使用不同的语言来编写服务，也可以使用不同的数据存储。 技术角度理解微服务化的核心就是将传统的一站式应用，根据业务拆分成一个一个的服务，彻底地去耦合,每一个微服务提供单个业务功能的服务，一个服务做一件事，从技术角度看就是一种小而独立的处理过程，类似进程概念，能够自行单独启动或销毁，拥有自己独立的数据库。 马丁的论文地址中文版 All in one 微服务 特性一：“组件化”与“多服务” 不拆分服务 拆分公共服务 特性二：围绕“业务功能”组织团队 拆分业务 特性三：“做产品”而不是“做项目” 有的团队只做前端 或者后端 每个团队前中后都有 特性四：“智能端点”与“傻瓜管道” restful 特性五：“去中心化”地治理技术 特性六：“去中心化”地管理数据 只有1个中心数据库 每个微服务各自有库 特性七：“基础设施”自动化 自动部署 自动部署 特性八：“容错”设计 特性九：“演进式”设计 版本号常用 版本号最后万不得已用 微服务强调的是服务的大小，它关注的是某一个点，是具体解决某一个问题/提供落地对应服务的一个服务应用,狭意的看,可以看作Eclipse里面的一个个微服务工程/或者Module 微服务架构微服务架构是一种架构模式，它提倡将单独应用程序划分成一组小的服务，服务之间互相协调、互相配合，为用户提供最终价值。每个服务运行在其独特的进程中，服务与服务间采用轻量级的通信机制互相协作（通常是基于HTTP协议的RESTful API）。每个服务都围绕着具体业务进行构建，并且能够被独立的部署到生产环境、类生产环境等。另外，应当尽量避免统一的、集中式的服务管理机制，对具体的一个服务而言，应根据业务上下文，选择合适的语言、工具对其进行构建。 微服务的优缺点优点每个服务足够内聚，足够小，代码容易理解这样能聚焦一个指定的业务功能或业务需求 开发简单、开发效率提高，一个服务可能就是专一的只干一件事。 微服务能够被小团队单独开发，这个小团队是2到5人的开发人员组成。 微服务是松耦合的，是有功能意义的服务，无论是在开发阶段或部署阶段都是独立的。 微服务能使用不同的语言开发。 易于和第三方集成，微服务允许容易且灵活的方式集成自动部署，通过持续集成工具，如Jenkins, Hudson, bamboo 。 微服务易于被一个开发人员理解，修改和维护，这样小团队能够更关注自己的工作成果。无需通过合作才能体现价值。 ==微服务允许你利用融合最新技术。== ==微服务只是业务逻辑的代码，不会和HTML,CSS 或其他界面组件混合。== ==每个微服务都有自己的存储能力，可以有自己的数据库。也可以有统一数据库。== 缺点开发人员要处理分布式系统的复杂性 多服务运维难度，随着服务的增加，运维的压力也在增大 系统部署依赖 服务间通信成本 数据一致性 系统集成测试 性能监控…… 微服务条目 落地技术 备注服务开发 Springboot、Spring、SpringMVC 微服务的技术栈? 微服务条目 落地技术 备注 服务开发 Springboot、Spring、SpringMVC 服务配置与管理 Netflix公司的Archaius、阿里的Diamond等 服务注册与发现 Eureka、Consul、Zookeeper等 服务调用 Rest、RPC、gRPC 服务熔断器 Hystrix、Envoy等 负载均衡 Ribbon、Nginx等 服务接口调用(客户端调用服务的简化工具) Feign等 消息队列 Kafka、RabbitMQ、ActiveMQ等 服务配置中心管理 SpringCloudConfig、Chef等 服务路由（API网关） Zuul等 服务监控 Zabbix、Nagios、Metrics、Spectator等 全链路追踪 Zipkin，Brave、Dapper等 服务部署 Docker、OpenStack、Kubernetes等 数据流操作开发包 SpringCloud Stream（封装与Redis,Rabbit、Kafka等发送接收消息） 事件消息总线 Spring Cloud Bus 为什么选择SpringCloud作为微服务架构 当前各大IT公司用的微服务架构有哪些?阿里Dubbo/HSF （好舒服） 京东JSF （京舒服） 新浪微博Motan （茅台） 当当网DubboX","categories":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/tags/springcloud/"}],"keywords":[{"name":"springcloud","slug":"springcloud","permalink":"http://yoursite.com/categories/springcloud/"}]},{"title":"G_Spring Boot与监控管理","slug":"十六、SpringBoot与监控管理","date":"2018-11-01T12:19:32.000Z","updated":"2018-11-26T01:37:55.816Z","comments":true,"path":"2018/11/01/十六、SpringBoot与监控管理/","link":"","permalink":"http://yoursite.com/2018/11/01/十六、SpringBoot与监控管理/","excerpt":"","text":"SpringBoot与监控管理==通过引入spring-boot-starter-actuator==，可以使用Spring Boot为我们提供的准生产环境下的应用监控和管理功能。我们可以通过HTTP，JMX，SSH协议来进行操作，自动得到审计、健康及指标信息等 步骤： ==引入spring-boot-starter-actuator== 通过http方式访问监控端点 可进行shutdown（POST 提交，此端点默认关闭） 各种监控指标通过 http://localhost:8080/autoconfig 进行访问 端点名 描述 autoconfig 所有自动配置信息 auditevents 审计事件 beans 所有Bean的信息 configprops 所有配置属性 dump 线程状态信息 env 当前环境信息 health 应用健康状况 info 当前应用信息 metrics 应用的各项指标 mappings 应用@RequestMapping映射路径 shutdown 关闭当前应用（默认关闭） trace 追踪信息（最新的http请求） 定制端点信息 定制端点一般通过endpoints+端点名+属性名来设置。 修改端点id（endpoints.beans.id=mybeans 或者 endpoints.beans.path=/mybeans） ==开启远程应用关闭功能（endpoints.shutdown.enabled=true）==可以通过post 请求关闭应用 关闭端点（endpoints.beans.enabled=false） 开启所需端点 endpoints.enabled=false endpoints.beans.enabled=true 定制端点访问根路径 management.context-path=/manage 关闭http端点 management.port = -1 application.properties 常见配置 12management.context-path=/manage management.port = 8181 定制健康信息http://localhost:8181/manage/health 如果使用redis那么就会有redis 的健康信息 定制MyApp 的健康信息 12345678910111213import org.springframework.boot.actuate.health.HealthIndicator;import org.springframework.stereotype.Component;@Componentpublic class MyAppHealthIndicator implements HealthIndicator &#123; @Override public Health health() &#123; int errorCode = check(); // perform some specific health check if (errorCode != 0) &#123; return Health.down().withDetail(\"Error Code\", errorCode).build(); &#125; return Health.up().build(); &#125;&#125;","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"},{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"监控管理","slug":"监控管理","permalink":"http://yoursite.com/tags/监控管理/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}]},{"title":"D_Spring Boot与security","slug":"十三、Spring Boot与安全","date":"2018-10-25T12:19:32.000Z","updated":"2018-11-26T01:39:24.446Z","comments":true,"path":"2018/10/25/十三、Spring Boot与安全/","link":"","permalink":"http://yoursite.com/2018/10/25/十三、Spring Boot与安全/","excerpt":"","text":"Spring Boot与安全安全框架 Apache Shiro 简单 易用 Spring Security Spring Security是针对Spring项目的安全框架，也是Spring Boot底层安全模块默认的技术选型。 Spring Security仅需引入spring-boot-starter-security模块，进行少量的配置，即可实现强大的安全管理。 官网文档 几个类： 123WebSecurityConfigurerAdapter：自定义Security策略AuthenticationManagerBuilder：自定义认证策略@EnableWebSecurity ：开启 WebSecurity 模式 ?应用程序的两个主要区域是==“认证”和“授权”（或者访问控制）==。这两个主要区域是Spring Security 的两个目标。 ?“认证”（Authentication），是建立一个他声明的主体的过程（一个“主体”一般是指用户，设备或一些可以在你的应用程序中执行动作的其他系统）。 ?“授权”（Authorization）指确定一个主体是否允许在你的应用程序执行一个动作的过程。为了抵达需要授权的店，主体的身份已经有认证过程建立。 使用场景 1.登陆/注销 –HttpSecurity配置登陆、注销功能 2.Thymeleaf提供的SpringSecurity标签支持 –需要引入thymeleaf-extras-springsecurity4 –sec:authentication=“name”获得当前用户的用户名 –sec:authorize=“hasRole(‘ADMIN’)”当前用户必须拥有ADMIN权限时才会显示标签内容 3.remember me –表单添加remember-me的checkbox –配置启用remember-me功能 4.CSRF（Cross-site request forgery）跨站请求伪造 HttpSecurity 启用 csrf 功能，会为表单添加 _csrf的值，提交携带来预防CSRF；","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"},{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"security","slug":"security","permalink":"http://yoursite.com/tags/security/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}]},{"title":"F_Spring Boot与开发热部署","slug":"十五、SpringBoot与开发热部署","date":"2018-10-25T12:19:32.000Z","updated":"2018-11-26T01:38:49.223Z","comments":true,"path":"2018/10/25/十五、SpringBoot与开发热部署/","link":"","permalink":"http://yoursite.com/2018/10/25/十五、SpringBoot与开发热部署/","excerpt":"","text":"SpringBoot与开发热部署热部署在开发中我们修改一个Java文件后想看到效果不得不重启应用，这导致大量时间花费，我们希望不重启应用的情况下，程序可以自动部署（热部署）。有以下四种情况，如何能实现热部署。 1、模板引擎 在Spring Boot中开发情况下禁用模板引擎的cache 页面模板改变ctrl+F9可以重新编译当前页面并生效 2、Spring Loaded Spring官方提供的热部署程序，实现修改类文件的热部署 下载Spring Loaded（项目地址https://github.com/spring-projects/spring-loaded） 添加运行时参数； javaagent:C:/springloaded-1.2.5.RELEASE.jar –noverify 3、JRebel 收费的一个热部署软件 安装插件使用即可 4、Spring Boot Devtools（推荐） 引入依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;/dependency&gt; IDEA使用ctrl+F9 或做一些小调整 Intellij IEDA和Eclipse不同，Eclipse设置了自动编译之后，修改类它会自动编译，而IDEA在非RUN或DEBUG情况下才会自动编译（前提是你已经设置了Auto-Compile）。 设置自动编译（settings-compiler-make project automatically） ctrl+shift+alt+/（maintenance） 勾选compiler.automake.allow.when.app.running","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"},{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"热部署","slug":"热部署","permalink":"http://yoursite.com/tags/热部署/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}]},{"title":"E_Spring Boot与分布式","slug":"十四、Spring Boot与分布式","date":"2018-10-25T12:19:32.000Z","updated":"2018-11-26T01:39:11.404Z","comments":true,"path":"2018/10/25/十四、Spring Boot与分布式/","link":"","permalink":"http://yoursite.com/2018/10/25/十四、Spring Boot与分布式/","excerpt":"","text":"Spring Boot与分布式分布式应用框架 国内常用zookeeper+dubbo Spring Boot推荐使用全栈的Spring，Spring Boot+Spring Cloud 各种常见的架构 单一应用架构当网站流量很小时，只需一个应用，将所有功能都部署在一起，以减少部署节点和成本。此时，用于简化增删改查工作量的数据访问框架(ORM)是关键。 垂直应用架构当访问量逐渐增大，单一应用增加机器带来的加速度越来越小，将应用拆成互不相干的几个应用，以提升效率。此时，用于加速前端页面开发的Web框架(MVC)是关键。 分布式服务架构当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。此时，用于提高业务复用及整合的分布式服务框架(RPC)是关键。 流动计算架构当服务越来越多，容量的评估，小服务资源的浪费等问题逐渐显现，此时需增加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。此时，用于提高机器利用率的资源调度和治理中心(SOA)是关键。 ==RPC(远程过程调用)：Dubbo 以及Spring cloud== ==但是还需要的是注册中心：ZooKeeper== ==类似于 注册中心就是婚介所，而RPC 就是婚介所的男生找女生的规则，不会让一个男生乱找女生== Zookeeper和Dubbo ZooKeeper ZooKeeper 是一个分布式的，开放源码的分布式应用程序协调服务。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。 Dubbo Dubbo是Alibaba开源的分布式服务框架，它最大的特点是按照分层的方式来架构，使用这种方式可以使各个层之间解耦合（或者最大限度地松耦合）。从服务模型的角度来看，Dubbo采用的是一种非常简单的模型，要么是提供方提供服务，要么是消费方消费服务，所以基于这一点可以抽象出服务提供方（Provider）和服务消费方（Consumer）两个角色。 安装zookeeper参考文档 https://hub.docker.com/_/zookeeper/ 1docker pull registry.docker-cn.com/library/zookeeper Start a Zookeeper server instance 12&gt; $ docker run --name some-zookeeper --restart always -d zookeeper&gt; &gt; This image includes EXPOSE 2181 2888 3888 (the zookeeper client port, follower port, election port respectively 2181：对外提供端口 2888：内部同步端口 3888：节点挂了，选举端口 123docker run --name myzookeeper -p 2181:2181 --restart always -d zookeeper查看服务docker run -it --rm --link myzookeeper:zookeeper zookeeper zkCli.sh -server zookeeper 原理：有很多服务，例如甲乙两个服务在不同的机器上：甲是消费者（买票的），乙是电影院（卖票的）； 乙把自己的买票服务==通过Dubbo的服务提供者==登记在zookeeper(注册中心)上（通过安装Dubbo以及zookeeper客户端,配置Dubbo,），然后甲需要把乙的服务的接口（全类名)复制在本地,并且安装配置好Dubbo，然后甲就可以用（乙接口的全类名）RPC(远程接口调用)乙发布在zookeeper 上的服务（就是找乙提供的服务全类名一样的服务）； 步骤：1.引入Dubbo 和zookeeper client 的依赖包 2.配置dubbo 的扫描包和注册中心地址 3.使用==dubbo 的@Service== 发布服务 SpringCloudSpring Cloud Spring Cloud是一个分布式的整体解决方案。Spring Cloud 为开发者提供了在分布式系统（配置管理，服务发现，熔断，路由，微代理，控制总线，一次性token，全局琐，leader选举，分布式session，集群状态）中快速构建的工具，使用Spring Cloud的开发者可以快速的启动服务或构建应用、同时能够快速和云平台资源进行对接。 SpringCloud分布式开发五大常用组件?服务发现——Netflix Eureka （注册中心） ?客服端负载均衡——Netflix Ribbon ?断路器——Netflix Hystrix ?服务网关——Netflix Zuul ?分布式配置——Spring Cloud Config 实现步骤：注册中心1、新建注册中心 Eureka 2、配置文件 application.yml 12345678server: port: 8761eureka: instance: hostname: eureka-server # eureka的主机名 client: register-with-eureka: false # 不把自己注册到eureka上 fetch-registry: false # 不从eureka上获取服务的注册信息 3、在主程序加上注解，使eureka 生效 12345678@EnableEurekaServer@SpringBootApplicationpublic class EurekaApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaApplication.class, args); &#125;&#125; 4、访问服务 1http://localhost:8761/eureka/ 服务提供者1、application.yml 12345678910server: port: 8001spring: application: name: ticket-providereureka: instance: prefer-ip-address: true # 注册服务时候使用ip 作为地址# client:# service-url: http://localhost:8761/eureka/ 2、service 123456789101112131415161718192021222324package com.springboot.springcloud.ticketprovider.service;import org.springframework.stereotype.Service;@Servicepublic class TicketProviderImp implements TicketProviderService &#123; private int count = 10; @Override public String buyTicket(int num) &#123; if(count &gt;= num)&#123; count = count - num; System.out.println(\"买到\"+num+\"张票;\"+\"还剩：\"+count+\"张票\"); &#125;else &#123; System.out.println(\"买到\"+count+\"张票;\"+\"还剩：\"+0+\"张票\"); System.out.println(\"还有\"+(num-count)+\"张票没有买到\"); count = 0; &#125; return \"》》》》》》》》》结束《《《《《《《《\"; &#125;&#125; 3、controller 1234567891011121314151617181920212223package com.springboot.springcloud.ticketprovider.controller;import com.springboot.springcloud.ticketprovider.service.TicketProviderImp;import com.springboot.springcloud.ticketprovider.service.TicketProviderService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RestController;import javax.swing.plaf.PanelUI;@RestControllerpublic class TicketController &#123; @Autowired TicketProviderImp ticketProviderService; @GetMapping(\"/ticket/&#123;id&#125;\") public String buyTicket(@PathVariable(\"id\") int num)&#123; String s = ticketProviderService.buyTicket(num); return s; &#125;&#125; 消费者1、application.yml 12345678910server: port: 8100spring: application: name: consumereureka: instance: prefer-ip-address: true 2、controller 12345678910111213141516171819202122package com.springboot.springcloud.consumer.controller;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RestController;import org.springframework.web.client.RestTemplate;@RestControllerpublic class ConsumerController &#123; @Autowired RestTemplate restTemplate; @GetMapping(\"/buy/&#123;id&#125;\") public String buy(@PathVariable(\"id\") int num)&#123; System.out.println(num); String res = restTemplate.getForObject(\"http://TICKET-PROVIDER/ticket/\"+num,String.class); return res; &#125;&#125; 3、主程序 1234567891011121314@EnableDiscoveryClient //开启服务发现功能@SpringBootApplicationpublic class ConsumerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ConsumerApplication.class, args); &#125; @LoadBalanced //使用负载均衡 @Bean public RestTemplate restTemplate()&#123; return new RestTemplate(); &#125;&#125;","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"},{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/tags/分布式/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}]},{"title":"B_Spring Boot与检索ElasticSearch","slug":"十一、Spring Boot与检索ElasticSearch","date":"2018-10-23T12:19:32.000Z","updated":"2018-11-26T01:34:45.621Z","comments":true,"path":"2018/10/23/十一、Spring Boot与检索ElasticSearch/","link":"","permalink":"http://yoursite.com/2018/10/23/十一、Spring Boot与检索ElasticSearch/","excerpt":"","text":"Spring Boot与检索ElasticSearch简介 Elasticsearch是一个分布式搜索服务，提供Restful API，底层基于Lucene,采用多shard（分片）的方式保证数据安全，并且提供自动resharding的功能 我们的应用经常需要添加检索功能，开源的 ElasticSearch 是目前全文搜索引擎的首选 wiki stackoverflow github 安装1$&gt; docker search elasticsearch 1docker pull registry.docker-cn.com/library/elasticsearch 1docker run -d --name myes -p 9200:9200 -p 9300:9300 -e ES_JAVA_OPTS=&quot;-Xms256m -Xmx512m&quot; -e &quot;discovery.type=single-node&quot; 5acf0e8da90b 123docker rm iddocker ps docker ps -a http://192.168.116.128:9200/ 12345678910111213&#123; \"name\" : \"8_vFKb0\", \"cluster_name\" : \"elasticsearch\", \"cluster_uuid\" : \"tFUd7tU3R2-3xM97aZIgug\", \"version\" : &#123; \"number\" : \"5.6.12\", \"build_hash\" : \"cfe3d9f\", \"build_date\" : \"2018-09-10T20:12:43.732Z\", \"build_snapshot\" : false, \"lucene_version\" : \"6.6.1\" &#125;, \"tagline\" : \"You Know, for Search\"&#125; 中文API 和Mysql 一一对应： 索引——哪个库 类型——哪个表 文档——哪条记录 属性——哪些字段 你也许已经注意到 索引 这个词在 Elasticsearch 语境中包含多重意思， 所以有必要做一点儿说明： 索引（名词）： 如前所述，==一个 索引 类似于传统关系数据库中的一个 数据库== ，是一个存储关系型文档的地方。 索引 (index) 的复数词为 indices 或 indexes 。 索引（动词）： 索引一个文档 就是存储一个文档到一个 索引 （名词）中以便它可以被检索和查询到。==这非常类似于 SQL 语句中的 INSERT 关键词==，除了文档已存在时新文档会替换旧文档情况之外。 倒排索引： 关系型数据库通过增加一个 索引 比如一个 B树（B-tree）索引 到指定的列上，以便提升数据检索速度。Elasticsearch 和 Lucene 使用了一个叫做 倒排索引 的结构来达到相同的目的。 + 默认的，一个文档中的每一个属性都是 被索引 的（有一个倒排索引）和可搜索的。一个没有倒排索引的属性是不能被搜索到的。我们将在 倒排索引 讨论倒排索引的更多细节。 第一个业务需求就是存储雇员数据。 这将会以 雇员文档 的形式存储：一个文档代表一个雇员。存储数据到 Elasticsearch 的行为叫做 索引 ，但在索引一个文档之前，需要确定将文档存储在哪里。 一个 Elasticsearch 集群可以 包含多个 索引 ，相应的每个索引可以包含多个 类型 。 这些不同的类型存储着多个 文档 ，每个文档又有 多个 属性 。 测试通过postman 发送请求 http://192.168.116.128:9200/megacorp/employee/1 1234567&#123; \"first_name\" : \"John\", \"last_name\" : \"Smith\", \"age\" : 25, \"about\" : \"I love to go rock climbing\", \"interests\": [ \"sports\", \"music\" ]&#125; 应答 12345678910111213&#123; \"_index\": \"megacorp\", \"_type\": \"employee\", \"_id\": \"1\", \"_version\": 1, \"result\": \"created\", \"_shards\": &#123; \"total\": 2, \"successful\": 1, \"failed\": 0 &#125;, \"created\": true&#125; restful api 插入数据小结==将 HTTP 命令由 PUT 改为 GET 可以用来检索文档，同样的，可以使用 DELETE 命令来删除文档，以及使用 HEAD 指令来检查文档是否存在。如果想更新已存在的文档，只需再次 PUT 。== 检索搜索所有雇员 GET1http://192.168.116.128:9200/megacorp/employee/_search 搜索姓氏为 Smith 的雇员 GET1http://192.168.116.128:9200/megacorp/employee/_search?q=last_name:Smith 响应报文 “_score”: 0.2876821, //相关性得分 “max_score”: 0.2876821, //最大的得分 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&#123; \"took\": 40, \"timed_out\": false, \"_shards\": &#123; \"total\": 5, \"successful\": 5, \"skipped\": 0, \"failed\": 0 &#125;, \"hits\": &#123; \"total\": 2, \"max_score\": 0.2876821, \"hits\": [ &#123; \"_index\": \"megacorp\", \"_type\": \"employee\", \"_id\": \"2\", \"_score\": 0.2876821, //相关性得分 \"_source\": &#123; \"first_name\": \"Jane\", \"last_name\": \"Smith\", \"age\": 32, \"about\": \"I like to collect rock albums\", \"interests\": [ \"music\" ] &#125; &#125;, &#123; \"_index\": \"megacorp\", \"_type\": \"employee\", \"_id\": \"1\", \"_score\": 0.2876821, \"_source\": &#123; \"first_name\": \"John\", \"last_name\": \"Smith\", \"age\": 25, \"about\": \"I love to go rock climbing\", \"interests\": [ \"sports\", \"music\" ] &#125; &#125; ] &#125;&#125; 非常重要的查询表达式12345678GET /megacorp/employee/_search&#123; \"query\" : &#123; \"match\" : &#123; \"last_name\" : \"Smith\" &#125; &#125;&#125; 12345678910111213141516171819&#123; \"query\" : &#123; \"bool\": &#123; \"must\": &#123; \"match\" : &#123; \"last_name\" : \"smith\" &#125; &#125;, \"filter\": &#123; \"range\" : &#123; \"age\" : &#123; \"gt\" : 30 &#125; &#125; &#125; &#125; &#125;&#125; //1部分与我们之前使用的 match 查询 一样。//2部分是一个 range 过滤器 ， 它能找到年龄大于 30 的文档，其中 gt 表示_大于(_great than)。 高级搜索全文搜索math ——about 按照每个单词分别检索 搜索下所有喜欢攀岩（rock climbing）的雇员 12345678GET /megacorp/employee/_search&#123; \"query\" : &#123; \"match\" : &#123; \"about\" : \"rock climbing\" &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031&#123; ... \"hits\": &#123; \"total\": 2, \"max_score\": 0.16273327, \"hits\": [ &#123; ... \"_score\": 0.16273327, \"_source\": &#123; \"first_name\": \"John\", \"last_name\": \"Smith\", \"age\": 25, \"about\": \"I love to go rock climbing\", \"interests\": [ \"sports\", \"music\" ] &#125; &#125;, &#123; ... \"_score\": 0.016878016, \"_source\": &#123; \"first_name\": \"Jane\", \"last_name\": \"Smith\", \"age\": 32, \"about\": \"I like to collect rock albums\", \"interests\": [ \"music\" ] &#125; &#125; ] &#125;&#125; 相关性得分Elasticsearch 默认按照相关性得分排序，即每个文档跟查询的匹配程度。第一个最高得分的结果很明显：John Smith 的 about 属性清楚地写着 “rock climbing” 。 但为什么 Jane Smith 也作为结果返回了呢？原因是她的 about 属性里提到了 “rock” 。因为只有 “rock” 而没有 “climbing” ，所以她的相关性得分低于 John 的。 这是一个很好的案例，阐明了 Elasticsearch 如何 在 全文属性上搜索并返回相关性最强的结果。Elasticsearch中的 相关性 概念非常重要，也是完全区别于传统关系型数据库的一个概念，数据库中的一条记录要么匹配要么不匹配。 短语搜索match_phrase ——about 按照完整的匹配 12345678GET /megacorp/employee/_search&#123; \"query\" : &#123; \"match_phrase\" : &#123; \"about\" : \"rock climbing\" &#125; &#125;&#125; 高亮搜索12345678910111213GET /megacorp/employee/_search&#123; \"query\" : &#123; \"match_phrase\" : &#123; \"about\" : \"rock climbing\" &#125; &#125;, \"highlight\": &#123; \"fields\" : &#123; \"about\" : &#123;&#125; &#125; &#125;&#125; 响应报文 12345678910111213141516171819202122232425&#123; ... \"hits\": &#123; \"total\": 1, \"max_score\": 0.23013961, \"hits\": [ &#123; ... \"_score\": 0.23013961, \"_source\": &#123; \"first_name\": \"John\", \"last_name\": \"Smith\", \"age\": 25, \"about\": \"I love to go rock climbing\", \"interests\": [ \"sports\", \"music\" ] &#125;, \"highlight\": &#123; \"about\": [ \"I love to go &lt;em&gt;rock&lt;/em&gt; &lt;em&gt;climbing&lt;/em&gt;\" ] &#125; &#125; ] &#125;&#125; Springboot 整合Elastic SearchSpringBoot 默认支持两种技术来和ES交互 1、Jest (默认不生效) 需要导入jest 的工具包（io.searchbox.client.JestClient） 2、SpringData ElasticSearch Client 节点信息 clusterNodes；clusterName ElasticsearchTemlate 操作es 编写一个ElasticsearchRespostity SpringData 的方法 基本和 JPA 查数据库一样的 版本问题参考【官网文档】（https://github.com/spring-projects/spring-data-elasticsearch） 官方文档 spring data elasticsearch elasticsearch 3.1.x 6.2.2 3.0.x 5.5.0 2.1.x 2.4.0 2.0.x 2.2.0 1.3.x 1.5.2 如何使用SpringData ElasticSearch 如何定义Repository方法来查寻","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"},{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://yoursite.com/tags/ElasticSearch/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}]},{"title":"C_Spring Boot与任务","slug":"十二、Spring Boot与任务","date":"2018-10-23T12:19:32.000Z","updated":"2018-11-26T01:37:16.825Z","comments":true,"path":"2018/10/23/十二、Spring Boot与任务/","link":"","permalink":"http://yoursite.com/2018/10/23/十二、Spring Boot与任务/","excerpt":"","text":"Spring Boot与任务异步任务在Java应用中，绝大多数情况下都是通过同步的方式来实现交互处理的；但是在处理与第三方系统交互的时候，容易造成响应迟缓的情况，之前大部分都是使用多线程来完成此类任务，其实，在Spring 3.x之后，就已经内置了@Async来完美解决这个问题。 两个注解： @EnableAysnc、@Aysnc 主方法@EnableAsync 开启异步 12345678@EnableAsync@SpringBootApplicationpublic class SpringbootTask10Application &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringbootTask10Application.class, args); &#125;&#125; 控制器接受映射1234567891011121314@RestControllerpublic class AsyncController &#123; @Autowired AsyncService asyncService; @GetMapping(\"/hello\") public String hello()&#123; asyncService.helloService(); return \"success\"; &#125;&#125; 服务器设置异步@Async 12345678910111213141516@Async@Servicepublic class AsyncService &#123; public void helloService()&#123; try &#123; Thread.sleep(3000); System.out.println(\"线程第一句....\"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"线程第二句\"); &#125;&#125; 定时任务项目开发中经常需要执行一些定时任务，比如需要在每天凌晨时候，分析一次前一天的日志信息。Spring为我们提供了异步执行任务调度的方式，提供TaskExecutor 、TaskScheduler 接口。 两个注解：@EnableScheduling、@Scheduled cron表达式： 123456789/** * second（秒）, minute（分）, hour（时）, day of month（日）, month（月）, day of week（周几）. * 例子：0 * * * * MON-FRI * 【0 0/5 14,18 * * ?】 每天14点整，和18点整，每隔5分钟执行一次 * 【0 15 10 ? * 1-6】 每个月的周一至周六10:15分执行一次 * 【0 0 2 ? * 6L】每个月的最后一个周六凌晨2点执行一次 * 【0 0 2 LW * ?】每个月的最后一个工作日凌晨2点执行一次 * 【0 0 2-4 ? * 1#1】每个月的第一个周一凌晨2点到4点期间，每个整点都执行一次； */ 开启定时服务123456789@EnableAsync@EnableScheduling@SpringBootApplicationpublic class SpringbootTask10Application &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringbootTask10Application.class, args); &#125;&#125; 定义调度服务123456789101112131415161718192021222324252627282930@Servicepublic class ScheduleService &#123; /** * second（秒）, minute（分）, hour（时）, day of month（日）, month（月）, day of week（周几）. * 例子：0 * * * * MON-FRI * 【0 0/5 14,18 * * ?】 每天14点整，和18点整，每隔5分钟执行一次 * 【0 15 10 ? * 1-6】 每个月的周一至周六10:15分执行一次 * 【0 0 2 ? * 6L】每个月的最后一个周六凌晨2点执行一次 * 【0 0 2 LW * ?】每个月的最后一个工作日凌晨2点执行一次 * 【0 0 2-4 ? * 1#1】每个月的第一个周一凌晨2点到4点期间，每个整点都执行一次； */ private int count = 0; @Scheduled(cron=\"0,30 * * * * MON-FRI\") public void fuckstart()&#123; System.out.println(\"insert》》》》into&gt;&gt;&gt;&gt;&gt;\"); &#125; @Scheduled(cron=\"1-10,31-55 * * * * MON-FRI\") public void fucking()&#123; System.out.println(\"piston motion \"+ count++); &#125; @Scheduled(cron=\"11,56 * * * * MON-FRI\") public void fuckend()&#123; System.out.println(\"fire out\"); &#125;&#125; 邮件服务复杂版本 public void MimeMessage() 的效果如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455@RunWith(SpringRunner.class)@SpringBootTestpublic class AsMailTaskApplicationTests &#123;@AutowiredJavaMailSenderImpl mailSender;@Testpublic void SimpleMailMessage() &#123; //简单邮件 SimpleMailMessage message = new SimpleMailMessage(); //邮件标题 message.setSubject(\"ITAEM团队招新了\"); //邮件内容 message.setText(\"招新的方向：前端、后台、安卓、UI、AI、大数据\"); //发送者：必填 message.setTo(\"123456789@qq.com\"); //接收者：必填 message.setFrom(\"987654321@qq.com\"); mailSender.send(message);&#125;@Testpublic void MimeMessage() throws Exception&#123; //复杂邮件 MimeMessage mimeMessage = mailSender.createMimeMessage(); //邮件发送助手 MimeMessageHelper helper = new MimeMessageHelper(mimeMessage, true); //邮件设置 helper.setSubject(\"ITAEM团队招新了\"); //使用 HTML 格式，true helper.setText(\"招新的方向：&lt;b style='color:red'&gt;前端、后台、安卓、UI、AI、大数据&lt;/b&gt;\",true); //接收者 helper.setTo(\"987654321@qq.com\"); //发送者 helper.setFrom(\"123456789@qq.com\"); //上传附件：文件名、文件路径 helper.addAttachment(\"ITAEM_logo.jpg\",new File(\"C:\\\\Users\\\\linhongcun\\\\Desktop\\\\logo.jpg\")); mailSender.send(mimeMessage);&#125; 2、配置 123456spring.mail.username=123456789@qq.com spring.mail.password=abcdefgspring.mail.host=smtp.qq.comspring.mail.properties.mail.smtp.ssl.enable=trueserver.port=80server.context-path=/ spring.mail.password 可以通过如下方法获取 1、登录qq邮箱，点击 设置 -&gt; 点击 账户 参考博客","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"},{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"任务","slug":"任务","permalink":"http://yoursite.com/tags/任务/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}]},{"title":"A_Spring Boot与消息","slug":"十、Spring Boot与消息","date":"2018-10-20T12:19:32.000Z","updated":"2018-11-26T01:30:51.518Z","comments":true,"path":"2018/10/20/十、Spring Boot与消息/","link":"","permalink":"http://yoursite.com/2018/10/20/十、Spring Boot与消息/","excerpt":"","text":"Spring Boot与消息JMS AMQP RabbitMQ 1.大多应用中，可通过消息服务中间件来提升系统异步通信、扩展解耦能力 2.消息服务中两个重要概念： ​ 消息代理（message broker）和目的地（destination） 当消息发送者发送消息以后，将由消息代理接管，消息代理保证消息传递到指定目的地。 3.消息队列主要有两种形式的目的地 1.队列（queue）：点对点消息通信（point-to-point） 2.主题（topic）：发布（publish）/订阅（subscribe）消息通信 原理 应用场合 基本概念4.点对点式： –消息发送者发送消息，消息代理将其放入一个队列中，消息接收者从队列中获取消息内容，消息读取后被移出队列 –消息只有唯一的发送者和接受者，但并不是说只能有一个接收者 5.发布订阅式： –发送者（发布者）发送消息到主题，多个接收者（订阅者）监听（订阅）这个主题，那么就会在消息到达时同时收到消息 – 6.JMS（Java Message Service）JAVA消息服务： –基于JVM消息代理的规范。ActiveMQ、HornetMQ是JMS实现 – 7.AMQP（Advanced Message Queuing Protocol） –高级消息队列协议，也是一个消息代理的规范，兼容JMS –RabbitMQ是AMQP的实现 JMS AMQP 定义 Java api 网络线级协议 跨语言 否 是 跨平台 否 是 Model 提供两种消息模型： （1）、Peer-2-Peer （2）、Pub/sub 提供了五种消息模型： （1）、direct exchange （2）、fanout exchange （3）、topic change （4）、headers exchange （5）、system exchange 本质来讲，后四种和JMS的pub/sub模型没有太大差别，仅是在路由机制上做了更详细的划分； 支持消息类型 多种消息类型： TextMessage MapMessage BytesMessage StreamMessage ObjectMessage Message （只有消息头和属性） byte[] 当实际应用时，有复杂的消息，可以将消息序列化后发送。 综合评价 JMS 定义了JAVA API层面的标准；在java体系中，多个client均可以通过JMS进行交互，不需要应用修改代码，但是其对跨平台的支持较差； AMQP定义了wire-level层的协议标准；天然具有跨平台、跨语言特性。 8.Spring支持 –spring-jms提供了对JMS的支持 –spring-rabbit提供了对AMQP的支持 –需要ConnectionFactory的实现来连接消息代理 –提供JmsTemplate、RabbitTemplate来发送消息 –@JmsListener（JMS）、@RabbitListener（AMQP）注解在方法上监听消息代理发布的消息 –@EnableJms、@EnableRabbit开启支持 – 9.Spring Boot自动配置 –JmsAutoConfiguration –RabbitAutoConfiguration RabbitMQRabbitMQ简介： RabbitMQ是一个由erlang开发的AMQP(Advanved Message Queue Protocol)的开源实现。 核心概念 Message 消息，消息是不具名的，它由消息头和消息体组成。消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。 Publisher 消息的生产者，也是一个向交换器发布消息的客户端应用程序。 Exchange 交换器，用来接收生产者发送的消息并将这些消息路由给服务器中的队列。 Exchange 有 4 种类型：direct(默认)，fanout, topic, 和headers，不同类型的Exchange 转发消息的策略有所区别 Queue 消息队列，用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。 Binding 绑定，用于消息队列和交换器之间的关联。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表。 Exchange 和Queue的绑定可以是多对多的关系。 Connection 网络连接，比如一个TCP连接。 Channel 信道，多路复用连接中的一条独立的双向数据流通道。信道是建立在真实的TCP连接内的虚拟连接，AMQP 命令都是通过信道发出去的，不管是发布消息、订阅队列还是接收消息，这些动作都是通过信道完成。因为对于操作系统来说建立和销毁 TCP 都是非常昂贵的开销，所以引入了信道的概念，以复用一条 TCP 连接。 Consumer 消息的消费者，表示一个从消息队列中取得消息的客户端应用程序。 Virtual Host 虚拟主机，表示一批交换器、消息队列和相关对象。虚拟主机是共享相同的身份认证和加密环境的独立服务器域。每个 vhost 本质上就是一个 mini 版的 RabbitMQ 服务器，拥有自己的队列、交换器、绑定和权限机制。vhost 是 AMQP 概念的基础，必须在连接时指定，RabbitMQ 默认的 vhost 是 / 。 Broker 表示消息队列服务器实体 RabbitMQ运行机制AMQP 中的消息路由 •AMQP 中消息的路由过程和 Java 开发者熟悉的 JMS 存在一些差别，AMQP 中增加了 Exchange 和 Binding 的角色。生产者把消息发布到 Exchange 上，消息最终到达队列并被消费者接收，而 Binding 决定交换器的消息应该发送到那个队列。 Exchange 类型Exchange分发消息时根据类型的不同分发策略有区别，目前共四种类型：direct、fanout、topic、headers 。headers 匹配 AMQP 消息的 header 而不是路由键， headers 交换器和 direct 交换器完全一致，但性能差很多，目前几乎用不到了，所以直接看另外三种类型： 消息中的路由键（routingkey）如果和 Binding 中的 binding key 一致， 交换器就将消息发到对应的队列中。路由键与队列名完全匹配，如果一个队列绑定到交换机要求路由键为“dog”，则只转发 routingkey 标记为“dog”的消息，不会转发“dog.puppy”，也不会转发“dog.guard”等等。它是完全匹配、单播的模式。 Fanout 每个发到 fanout 类型交换器的消息都会分到所有绑定的队列上去。fanout 交换器不处理路由键，只是简单的将队列绑定到交换器上，每个发送到交换器的消息都会被转发到与该交换器绑定的所有队列上。很像子网广播，每台子网内的主机都获得了一份复制的消息。fanout 类型转发消息是最快的。 topic topic 交换器通过模式匹配分配消息的路由键属性，将路由键和某个模式进行匹配，此时队列需要绑定到一个模式上。它将路由键和绑定键的字符串切分成单词，这些单词之间用点隔开。它同样也会识别两个通配符：符号“#”和符号“”。#匹配0个或多个单词，匹配一个单词。 安装Rabbit MQ 1docker pull registry.docker-cn.com/library/rabbitmq:3-management 1234docker images;REPOSITORY TAG IMAGE ID CREATED SIZEregistry.docker-cn.com/library/rabbitmq 3-management d69a5113ceae 13 days ago 149 MB 运行 1docker run -d -p 5672:5672 -p 15672:15672 --name myrabbitmq d69a5113ceae 访问检查 http://192.168.116.128:15672/#/ guest / guest RabbitMQ整合1.引入 spring-boot-starter-amqp 2.application.yml配置 3.测试RabbitMQ 1.AmqpAdmin：管理组件 2.RabbitTemplate：消息发送处理组件 配置文件1234spring.rabbitmq.host=192.168.116.128spring.rabbitmq.username=guestspring.rabbitmq.password=guest#spring.rabbitmq.virtual-host= ????????? 测试类1234567891011121314151617181920212223242526272829303132333435@RunWith(SpringRunner.class)@SpringBootTestpublic class SpringbootRabbitmqApplicationTests &#123; @Autowired AmqpAdmin amqpAdmin; @Autowired RabbitTemplate rabbitTemplate;/** * 发送消息 */@Testpublic void contextLoads() &#123; Map&lt;String,Object&gt; map = new HashMap&lt;String,Object&gt;(); map.put(\"msg\",\"helloworld\"); map.put(\"data\", Arrays.asList(\"fuck\",123,true)); rabbitTemplate.convertAndSend(\"exchange.direct\",\"atguigu\",map);&#125;/** * 接受消息 */@Testpublic void getMessage()&#123; Message atguigu = rabbitTemplate.receive(\"atguigu\"); System.out.println(atguigu.toString()); Object o = rabbitTemplate.receiveAndConvert(\"atguigu\"); System.out.println(o.getClass()); System.out.println(o.toString());&#125; json格式转换 pom.xml 1234&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;&lt;/dependency&gt; 模板类 RabbitTemplate MessageConverter 123456789101112131415package rabbitmq.springboot_rabbitmq.config;import org.springframework.amqp.support.converter.Jackson2JsonMessageConverter;import org.springframework.amqp.support.converter.MessageConverter;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configurationpublic class RabbitMqConfiger &#123; @Bean public MessageConverter mymessageConverter()&#123; return new Jackson2JsonMessageConverter(); &#125;&#125; RabbitMq @RabbitListener 监听器 @EnableRabbit Rabbit 12345678@EnableRabbit@SpringBootApplicationpublic class SpringbootRabbitmqApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringbootRabbitmqApplication.class, args); &#125;&#125; 12345678@Servicepublic class BookService &#123; @RabbitListener(queues = \"atguigu\") public void receiveBook(Book book)&#123; System.out.println(\"收到书\"+book); &#125;&#125; 可能发生的错误 1com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Cannot construct instance of `rabbitmq.springboot_rabbitmq.bean.Book` (no Creators, like default construct, exist): cannot deserialize from Object value (no delegate- or property-based Creator) 采用json构造器解决 12345@JsonCreatorpublic Book(@JsonProperty(\"name\") String name, @JsonProperty(\"auther\") String auther) &#123; this.name = name; this.auther = auther;&#125; AmqpAdmin12345678910111213@AutowiredAmqpAdmin amqpAdmin;@Testpublic void testAmqp()&#123; // Exchange amqpAdmin.declareExchange(new DirectExchange(\"test_exchange\")); //queue amqpAdmin.declareQueue(new Queue(\"test_queue\")); // amqpAdmin.declareBinding(new Binding(\"test_queue\", Binding.DestinationType.QUEUE,\"test_exchange\",\"test_key\",null));&#125;","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"},{"name":"JMS","slug":"JMS","permalink":"http://yoursite.com/tags/JMS/"},{"name":"AMQP","slug":"AMQP","permalink":"http://yoursite.com/tags/AMQP/"},{"name":"activeMq","slug":"activeMq","permalink":"http://yoursite.com/tags/activeMq/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}]},{"title":"16_Spring Boot的Restful CURD(下)","slug":"16_Spring Boot的Restful CURD(下)","date":"2018-10-13T12:19:32.000Z","updated":"2018-11-26T01:34:18.436Z","comments":true,"path":"2018/10/13/16_Spring Boot的Restful CURD(下)/","link":"","permalink":"http://yoursite.com/2018/10/13/16_Spring Boot的Restful CURD(下)/","excerpt":"","text":"6）、CRUD-员工添加添加页面 123456789101112131415161718192021222324252627282930313233343536&lt;form&gt; &lt;div class=\"form-group\"&gt; &lt;label&gt;LastName&lt;/label&gt; &lt;input type=\"text\" class=\"form-control\" placeholder=\"zhangsan\"&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label&gt;Email&lt;/label&gt; &lt;input type=\"email\" class=\"form-control\" placeholder=\"zhangsan@atguigu.com\"&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label&gt;Gender&lt;/label&gt;&lt;br/&gt; &lt;div class=\"form-check form-check-inline\"&gt; &lt;input class=\"form-check-input\" type=\"radio\" name=\"gender\" value=\"1\"&gt; &lt;label class=\"form-check-label\"&gt;男&lt;/label&gt; &lt;/div&gt; &lt;div class=\"form-check form-check-inline\"&gt; &lt;input class=\"form-check-input\" type=\"radio\" name=\"gender\" value=\"0\"&gt; &lt;label class=\"form-check-label\"&gt;女&lt;/label&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label&gt;department&lt;/label&gt; &lt;select class=\"form-control\"&gt; &lt;option&gt;1&lt;/option&gt; &lt;option&gt;2&lt;/option&gt; &lt;option&gt;3&lt;/option&gt; &lt;option&gt;4&lt;/option&gt; &lt;option&gt;5&lt;/option&gt; &lt;/select&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label&gt;Birth&lt;/label&gt; &lt;input type=\"text\" class=\"form-control\" placeholder=\"zhangsan\"&gt; &lt;/div&gt; &lt;button type=\"submit\" class=\"btn btn-primary\"&gt;添加&lt;/button&gt;&lt;/form&gt; 提交的数据格式不对：生日：日期； 2017-12-12；2017/12/12；2017.12.12； 日期的格式化；SpringMVC将页面提交的值需要转换为指定的类型; 2017-12-12—-Date； 类型转换，格式化; 1spring.mvc.date-format=yyyy-MM-dd 默认日期是按照/的方式； 1234567891011121314151617181920 @GetMapping(\"/addemp\") public String toAddPage(Model model)&#123; Collection&lt;Department&gt; departments = departmentDao.getDepartments(); model.addAttribute(\"depts\",departments); logger.info(\"员工增加页面\"); return \"emp/add\"; &#125; @PostMapping(\"/emp\") public String addEmp(Employee employee) &#123;// SpringMvc 自动将请求参数和入参对象的属性进行一一绑定，请求参数名和入参javabean 对象参数保持一致 logger.info(\"增加员工信息\"+employee); employeeDao.save(employee);// redirect 表示重定向到一个地址 / 代表当前项目路径// forward 表示转发到一个地址,// return \"redirect:/emp\"; return \"redirect:/emp\"; &#125;","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}]},{"title":"15_Restful CURD（上）","slug":"15_Spring Boot的Restful CURD(上)","date":"2018-10-10T12:19:32.000Z","updated":"2018-11-26T01:34:21.726Z","comments":true,"path":"2018/10/10/15_Spring Boot的Restful CURD(上)/","link":"","permalink":"http://yoursite.com/2018/10/10/15_Spring Boot的Restful CURD(上)/","excerpt":"","text":"Spring Boot的Restful CURD3）、登陆开发期间模板引擎页面修改以后，要实时生效 1）、禁用模板引擎的缓存 12# 禁用缓存spring.thymeleaf.cache=false 2）、页面修改完成以后ctrl+f9：重新编译； 登陆错误消息的显示 1&lt;p style=\"color: red\" th:text=\"$&#123;msg&#125;\" th:if=\"$&#123;not #strings.isEmpty(msg)&#125;\"&gt;&lt;/p&gt; 4）、拦截器进行登陆检查拦截器 123456789101112131415161718192021222324252627282930/** * 登陆检查， */public class LoginHandlerInterceptor implements HandlerInterceptor &#123; //目标方法执行之前 @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; Object user = request.getSession().getAttribute(\"loginUser\"); if(user == null)&#123; //未登陆，返回登陆页面 request.setAttribute(\"msg\",\"没有权限请先登陆\"); request.getRequestDispatcher(\"/index.html\").forward(request,response); return false; &#125;else&#123; //已登陆，放行请求 return true; &#125; &#125; @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; &#125; @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; &#125;&#125; 注册拦截器 123456789101112131415161718192021 //所有的WebMvcConfigurerAdapter组件都会一起起作用@Configurationpublic class MyMvcConfig implements WebMvcConfigurer&#123; @Override public void addInterceptors(InterceptorRegistry registry) &#123; //静态资源； *.css , *.js //SpringBoot已经做好了静态资源映射 registry.addInterceptor(new LoginHandlerInterceptor()).addPathPatterns(\"/**\") .excludePathPatterns(\"/index.html\",\"/user/login\",\"/\",\"/asserts/**\",\"/webjars/**\",\"/index\"); // .excludePathPatterns(\"/index.html\",\"/\",\"/user/login\",\"/asserts/**\",\"/webjars/**\",\"/static/**\"); &#125; //第一种修改配置方案 @Override public void addViewControllers(ViewControllerRegistry registry) &#123; //浏览器发送 /atguigu 请求发送到 success registry.addViewController(\"/\").setViewName(\"index\"); registry.addViewController(\"/main.html\").setViewName(\"dashboard\"); &#125; 5）、CRUD-员工列表实验要求： 1）、RestfulCRUD：CRUD满足Rest风格； URI： /资源名称/资源标识 HTTP请求方式区分对资源CRUD操作 普通CRUD（uri来区分操作） RestfulCRUD 查询 getEmp emp—-GET 添加 addEmp?xxx emp—-POST 修改 updateEmp?id=xxx&amp;xxx=xx emp/{id}—-PUT 删除 deleteEmp?id=1 emp/{id}—-DELETE 2）、实验的请求架构; 实验功能 请求URI 请求方式 查询所有员工 emps GET 查询某个员工(来到修改页面) emp/1 GET 来到添加页面 emp GET 添加员工 emp POST 来到修改页面（查出员工进行信息回显） emp/1 GET 修改员工 emp PUT 删除员工 emp/1 DELETE 3）、员工列表： thymeleaf公共页面元素抽取12345678910111213141、抽取公共片段&lt;div th:fragment=\"copy\"&gt;&amp;copy; 2011 The Good Thymes Virtual Grocery&lt;/div&gt;2、引入公共片段&lt;div th:insert=\"~&#123;footer :: copy&#125;\"&gt;&lt;/div&gt;~&#123;templatename::selector&#125;：模板名::选择器~&#123;templatename::fragmentname&#125;:模板名::片段名3、默认效果：insert的公共片段在div标签中如果使用th:insert等属性进行引入，可以不用写~&#123;&#125;：行内写法可以加上：[[~&#123;&#125;]];[(~&#123;&#125;)]； 三种引入公共片段的th属性： th:insert：将公共片段整个插入到声明引入的元素中 th:replace：将声明引入的元素替换为公共片段 th:include：将被引入的片段的内容包含进这个标签中 1234567891011121314151617181920212223&lt;footer th:fragment=\"copy\"&gt;&amp;copy; 2011 The Good Thymes Virtual Grocery&lt;/footer&gt;引入方式&lt;div th:insert=\"footer :: copy\"&gt;&lt;/div&gt;&lt;div th:replace=\"footer :: copy\"&gt;&lt;/div&gt;&lt;div th:include=\"footer :: copy\"&gt;&lt;/div&gt;效果&lt;div&gt; &lt;footer&gt; &amp;copy; 2011 The Good Thymes Virtual Grocery &lt;/footer&gt;&lt;/div&gt;&lt;footer&gt;&amp;copy; 2011 The Good Thymes Virtual Grocery&lt;/footer&gt;&lt;div&gt;&amp;copy; 2011 The Good Thymes Virtual Grocery&lt;/div&gt; 引入片段的时候传入参数： 1234567891011121314151617&lt;nav class=\"col-md-2 d-none d-md-block bg-light sidebar\" id=\"sidebar\"&gt; &lt;div class=\"sidebar-sticky\"&gt; &lt;ul class=\"nav flex-column\"&gt; &lt;li class=\"nav-item\"&gt; &lt;a class=\"nav-link active\" th:class=\"$&#123;activeUri=='main.html'?'nav-link active':'nav-link'&#125;\" href=\"#\" th:href=\"@&#123;/main.html&#125;\"&gt; &lt;svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"feather feather-home\"&gt; &lt;path d=\"M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z\"&gt;&lt;/path&gt; &lt;polyline points=\"9 22 9 12 15 12 15 22\"&gt;&lt;/polyline&gt; &lt;/svg&gt; Dashboard &lt;span class=\"sr-only\"&gt;(current)&lt;/span&gt; &lt;/a&gt; &lt;/li&gt;&lt;!--引入侧边栏;传入参数--&gt;&lt;div th:replace=\"commons/bar::#sidebar(activeUri='emps')\"&gt;&lt;/div&gt; 6）、CRUD-员工添加","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}]},{"title":"14_国际化","slug":"14_Spring Boot的国际化","date":"2018-10-10T12:19:32.000Z","updated":"2018-11-26T01:34:15.249Z","comments":true,"path":"2018/10/10/14_Spring Boot的国际化/","link":"","permalink":"http://yoursite.com/2018/10/10/14_Spring Boot的国际化/","excerpt":"","text":"Spring Boot的国际化1）编写国际化配置文件 2）使用ResourceBundleMessageSource 3）在页面使用fmt:message取出国际化内容 步骤： 1、编写国际化配置文件，抽取页面需要显示的国际化消息 2、Spring Boot 自动配置好了管理国际化资源文件的组件； 12345678910111213141516171819202122232425262728293031323334353637 @Bean @ConfigurationProperties(prefix = \"spring.messages\") public MessageSourceProperties messageSourceProperties() &#123; return new MessageSourceProperties(); &#125; /** * Comma-separated list of basenames (essentially a fully-qualified classpath * location), each following the ResourceBundle convention with relaxed support for * slash based locations. If it doesn't contain a package qualifier (such as * \"org.mypackage\"), it will be resolved from the classpath root. */ private String basename = \"messages\";//我们的配置文件可以直接放在类路径下叫messages.properties； @Bean public MessageSource messageSource() &#123; MessageSourceProperties properties = messageSourceProperties(); ResourceBundleMessageSource messageSource = new ResourceBundleMessageSource(); if (StringUtils.hasText(properties.getBasename())) &#123; //设置国际化资源文件的基础名（去掉语言国家代码的） messageSource.setBasenames(StringUtils.commaDelimitedListToStringArray( StringUtils.trimAllWhitespace(properties.getBasename()))); &#125; if (properties.getEncoding() != null) &#123; messageSource.setDefaultEncoding(properties.getEncoding().name()); &#125; messageSource.setFallbackToSystemLocale(properties.isFallbackToSystemLocale()); Duration cacheDuration = properties.getCacheDuration(); if (cacheDuration != null) &#123; messageSource.setCacheMillis(cacheDuration.toMillis()); &#125; messageSource.setAlwaysUseMessageFormat(properties.isAlwaysUseMessageFormat()); messageSource.setUseCodeAsDefaultMessage(properties.isUseCodeAsDefaultMessage()); return messageSource; &#125; 3、去页面获取国际化的值； 效果：根据浏览器语言设置的信息切换了国际化； 页面代码 123456789101112131415161718192021222324252627282930313233343536&lt;!DOCTYPE html&gt;&lt;html lang=\"en\" xmlns:th=\"http://www.thymeleaf.org\"&gt; &lt;head&gt; &lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"&gt; &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\"&gt; &lt;meta name=\"description\" content=\"\"&gt; &lt;meta name=\"author\" content=\"\"&gt; &lt;title&gt;Signin Template for Bootstrap&lt;/title&gt; &lt;!-- Bootstrap core CSS --&gt; &lt;link href=\"asserts/css/bootstrap.min.css\" th:href=\"@&#123;/webjars/bootstrap/4.1.3/css/bootstrap.css&#125;\" rel=\"stylesheet\"&gt; &lt;!-- Custom styles for this template --&gt; &lt;link href=\"asserts/css/signin.css\" th:href=\"@&#123;/asserts/css/signin.css&#125;\" rel=\"stylesheet\"&gt; &lt;/head&gt; &lt;body class=\"text-center\"&gt; &lt;form class=\"form-signin\" action=\"dashboard.html\"&gt; &lt;img class=\"mb-4\" th:src=\"@&#123;asserts/img/bootstrap-solid.svg&#125;\" src=\"asserts/img/bootstrap-solid.svg\" alt=\"\" width=\"72\" height=\"72\"&gt; &lt;h1 class=\"h3 mb-3 font-weight-normal\" th:text=\"#&#123;login.tip&#125;\"&gt;Please sign in&lt;/h1&gt; &lt;label class=\"sr-only\"&gt;Username&lt;/label&gt; &lt;input type=\"text\" class=\"form-control\" placeholder=\"Username\" th:placeholder=\"#&#123;login.username&#125;\" required=\"\" autofocus=\"\"&gt; &lt;label class=\"sr-only\"&gt;Password&lt;/label&gt; &lt;input type=\"password\" class=\"form-control\" placeholder=\"Password\" th:placeholder=\"#&#123;login.passwd&#125;\" required=\"\"&gt; &lt;div class=\"checkbox mb-3\"&gt; &lt;label&gt; &lt;input type=\"checkbox\" value=\"remember-me\"&gt; [[#&#123;login.rememberMe&#125;]] &lt;/label&gt; &lt;/div&gt; &lt;button class=\"btn btn-lg btn-primary btn-block\" type=\"submit\" th:text=\"#&#123;login.btn&#125;\"&gt;Sign in&lt;/button&gt; &lt;p class=\"mt-5 mb-3 text-muted\"&gt;? 2017-2018&lt;/p&gt; &lt;a class=\"btn btn-sm\" th:href=\"@&#123;/(l='zh_CN')&#125;\"&gt;中文&lt;/a&gt; &lt;a class=\"btn btn-sm\" th:href=\"@&#123;/(l=en_US)&#125;\"&gt;English&lt;/a&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt; 国际化Locale（区域信息对象）；LocaleResolver（获取区域信息对象）； 12345678910111213 @Bean @ConditionalOnMissingBean @ConditionalOnProperty(prefix = \"spring.mvc\", name = \"locale\") public LocaleResolver localeResolver() &#123; if (this.mvcProperties .getLocaleResolver() == WebMvcProperties.LocaleResolver.FIXED) &#123; return new FixedLocaleResolver(this.mvcProperties.getLocale()); &#125; AcceptHeaderLocaleResolver localeResolver = new AcceptHeaderLocaleResolver(); localeResolver.setDefaultLocale(this.mvcProperties.getLocale()); return localeResolver; &#125;默认根据浏览器语言进行返回 4）、点击链接切换国际化 12345678910111213141516171819202122232425262728293031import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.util.Locale;import org.slf4j.Logger;import org.slf4j.LoggerFactory;/** * * 可以在请求头里面带上区域信息 * */public class MyLocaleResolver implements LocaleResolver&#123; Logger logger = LoggerFactory.getLogger(getClass()); @Override public Locale resolveLocale(HttpServletRequest httpServletRequest) &#123; String l = httpServletRequest.getParameter(\"l\"); Locale locale = Locale.getDefault(); if(!StringUtils.isEmpty(l))&#123; String[] split = l.split(\"_\"); logger.info(l); locale = new Locale(split[0],split[1]); &#125; return locale; &#125; @Override public void setLocale(HttpServletRequest httpServletRequest, @Nullable HttpServletResponse httpServletResponse, @Nullable Locale locale) &#123; &#125;&#125; 在配置类中将组件放入容器中 1234@Beanpublic LocaleResolver localeResolver()&#123; return new MyLocaleResolver();&#125;","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}]},{"title":"13_Spring MVC 自动配置","slug":"13_Spring MVC 自动配置","date":"2018-10-08T12:19:32.000Z","updated":"2018-11-26T01:34:08.709Z","comments":true,"path":"2018/10/08/13_Spring MVC 自动配置/","link":"","permalink":"http://yoursite.com/2018/10/08/13_Spring MVC 自动配置/","excerpt":"","text":"Spring Boot的Spring MVC 自动配置模板引擎27.1.1 Spring MVC Auto-configurationSpring Boot 自动配置好了 SpringMVC 下面就是SpringBoot 对SpringMVC的默认配置： 原文参考链接 Spring Boot provides auto-configuration for Spring MVC that works well with most applications. The auto-configuration adds the following features on top of Spring’s defaults: Inclusion of ContentNegotiatingViewResolver and BeanNameViewResolver beans. 自动配置了ViewResolver(视图解析器：根据方法的返回值得到的视图对象（view）,视图对象决定如何渲染（转发页面？重定向？）) ContentNegotiatingViewResolver：组合所有的视图解析器的； ==如何定制：我们可以自己给容器中添加一个视图解析器；自动的将其组合进来；== Support for serving static resources, including support for WebJars (covered later in this document)). Automatic registration of Converter, GenericConverter, and Formatter beans. Converter自动注册转化器 文本 到数字 Formatter 格式转化器：2018-12-01 转化成 date 1234567Bean @ConditionalOnProperty(prefix = \"spring.mvc\", name = \"date‐format\")//在文件中配置日期格式化的规则 public Formatter&lt;Date&gt; dateFormatter() &#123; return new DateFormatter(this.mvcProperties.getDateFormat());//日期格式化组件 &#125; ==自己添加的格式化器转换器，我们只需要放在容器中即可== Support for HttpMessageConverters (covered later in this document). HttpMessageConverter：SpringMVC用来转换Http请求和响应的；User—-Json HttpMessageConverters 是从容器中确定；获取所有的HttpMessageConverter； ==自己给容器中添加HttpMessageConverter，只需要将自己的组件注册容器中（@Bean,@Component）== 123456789101112131415import org.springframework.boot.autoconfigure.web.HttpMessageConverters;import org.springframework.context.annotation.*;import org.springframework.http.converter.*;@Configurationpublic class MyConfiguration &#123; @Bean public HttpMessageConverters customConverters() &#123; HttpMessageConverter&lt;?&gt; additional = ... HttpMessageConverter&lt;?&gt; another = ... return new HttpMessageConverters(additional, another); &#125;&#125; Automatic registration of MessageCodesResolver (covered later in this document).定义错误代码生成规则 ==我们可以配置一个ConfigurableWebBindingInitializer来替换默认的；（添加到容器）== 12初始化WebDataBinder；请求数据=====JavaBean； Static index.html support. Custom Favicon support (covered later in this document). Automatic use of a ConfigurableWebBindingInitializer bean (covered later in this document). org.springframework.boot.autoconfigure.web：自动配置 If you want to keep Spring Boot MVC features and you want to add additional MVC configuration (interceptors, formatters, view controllers, and other features), you can add your own @Configuration class of type WebMvcConfigurer but without @EnableWebMvc. If you wish to provide custom instances of RequestMappingHandlerMapping, RequestMappingHandlerAdapter, or ExceptionHandlerExceptionResolver, you can declare a WebMvcRegistrationsAdapter instance to provide such components. If you want to take complete control of Spring MVC, you can add your own @Configuration annotated with @EnableWebMvc. 2、扩展SpringMVC12345678&lt;mvc:view-controller path=\"/hello\" view-name=\"success\" /&gt;&lt;mvc:interceptors&gt; &lt;mvc:interceptor&gt; &lt;mvc:mapping path=\"/hello\"/&gt; &lt;bean&gt;&lt;/bean&gt; &lt;/mvc:interceptor&gt;&lt;/mvc:interceptors&gt; ==编写一个配置类（@Configuration ），是一个WebMvcConfigurer 类型，但是不能标注 @EnableWebMvc== 既保留了Springboot的自动配置，也能用扩展的配置 12345678910// 使用WebMvcConfigurer来扩展SpringMvc的功能@Configurationpublic class MyMvcConfig implements WebMvcConfigurer&#123; @Override public void addViewControllers(ViewControllerRegistry registry) &#123; //浏览器发送 /atguigu 请求发送到 success registry.addViewController(\"/atguigu\").setViewName(\"success\"); &#125;&#125; 原理： 1）、WebMvcAutoConfiguration是SpringMVC的自动配置类 2）、在做其他自动配置时导入 @Import(EnableWebMvcConfiguration.class) 1234567891011121314151617@Configurationpublic static class EnableWebMvcConfiguration extends DelegatingWebMvcConfiguration &#123; //从容器中获取所有的WebMvcConfigurer @Autowired(required = false) public void setConfigurers(List&lt;WebMvcConfigurer&gt; configurers) &#123; if (!CollectionUtils.isEmpty(configurers)) &#123; this.configurers.addWebMvcConfigurers(configurers); //一个参考实现；将所有的WebMvcConfigurer相关配置都来一起调用； @Override // public void addViewControllers(ViewControllerRegistry registry) &#123; // for (WebMvcConfigurer delegate : this.delegates) &#123; // delegate.addViewControllers(registry); // &#125; &#125; &#125;&#125; 3）、容器中所有的WebMvcConfigurer都会一起起作用； 4）、我们的配置类也会被调用； 效果：SpringMVC的自动配置和我们的扩展配置都会起作用； 3、全面接管SpringMVC；SpringBoot对SpringMVC的自动配置不需要了，所有都是我们自己配置；所有的SpringMVC的自动配置都失效了 我们需要在配置类中添加@EnableWebMvc即可； 1234567891011//使用WebMvcConfigurerAdapter可以来扩展SpringMVC的功能@EnableWebMvc@Configurationpublic class MyMvcConfig extends WebMvcConfigurerAdapter &#123; @Override public void addViewControllers(ViewControllerRegistry registry) &#123; // super.addViewControllers(registry); //浏览器发送 /atguigu 请求来到 success registry.addViewController(\"/atguigu\").setViewName(\"success\"); &#125;&#125; 原理：为什么@EnableWebMvc自动配置就失效了； 1）@EnableWebMvc的核心 12@Import(DelegatingWebMvcConfiguration.class)public @interface EnableWebMvc &#123; 2）、 12@Configurationpublic class DelegatingWebMvcConfiguration extends WebMvcConfigurationSupport &#123; 3）、 12345678910@Configuration@ConditionalOnWebApplication@ConditionalOnClass(&#123; Servlet.class, DispatcherServlet.class,WebMvcConfigurerAdapter.class &#125;) //容器中没有这个组件的时候，这个自动配置类才生效@ConditionalOnMissingBean(WebMvcConfigurationSupport.class)@AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE + 10)@AutoConfigureAfter(&#123; DispatcherServletAutoConfiguration.class,ValidationAutoConfiguration.class &#125;) public class WebMvcAutoConfiguration &#123; 4）、@EnableWebMvc将WebMvcConfigurationSupport组件导入进来； 5）、导入的WebMvcConfigurationSupport只是SpringMVC最基本的功能； 5、如何修改SpringBoot的默认配置模式： 1）、SpringBoot在自动配置很多组件的时候，先看容器中有没有用户自己配置的（@Bean、@Component）如果有就用用户配置的，如果没有，才自动配置；如果有些组件可以有多个（ViewResolver）将用户配置的和自己默认的组合起来； 2）、在SpringBoot中会有非常多的xxxConfigurer帮助我们进行扩展配置 3）、在SpringBoot中会有很多的xxxCustomizer帮助我们进行定制配置","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}]},{"title":"12_web开发_静态资源文件夹","slug":"12_web开发_静态资源文件夹","date":"2018-09-30T12:19:32.000Z","updated":"2018-11-26T01:34:04.373Z","comments":true,"path":"2018/09/30/12_web开发_静态资源文件夹/","link":"","permalink":"http://yoursite.com/2018/09/30/12_web开发_静态资源文件夹/","excerpt":"","text":"web开发模板引擎JSP ; Velocity ; Freemarker ; Thymeleaf SpringBoot 推荐的是Thymeleaf语法简单，功能强大 1.引入thymeleaf12345&lt;!--引入spring-boot-starter-thymeleaf包--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt; 2.Thymeleaf 使用和语法1234567891011121314@ConfigurationProperties(prefix = \"spring.thymeleaf\")public class ThymeleafProperties &#123; private static final Charset DEFAULT_ENCODING = StandardCharsets.UTF_8; // 只要把html模板页面放在classpath:/templates/ 就可以自动渲染了 public static final String DEFAULT_PREFIX = \"classpath:/templates/\"; public static final String DEFAULT_SUFFIX = \".html\"; /** * Whether to check that the template exists before rendering it. */ private boolean checkTemplate = true; // 只要把html模板页面放在classpath:/templates/ 就可以自动渲染了 使用：先导入thymeleaf 的名空间 1&lt;html lang=\"en\" xmlns:th=\"http://www.thymeleaf.org\"&gt; 例子： 123456789101112&lt;!DOCTYPE html&gt;&lt;html lang=\"en\" xmlns:th=\"http://www.thymeleaf.org\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;!--th:text 将后面的文本替换成我们的传递的值--&gt;&lt;body &gt;这是成功&lt;div th:text=\"$&#123;hello&#125;\"&gt;你这个猪头&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 3.语法规则1）、th:text；改变当前元素里面的文本内容；th：任意html属性；来替换原生属性的值 2）、表达式？ 知道什么时候用哪种表达式就好了（就5种） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768Simple expressions:（表达式语法） Variable Expressions: $&#123;...&#125;：获取变量值；OGNL； 1）、获取对象的属性、调用方法 2）、使用内置的基本对象： #ctx : the context object. #vars: the context variables. #vars: the context variables. #locale : the context locale. #request : (only in Web Contexts) the HttpServletRequest object. #response : (only in Web Contexts) the HttpServletResponse object. #session : (only in Web Contexts) the HttpSession object. #servletContext : (only in Web Contexts) the ServletContext object. $&#123;session.foo&#125; 3）、内置的一些工具对象：#execInfo : information about the template being processed. #messages : methods for obtaining externalized messages inside variables expressions, in the same way as they would be obtained using #&#123;…&#125; syntax.#uris : methods for escaping parts of URLs/URIs#conversions : methods for executing the configured conversion service (if any). #dates : methods for java.util.Date objects: formatting, component extraction, etc.#calendars : analogous to #dates , but for java.util.Calendar objects.#numbers : methods for formatting numeric objects. #strings : methods for String objects: contains, startsWith, prepending/appending, etc.#objects : methods for objects in general.#bools : methods for boolean evaluation.#arrays : methods for arrays.#lists : methods for lists.#sets : methods for sets.#maps : methods for maps.#aggregates : methods for creating aggregates on arrays or collections.#ids : methods for dealing with id attributes that might be repeated (for example, as a result of an iteration). Selection Variable Expressions: *&#123;...&#125;：选择表达式：和$&#123;&#125;在功能上是一样； 补充：配合 th:object=&quot;$&#123;session.user&#125;： ==例子================================================================== &lt;div th:object=&quot;$&#123;session.user&#125;&quot;&gt; &lt;p&gt;Name: &lt;span th:text=&quot;*&#123;firstName&#125;&quot;&gt;Sebastian&lt;/span&gt;.&lt;/p&gt; &lt;p&gt;Surname: &lt;span th:text=&quot;*&#123;lastName&#125;&quot;&gt;Pepper&lt;/span&gt;.&lt;/p&gt; &lt;p&gt;Nationality: &lt;span th:text=&quot;*&#123;nationality&#125;&quot;&gt;Saturn&lt;/span&gt;.&lt;/p&gt; &lt;/div&gt; ========================================================================== Message Expressions: #&#123;...&#125;：获取国际化内容 Link URL Expressions: @&#123;...&#125;：定义URL； ==例子================================================ @&#123;/order/process(execId=$&#123;execId&#125;,execType=&apos;FAST&apos;)&#125; ================================================== Fragment Expressions: ~&#123;...&#125;：片段引用表达式 &lt;div th:insert=&quot;~&#123;commons :: main&#125;&quot;&gt;...&lt;/div&gt; Literals（字面量） Text literals: &apos;one text&apos; , &apos;Another one!&apos; ,… Number literals: 0 , 34 , 3.0 , 12.3 ,… Boolean literals: true , false Null literal: null Literal tokens: one , sometext , main ,…Text operations:（文本操作） String concatenation: + Literal substitutions: |The name is $&#123;name&#125;|Arithmetic operations:（数学运算） Binary operators: + , ‐ , * , / , % Minus sign (unary operator): ‐Boolean operations:（布尔运算） Binary operators: and , or Boolean negation (unary operator): ! , notComparisons and equality:（比较运算） Comparators: &gt; , &lt; , &gt;= , &lt;= ( gt , lt , ge , le ) Equality operators: == , != ( eq , ne )Conditional operators:条件运算（三元运算符） If‐then: (if) (then) If‐then‐else: (if) (then) : (else) Default: (value) : (defaultvalue)Special tokens: No‐Operation: _ 123456789101112131415161718192021222324252627282930&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot; xmlns:th=&quot;http://www.thymeleaf.org&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;!--th:text 将后面的文本替换成我们的传递的值--&gt;&lt;body &gt;这是标题成功&lt;div&gt;&lt;h3&gt;dasd&lt;/h3&gt;&lt;/div&gt;&lt;div th:id=&quot;$&#123;hello&#125;&quot; th:class=&quot;$&#123;hello&#125;&quot; &gt;&lt;/div&gt;&lt;div th:text=&quot;$&#123;hello&#125;&quot;&gt;你这个猪头&lt;/div&gt;&lt;div th:utext=&quot;$&#123;hello&#125;&quot;&gt;你这个猪头&lt;/div&gt;&lt;!--th:each 在哪个标签里，每次遍历都会生成同样的标签，3个div--&gt;&lt;div th:text=&quot;$&#123;fuck&#125;&quot; th:each=&quot;fuck:$&#123;fucks&#125;&quot;&gt;遍历内容&lt;/div&gt;&lt;hr/&gt;&lt;!--一个div中 有三个span--&gt;&lt;div&gt; &lt;span th:each=&quot;fuck:$&#123;fucks&#125;&quot; &gt;[[$&#123;fuck&#125;]]&lt;/span&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}]},{"title":"11_web开发_静态资源文件夹","slug":"11_web开发_静态资源文件夹","date":"2018-09-30T12:19:32.000Z","updated":"2018-11-26T01:33:01.768Z","comments":true,"path":"2018/09/30/11_web开发_静态资源文件夹/","link":"","permalink":"http://yoursite.com/2018/09/30/11_web开发_静态资源文件夹/","excerpt":"","text":"web开发使用SpringBoot1)、创建SpringBoot应用，选中我们需要的模块 2）、SpringBoot已经默认将这些配置好了，只需要在配置文件中指定少量配置就可以运行起来 3）、直接编写业务代码 自动配置原理？这个场景SpringBoot棒我们配置了什么？能不能修改？能修改哪些配置？能不能扩展？ 12XXXXAutoConfiguration:帮我们给容器中自动配置组件xxxxProperties:配置类来封装配置文件的内容 静态资源的映射规则//可以通过ResourceProperties 设置和静态资源有关参数 123456@ConfigurationProperties(prefix = \"spring.resources\", ignoreUnknownFields = false)public class ResourceProperties &#123;//可以通过ResourceProperties 设置和静态资源有关参数 private static final String[] CLASSPATH_RESOURCE_LOCATIONS = &#123; \"classpath:/META-INF/resources/\", \"classpath:/resources/\", \"classpath:/static/\", \"classpath:/public/\" &#125;; 123456789101112131415161718192021222324252627282930313233343536 @Override public void addResourceHandlers(ResourceHandlerRegistry registry) &#123; if (!this.resourceProperties.isAddMappings()) &#123; logger.debug(\"Default resource handling disabled\"); return; &#125; Duration cachePeriod = this.resourceProperties.getCache().getPeriod(); CacheControl cacheControl = this.resourceProperties.getCache() .getCachecontrol().toHttpCacheControl(); if (!registry.hasMappingForPattern(\"/webjars/**\")) &#123; customizeResourceHandlerRegistration(registry .addResourceHandler(\"/webjars/**\") .addResourceLocations(\"classpath:/META-INF/resources/webjars/\") .setCachePeriod(getSeconds(cachePeriod)) .setCacheControl(cacheControl)); &#125; String staticPathPattern = this.mvcProperties.getStaticPathPattern(); if (!registry.hasMappingForPattern(staticPathPattern)) &#123; customizeResourceHandlerRegistration( registry.addResourceHandler(staticPathPattern) .addResourceLocations(getResourceLocations( this.resourceProperties.getStaticLocations())) .setCachePeriod(getSeconds(cachePeriod)) .setCacheControl(cacheControl)); &#125; &#125;// 配置欢迎页的映射 @Bean public WelcomePageHandlerMapping welcomePageHandlerMapping( ApplicationContext applicationContext) &#123; return new WelcomePageHandlerMapping( new TemplateAvailabilityProviders(applicationContext), applicationContext, getWelcomePage(), this.mvcProperties.getStaticPathPattern()); &#125; 1)、==所有 /webjars/**,都去classpath:/META-INF/resources/webjars/ 找资源；== webjars:以jar包的方式引入静态资源； https://www.webjars.org/ 访问路径 http://localhost/webjars/jquery/3.3.1-1/jquery.js 123456&lt;!--引入jquery的web jar--&gt;&lt;dependency&gt; &lt;groupId&gt;org.webjars&lt;/groupId&gt; &lt;artifactId&gt;jquery&lt;/artifactId&gt; &lt;version&gt;3.3.1-1&lt;/version&gt;&lt;/dependency&gt; ==2）、“/**” 访问项目的任何资源，（静态资源文件夹）== 12345&quot;classpath:/META-INF/resources/&quot;,&quot;classpath:/resources/&quot;,&quot;classpath:/static/&quot;, &quot;classpath:/public/&quot; &quot;/&quot; 当前项目根路径 localhost/abc === 访问”classpath:/resources/“, “classpath:/static/“, 等于访问上面的（静态资源文件夹）下的资源abc ==3）欢迎页：静态资源文件夹下的所有的html 页面；被“/**” 映射；== localhost/ 找index.html ==4)配置喜欢的图标== 所有的**/favicon.ico 都是在静态资源文件夹下找 5）修改静态文件夹的路径，一旦修改默认的就不起作用了 1spring.resources.static-locations=classpath:/resources/,classpath:/static/,classpath:/public/","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}]},{"title":"10_日志框架_slf4j+logback","slug":"10_日志框架_slf4j+logback","date":"2018-09-29T12:19:32.000Z","updated":"2018-11-26T01:33:05.188Z","comments":true,"path":"2018/09/29/10_日志框架_slf4j+logback/","link":"","permalink":"http://yoursite.com/2018/09/29/10_日志框架_slf4j+logback/","excerpt":"","text":"日志1.日志框架1.System.out.println(“”);将关键数据打印到控制台；全部去掉？全部写在一个文件中？ 2.框架来记录系统的一些运行信息；日志框架； 3.高大上的功能：异步模式，自动归档； 4.将以前框架卸载，换上新框架，改API, 5.JDBC—数据库驱动； 写了一个统一的接口层；日志门面（日志的一个抽象层） 给项目中导入具体的日志实现就行了；我们之前的日志框架都是实现的抽象层； 常见的日志框架 JUL,JCL,JBOSS-logging,logback,log4j,log2j 日志门面（抽象层） 日志实现 JCL(jakarta Commons Logging) 好久不更新2014 jul(java.util.logging) ==SLF4j(Simple Logging Facade for java)== Log4j ==Logback== Log4j2 jboss-logging 不易用 SpringBoot: ==SLF4j==?==Logback== SpringBoot底层是Sping 框架，Spring框架默认是JCL； 2.SLF4j使用1.如何在系统中使用SLF4j以后开发的时候，日志记录方法的调用，不应该直接调用日志的实现类，而是调用日志的抽象层 给系统导入SLF4j的jar 和 logback的实现jar 123456789import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class HelloWorld &#123; public static void main(String[] args) &#123; Logger logger = LoggerFactory.getLogger(HelloWorld.class); logger.info(\"Hello World\"); &#125;&#125; 每一个日志的实现都有自己配置文件，使用slf4j,==配置文件还是做成日志实现框架的配置文件；== 2.遗留问题a(slf4j+logback):spring (commons-logging) hibernate(jboss-logging) mybatis 统一日志记录，即:别的框架和我一起统一使用slf4j ? ==就是换jar包== 3.springboot 日志记录springboot 启动器123456&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.5.RELEASE&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt; springboot 日志功能123456&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;version&gt;2.0.5.RELEASE&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt; 总结：1）、SpringBoot底层也是使用slf4j+logback的方式进行日志记录 2）、SpringBoot也把其他的日志都替换成了slf4j； 3）、中间替换包？ 12345@SuppressWarnings(\"rawtypes\")public?abstract?class?LogFactory?&#123;????static?String?UNSUPPORTED_OPERATION_IN_JCL_OVER_SLF4J?=\"http://www.slf4j.org/codes.html#unsupported_operation_in_jcl_over_slf4j\";????static?LogFactory?logFactory?=?new?SLF4JLogFactory(); 4）、如果我们要引入其他框架？一定要把这个框架的默认日志依赖移除掉？Spring框架用的是commons-logging； 123456&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;5.0.9.RELEASE&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt; ==SpringBoot能自动适配所有的日志，而且底层使用slf4j+logback的方式记录日志，引入其他框架的时候，只需要====把这个框架依赖的日志框架排除掉即可；== 4.springBoot 使用1.默认配置Springboot 默认配置好了日志，直接使用就好了 1234567891011121314151617/记录器 ???Logger?logger?=?LoggerFactory.getLogger(getClass()); ???@Test ???public?void?contextLoads()?&#123; ???//System.out.println(); ???????//日志的级别； ???????//由低到高???trace&lt;debug&lt;info&lt;warn&lt;error ???????//可以调整输出的日志级别；日志就只会在这个级别以以后的高级别生效 ???????logger.trace(\"这是trace日志...\"); ???????logger.debug(\"这是debug日志...\"); ???????//SpringBoot默认给我们使用的是info级别的，没有指定级别的就用SpringBoot默认规定的级别；root级别???????logger.info(\"这是info日志...\"); ???????logger.warn(\"这是warn日志...\"); ???????logger.error(\"这是error日志...\"); ???????&#125; 123456789????日志输出格式：%d表示日期时间， ???????%thread表示线程名， ???????%‐5level：级别从左显示5个字符宽度 ???????%logger&#123;50&#125;?表示logger名字最长50个字符，否则按照句点分割。? ???????%msg：日志消息， ???????%n是换行符 ???????????‐‐&gt;????%d&#123;yyyy‐MM‐dd?HH:mm:ss.SSS&#125;?[%thread]?%‐5level?%logger&#123;50&#125;?‐?%msg%n SpringBoot修改日志的默认配置 1234567891011logging.level.com.atguigu=trace#logging.path=#?不指定路径在当前项目下生成springboot.log日志#?可以指定完整的路径；#logging.file=G:/springboot.log#?在当前磁盘的根路径下创建spring文件夹和里面的log文件夹；使用?spring.log?作为默认文件logging.path=/spring/log#??在控制台输出的日志的格式logging.pattern.console=%d&#123;yyyy‐MM‐dd&#125;?[%thread]?%‐5level?%logger&#123;50&#125;?‐?%msg%n#?指定文件中日志输出的格式logging.pattern.file=%d&#123;yyyy‐MM‐dd&#125;?===?[%thread]?===?%‐5level?===?%logger&#123;50&#125;?====?%msg%n logging.file logging.path Example Description (none) (none) 只在控制台输出 指定文件名 (none) my.log 输出日志到my.log文件 (none) 指定目录 /var/log 输出到指定目录的 spring.log 文件中 ==以下部分没有看== 2.指定配置给类路径下放上每个日志框架自己的配置文件即可；SpringBoot就不使用他默认配置的了 Logging System Customization Logback logback-spring.xml , logback-spring.groovy , logback.xml or logback.groovy Log4j2 log4j2-spring.xml or log4j2.xml JDK (Java Util Logging) logging.properties logback.xml：直接就被日志框架识别了； logback-spring.xml：日志框架就不直接加载日志的配置项，由SpringBoot解析日志配置，可以使用SpringBoot的高级Profile功能","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}]},{"title":"08_Profile 开发测试环境切换","slug":"08_Profile 开发测试环境切换","date":"2018-09-27T12:19:32.000Z","updated":"2018-11-26T01:33:19.773Z","comments":true,"path":"2018/09/27/08_Profile 开发测试环境切换/","link":"","permalink":"http://yoursite.com/2018/09/27/08_Profile 开发测试环境切换/","excerpt":"","text":"Profile1.多Profile 文件命名格式是application-{profile}.properties 或 application-{profile}.yml 默认使用application.properties 的配置 2.yml 支持多文档块方式1234567891011121314151617server: port: 80spring: profiles: active: dev---server: port: 8081spring: profiles: dev---server: port: 8082spring: profiles: pro 3.激活指定profile1.在主配置文件中指定 spring.profiles.active = dev 1234// 在主配置文件中指定 spring.profiles.active = devspring: profiles: active: dev 2.使用命令行， 1--spring.profiles.active=pro 可以用于生产jar包指定参数 1java -jar xx.jar --spring.profiles.active=pro 3.虚拟机参数 1-Dspring.profiles.active=pro","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}]},{"title":"09_配置文件加载顺序以及外部配置文件","slug":"09_配置文件加载顺序以及外部配置文件","date":"2018-09-27T12:19:32.000Z","updated":"2018-11-26T01:33:09.759Z","comments":true,"path":"2018/09/27/09_配置文件加载顺序以及外部配置文件/","link":"","permalink":"http://yoursite.com/2018/09/27/09_配置文件加载顺序以及外部配置文件/","excerpt":"","text":"配置文件加载位置springboot 启动会扫描以下位置的application.properties或者application.yml文件作为Spring boot的默认配置文件 1234567–file:./config/–file:./–classpath:/config/–classpath:/ 优先级由高到底，高优先级的配置会覆盖低优先级的配置； 由高到低全部加载，互补配置 –classpath:/ 默认配置路径 123server.port=8081#配置项目访问路径server.servlet.context-path=/root2 –file:./ 1server.port=8082 ==可以通过spring.config.location 来改变默认的配置文件位置,运维的时候紧急改变== 项目打包好以后，我们可以使用命令行参数形式，启动项目时，指定配置文件，形成互补配置 1java -jar xx.jar --spring.config.location=F:/application.properties 外部配置文件官方文档参考 SpringBoot也可以从以下位置加载配置； 优先级从高到低；高优先级的配置覆盖低优先级的配置，所有的配置会形成互补配置 1.命令行参数 所有的配置都可以在命令行上进行指定java -jar spring-boot-02-config-02-0.0.1-SNAPSHOT.jar —server.port=8087 —server.context-path=/abc 多个配置用空格分开； —配置项=值 2.来自java:comp/env的JNDI属性 3.Java系统属性（System.getProperties()） 4.操作系统环境变量 5.RandomValuePropertySource配置的random.*属性值 ==由jar包外向jar包内进行寻找；优先加载带profile== 和jar包同级目录下自动寻找 ==6.jar包外部的application-{profile}.properties或application.yml(带spring.profile)配置文件7.jar包内部的application-{profile}.properties或application.yml(带spring.profile)配置文件再来加载不带profile== ==8.jar包外部的application.properties或application.yml(不带spring.profile)配置文件== ==9.jar包内部的application.properties或application.yml(不带spring.profile)配置文件== 10.@Configuration注解类上的@PropertySource 11.通过SpringApplication.setDefaultProperties指定的默认属性所有支持的配置加载来源； 自动配置原理（还是不懂，看第19个视频“19、尚硅谷SpringBoot配置-自动配置原理.avi”）配置文件能配置的属性参照 自动配置原理： 1）、springboot 启动的时候加载主配置类，开启了自动配置功能==@EnableAutoConfiguration==","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}]},{"title":"03_Spring Boot的hello world","slug":" 03_Spring Boot的hello world","date":"2018-09-25T12:19:32.000Z","updated":"2018-11-26T01:33:45.391Z","comments":true,"path":"2018/09/25/ 03_Spring Boot的hello world/","link":"","permalink":"http://yoursite.com/2018/09/25/ 03_Spring Boot的hello world/","excerpt":"","text":"Spring Boot功能浏览器发送hello world 请求，服务器响应请求并处理，响应hello world字符串 传统做法：创建web 项目，导入Spring ，Spring mvc 的jar，然后编写配置文件，然后打成war, 放入tomcat 运行 1. 创建一个maven 工程2.导入spring boot 依赖1234567891011&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.16.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 3.编写主程序12345678910111213package com.groupid;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class HelloWorldMainApp &#123; public static void main(String[] args) &#123; //运行这个方法就可以启动应用了 SpringApplication.run(HelloWorldMainApp.class,args); &#125;&#125; 4.编写控制类12345678910111213141516package com.groupid.controller;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.ResponseBody;@Controllerpublic class HelloController &#123; @ResponseBody @RequestMapping(\"/hello\") public String hello()&#123; return \"hello world!\"; &#125;&#125; 5.运行主程序进行测试1http://localhost:8080/hello 6.简化部署123456789?&lt;!‐‐?这个插件，可以将应用打包成一个可执行的jar包；‐‐&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 通过java -jar 执行响应的jar包就可以执行起来了","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}]},{"title":"06_配置文件","slug":" 06_yaml配置文件","date":"2018-09-25T12:19:32.000Z","updated":"2018-11-26T01:33:28.786Z","comments":true,"path":"2018/09/25/ 06_yaml配置文件/","link":"","permalink":"http://yoursite.com/2018/09/25/ 06_yaml配置文件/","excerpt":"","text":"1、配置文件Spring Boot使用一个全局的配置文件，配置文件名是固定的 application.properties application.yml 配置文件的作用：修改SpringBoot 自动配置的默认值；SpringBoot 在底层都会给自动配置好 YAML (YAML Ain’t Markup Language ) YAML A Markup Language 是一个标记语言 YAML isn’t Markup Language 不是一个标记语言 标记语言：以前配置都是xxx.xml文件； 1YAML 以数据为中心 例子 12server: port: 8081 xml 123&lt;server&gt; &lt;port&gt;8081&lt;/port&gt;&lt;/server&gt; 2、YAML 语法：1. 基本语法k:(空格)v ==键值对表示，空格一定要有,大小写貌似不敏感，因为 lastname = last-name = lastName== 以空格的缩进表示层级关系；只要是左对齐的一列数据，都是同一个层级的, 123server: port: 8082 path: /hello 2. 值的写法字面量：普通的值（数字，字符串，布尔）k: v：字面直接来写； 字符串默认不用加上单引号或者双引号； 1&quot;&quot;：双引号；不会转义字符串里面的特殊字符；特殊字符会作为本身想表示的意思 name: “zhangsan \\n lisi”：输出；zhangsan 换行 lisi ‘’：单引号；会转义特殊字符，特殊字符最终只是一个普通的字符串数据 name: ‘zhangsan \\n lisi’：输出；zhangsan \\n lisi 对象、Map（属性和值）（键值对）：k: v：在下一行来写对象的属性和值的关系；注意缩进 对象还是k: v的方式 123456789101112person: lastname: zhangsan age: 18 boss: false birth: 2017/12/12 maps: &#123;k1: v1,k2: v2&#125; lists: - lisi - zhaoliu dog: name: 小狗 age: 2 行内写法： 1maps: &#123;k1: v1,k2: v2&#125; 数组（List、Set）：用- 值表示数组中的一个元素 123lists: - lisi - zhaoliu 3.配置文件值注入配置文件写法： 123456789101112person: lastname: zhangsan age: 18 boss: false birth: 2017/12/12 maps: &#123;k1: v1,k2: v2&#125; lists: - lisi - zhaoliu dog: name: 小狗 age: 2 javaBean: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899package com.groupid.springboot.bean;import org.springframework.boot.context.properties.ConfigurationProperties;import org.springframework.stereotype.Component;import java.util.Date;import java.util.List;import java.util.Map;/** 将配置文件中的配置的每一个属性的值，映射到这个组件中* @ConfigurationProperties 告诉SpringBoot 将配置文件中的属性映射到类属性中* prefix =\"person\" 和配置文件中person 下的属性进行一一映射* 只有这个组件是容器中的组件，才能使用容器提供@ConfigurationProperties 的功能* */@Component@ConfigurationProperties(prefix =\"person\")public class Person &#123; private String lastName; private Integer age; private Boolean boss; private Date birth; private Map&lt;String,Object&gt; maps; private List&lt;Object&gt; lists; private Dog dog; @Override public String toString() &#123; return \"Person&#123;\" + \"lastName='\" + lastName + '\\'' + \", age=\" + age + \", boss=\" + boss + \", birth=\" + birth + \", maps=\" + maps + \", lists=\" + lists + \", dog=\" + dog + '&#125;'; &#125;// 快捷键 ALT+INSERT public String getLastName() &#123; return lastName; &#125; public void setLastName(String lastName) &#123; this.lastName = lastName; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; public Boolean getBoss() &#123; return boss; &#125; public void setBoss(Boolean boss) &#123; this.boss = boss; &#125; public Date getBirth() &#123; return birth; &#125; public void setBirth(Date birth) &#123; this.birth = birth; &#125; public Map&lt;String, Object&gt; getMaps() &#123; return maps; &#125; public void setMaps(Map&lt;String, Object&gt; maps) &#123; this.maps = maps; &#125; public List&lt;Object&gt; getLists() &#123; return lists; &#125; public void setLists(List&lt;Object&gt; lists) &#123; this.lists = lists; &#125; public Dog getDog() &#123; return dog; &#125; public void setDog(Dog dog) &#123; this.dog = dog; &#125;&#125; 我们可以导入配置文件处理器，以后编写配置就有提示了 123456&lt;!--配置文件中的元数据提示--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 1.properties配置文件中文乱码解决方案File-&gt;setting-&gt;encoding -&gt; 传输时采用ASCII码进行传输 2.@Value 获取值和@ConfigurationProperties获取值比较 @ConfigurationProperties @Value 功能 批量注入配置文件中的属性 一个个指定 松散绑定（松散语法） 支持 lastName = last_name = last-name 不支持 SpEL 不支持 支持 @Value(“#{11*2}”) JSR303数据校验 支持 @Validated @Email 不支持 复杂类型封装 支持 不支持 配置文件yml 还是properties 如果说，我们只是在某个业务逻辑中获取配置文件中某个属性的值，那么就是@value 如果是，专门写了一个javeBean来和配置文件一一映射，那就是用@ConfigurationProperties 3.配置文件注入值数据校验12345678910111213141516171819202122232425262728293031323334import org.springframework.beans.factory.annotation.Value;import org.springframework.boot.context.properties.ConfigurationProperties;import org.springframework.stereotype.Component;import org.springframework.validation.annotation.Validated;import javax.validation.constraints.Email;import java.util.Date;import java.util.List;import java.util.Map;/** 将配置文件中的配置的每一个属性的值，映射到这个组件中* @ConfigurationProperties 告诉SpringBoot 将配置文件中的属性映射到类属性中* prefix =\"person\" 和配置文件中person 下的属性进行一一映射* 只有这个组件是容器中的组件，才能使用容器提供@ConfigurationProperties 的功能* */@Component@Validated@ConfigurationProperties(prefix =\"person\")public class Person &#123;// @Value(\"$&#123;person.last-name&#125;\")// 必须是邮箱格式 @Email private String lastName;// @Value(\"#&#123;11*2&#125;\") private Integer age;// @Value(\"true\") private Boolean boss; private Date birth; private Map&lt;String,Object&gt; maps; private List&lt;Object&gt; lists; private Dog dog; 4.@PropertySource 自定义配置文件12345678910111213141516171819202122232425262728293031323334353637/** 将配置文件中的配置的每一个属性的值，映射到这个组件中* @ConfigurationProperties 告诉SpringBoot 将配置文件中的属性映射到类属性中* prefix =\"person\" 和配置文件中person 下的属性进行一一映射* 只有这个组件是容器中的组件，才能使用容器提供@ConfigurationProperties 的功能，默认从全局配置文件中获取值,自己的配置文件和全局起冲突，以全局为准* */@PropertySource(\"classpath:person.properties\")@Component@Validated@ConfigurationProperties(prefix =\"person\")public class Person &#123;// @Value(\"$&#123;person.last-name&#125;\")// 必须是邮箱格式 private String lastName;// @Value(\"#&#123;11*2&#125;\") private Integer age;// @Value(\"true\") private Boolean boss; private Date birth; private Map&lt;String,Object&gt; maps; private List&lt;Object&gt; lists; private Dog dog; @Override public String toString() &#123; return \"Person&#123;\" + \"lastName='\" + lastName + '\\'' + \", age=\" + age + \", boss=\" + boss + \", birth=\" + birth + \", maps=\" + maps + \", lists=\" + lists + \", dog=\" + dog + '&#125;'; &#125; 5.@ImportResource&amp;@BeanSpring BOOT 不能直接使用Spring 的xml 文件，自己的编写的配置文件也不能自动识别，可以通过@ImportResource标注在配置类上，配置类就是主配置类 bean.xml 1234567&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;bean id=\"helloservice\" class=\"com.groupid.springboot.service.HelloService\"&gt;&lt;/bean&gt;&lt;/beans&gt; 主配置类 12345678@ImportResource(locations = &#123;\"classpath:bean.xml\"&#125;)@SpringBootApplicationpublic class SpringBoot02ConfigApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringBoot02ConfigApplication.class, args); &#125;&#125; 测试类 12345678@AutowiredApplicationContext ioc;@Testpublic void testHello()&#123; Boolean res = ioc.containsBean(\"helloservice\"); System.out.println(res);&#125; SpringBoot 推荐全注解的方式, 1.配置类 @Configuration = sping 的配置文件 2.使用@bean给容器中添加组件 配置类 123456789101112@Configurationpublic class MyConfig &#123; /* * 默认bean的id就是方法名，用途是将方法的返回值添加到容器中 * */ @Bean public HelloService helloService()&#123; System.out.println(\"myconfig 的bean成功了\"); return new HelloService(); &#125;&#125; 测试类 123456789@AutowiredApplicationContext ioc;@Testpublic void testHello()&#123; //这里的helloService 是bean的id，和配置类中的方法名一一对应 Boolean res = ioc.containsBean(\"helloService\"); System.out.println(res);&#125;","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}]},{"title":"04_Spring Boot的hello world探究","slug":"04_Spring Boot的hello world探究","date":"2018-09-25T12:19:32.000Z","updated":"2018-11-26T01:33:39.749Z","comments":true,"path":"2018/09/25/04_Spring Boot的hello world探究/","link":"","permalink":"http://yoursite.com/2018/09/25/04_Spring Boot的hello world探究/","excerpt":"","text":"Spring Boot pom.xml1.pom文件版本仲裁1.父项目spring-boot-starter的parent123456789101112131415161718192021222324252627282930当前文件的父项目 以2.0.5.RELEASE 为例子 &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.5.RELEASE&lt;/version&gt; &lt;/parent&gt;spring-boot-starter-parent的父项目 &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;2.0.5.RELEASE&lt;/version&gt; &lt;relativePath&gt;../../spring-boot-dependencies&lt;/relativePath&gt; &lt;/parent&gt;这里面的依赖是来真正管理Spring Boot应用里面的所有依赖版本； &lt;properties&gt; &lt;activemq.version&gt;5.15.6&lt;/activemq.version&gt; &lt;antlr2.version&gt;2.7.7&lt;/antlr2.version&gt; &lt;appengine-sdk.version&gt;1.9.64&lt;/appengine-sdk.version&gt; &lt;artemis.version&gt;2.4.0&lt;/artemis.version&gt; &lt;aspectj.version&gt;1.8.13&lt;/aspectj.version&gt; &lt;assertj.version&gt;3.9.1&lt;/assertj.version&gt; &lt;atomikos.version&gt;4.0.6&lt;/atomikos.version&gt; &lt;bitronix.version&gt;2.1.4&lt;/bitronix.version&gt; .... &lt;wsdl4j.version&gt;1.6.3&lt;/wsdl4j.version&gt; &lt;xml-apis.version&gt;1.4.01&lt;/xml-apis.version&gt; &lt;xml-maven-plugin.version&gt;1.0.2&lt;/xml-maven-plugin.version&gt; &lt;xmlunit2.version&gt;2.5.1&lt;/xmlunit2.version&gt; &lt;/properties&gt; 这是Spring的版本仲裁中心，以后导入的包不需要写版本号的，当然如果不在这里面的包，要写版本号 2.启动器 spring-boot-starter的web模块123456&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 这里导入了web模块正常运行所依赖的场景组件 spring-boot-starter: spring boot 的场景启动器, spring boot将所有的功能场景抽取出来，做成一个个的starter(启动器)，只需要在项目里面引入这些starter相关场景的所有依赖都会导入进来，如果需要用什么场景，就导入什么场景的启动器 更多启动器 2.主程序类1234567891011/** * @SpringBootApplication 来标注一个主程序类，说明这是一个Spring Boot应用 */@SpringBootApplicationpublic class HelloWorldMainApp &#123; public static void main(String[] args) &#123; //运行这个方法就可以启动应用了 SpringApplication.run(HelloWorldMainApp.class,args); &#125;&#125; ==这里后面的我不是很清楚，需要再看&lt;&gt; 以及Spring 注解版== 由@SpringBootApplication标注的类，表明这个个类是SpringBoot的主配置类，Spring boot 就知道应该运行这个类的main方法来启动SpringBoot应用； 12345678910111213141516@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@SpringBootConfiguration@EnableAutoConfiguration@ComponentScan( excludeFilters = &#123;@Filter( type = FilterType.CUSTOM, classes = &#123;TypeExcludeFilter.class&#125;), @Filter( type = FilterType.CUSTOM, classes = &#123;AutoConfigurationExcludeFilter.class&#125;)&#125;)public @interface SpringBootApplication &#123; @SpringBootConfiguration :Spring Boot的配置类； 标注在某个类上，表示这是一个Spring Boot的配置类； @Configuration:配置类上来标注这个注解； 配置类 ——- 配置文件；配置类也是容器中的一个组件；@Component @EnableAutoConfiguration：开启自动配置功能； 以前我们需要配置的东西，Spring Boot帮我们自动配置；@EnableAutoConfiguration告诉SpringBoot开启自动配置功能；这样自动配置才能生效； 123@AutoConfigurationPackage@Import(EnableAutoConfigurationImportSelector.class)public @interface EnableAutoConfiguration &#123; @AutoConfigurationPackage：自动配置包@Import(AutoConfigurationPackages.Registrar.class)： Spring的底层注解@Import，给容器中导入一个组件；导入的组件由AutoConfigurationPackages.Registrar.class； ==将主配置类（@SpringBootApplication标注的类）的所在包及下面所有子包里面的所有组件扫描到Spring容器；==@Import(EnableAutoConfigurationImportSelector.class)； 给容器中导入组件？ EnableAutoConfigurationImportSelector：导入哪些组件的选择器； 将所有需要导入的组件以全类名的方式返回；这些组件就会被添加到容器中； 会给容器中导入非常多的自动配置类（xxxAutoConfiguration）；就是给容器中导入这个场景需要的所有组件，并配置好这些组件； ==Spring Boot在启动的时候从类路径下的META-INF/spring.factories中获取EnableAutoConfiguration指定的值，将====这些值作为自动配置类导入到容器中，自动配置类就生效，帮我们进行自动配置工作；==以前我们需要自己配置的东西，自动配置类都帮我们； J2EE的整体整合解决方案和自动配置都在spring-boot-autoconfigure-1.5.9.RELEASE.jar； ==Spring注解版（谷粒学院）==","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}]},{"title":"07_配置文件的占位符","slug":"07_配置文件的占位符","date":"2018-09-25T12:19:32.000Z","updated":"2018-11-26T01:33:24.377Z","comments":true,"path":"2018/09/25/07_配置文件的占位符/","link":"","permalink":"http://yoursite.com/2018/09/25/07_配置文件的占位符/","excerpt":"","text":"配置文件的占位符 随机数 12$&#123;random.value&#125;、$&#123;random.int&#125;、$&#123;random.long&#125;$&#123;random.int(10)&#125;、$&#123;random.int[1024,65536]&#125; 占位符获取之前配置的值，如果没有可以用：指定默认值 12345678person.last-name=李四$&#123;random.value&#125;person.age=$&#123;random.int&#125;person.birth=2017/12/18person.maps.k1=v1person.maps.k2 = v2person.lists=a,b,cperson.dog.name = $&#123;person.hello:hello&#125;dogperson.dog.age = 3","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}]},{"title":"05_Spring Initializer 快速创建Spring Boot项目","slug":"05_Spring Initializer 快速创建Spring Boot项目","date":"2018-09-25T12:19:32.000Z","updated":"2018-11-26T01:33:32.961Z","comments":true,"path":"2018/09/25/05_Spring Initializer 快速创建Spring Boot项目/","link":"","permalink":"http://yoursite.com/2018/09/25/05_Spring Initializer 快速创建Spring Boot项目/","excerpt":"","text":"IDE 都支持使用Spring 的创建向导快速创建项目选择你需要的启动器，向导会联网创建Spring Boot项目 默认生成Spring Boot 项目： 主程序已经ok了 resources 文件夹中的目录结构 static 保存静态资源：js css images; templates 保存所有的模板页面：（默认jar包，默认不支持jsp, 但是可以使用模板引擎，freemarker,thymeleaf） application.properties :Spring Boot 的默认配置文件，可以修改默认配置，直接打server. 就可以了","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}]},{"title":"02_Spring Boot的配置","slug":"02_Spring Boot的配置","date":"2018-09-21T12:19:32.000Z","updated":"2018-11-26T01:33:53.736Z","comments":true,"path":"2018/09/21/02_Spring Boot的配置/","link":"","permalink":"http://yoursite.com/2018/09/21/02_Spring Boot的配置/","excerpt":"","text":"Spring Boot环境约束jdk 1.8 : Spring Boot 推荐1.7以上 Maven 3.3以上版本 IntelliJIDEA2017：IntelliJ IDEA 2017.2.2 x64、STS，STS SpringBoot 1.5.9.RELEASE：1.5.9； maven 的设置给\\conf\\settings.xml 加上代码 123456789101112&lt;profile&gt; &lt;id&gt;jdk‐1.8&lt;/id&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;jdk&gt;1.8&lt;/jdk&gt; &lt;/activation&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;maven.compiler.compilerVersion&gt;1.8&lt;/maven.compiler.compilerVersion&gt; &lt;/properties&gt;&lt;/profile&gt; IDEA 设置在Configure -&gt; Setting -&gt; Build -&gt; build tools -&gt;Maven 中修改maven 配置","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}]},{"title":"01_Spring Boot简介","slug":"01_Spring Boot简介","date":"2018-09-21T12:19:32.000Z","updated":"2018-11-26T01:33:58.361Z","comments":true,"path":"2018/09/21/01_Spring Boot简介/","link":"","permalink":"http://yoursite.com/2018/09/21/01_Spring Boot简介/","excerpt":"","text":"Spring Boot简介Spring boot是一个伟大的框架，简化了应用开发，约定大于配置，just run 就可以创建一个独立的，产品级的应用 简化Spring应用开发额的一个框架 整个Spring技术栈的一个大整合 J2EE 开发的一站式解决方案 官网 优点 快速创建 使用嵌入式Servlet，不用打包成war 直接打包成jar包，都不要安装tomcat,爽！ starters 自动依赖与版本控制, 比如 你要用redis，那么可以直接导入lib包并且控制好版本 大量的自动配置，简化开发，也可以修改默认值，以前用个SSM框架，配置来配置去的 无需配置XML，并不是代码生成器生成的，所以无代码生成，开箱就可以用，告别大量xml编写时代 准生产环境的运行时应用监控 与云计算的天然继承 缺点入门容易，精通难，只有对spring底层API 很熟悉，才能做深度定制 微服务? 微服务是一种架构风格，martin fowler在2014年提出，一个应用应该是一组小型服务，可以通过http的方式进行互通 单体应用:所有的功能都写在一个应用里面，所有的牵一发动全身，当资源不够时，通过多个服务器来负载均衡 每个一个功能元素最终都是一个课独立替换，独立升级的软件单元 详细参照微服务文档 实际工作起来就像神经网络，对运维要求一下子变高了 Spring Boot 构建服务，通过Spring cloud 进行互联互调，用Spring cloud Data进行流氏数据计算批处理","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}],"keywords":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/categories/SpringBoot/"}]},{"title":"序列模型RNN","slug":"序列模型RNN","date":"2018-06-20T12:19:32.000Z","updated":"2018-09-14T08:47:17.071Z","comments":true,"path":"2018/06/20/序列模型RNN/","link":"","permalink":"http://yoursite.com/2018/06/20/序列模型RNN/","excerpt":"","text":"要点怎么样处理作为一个单词序列的文本？或是一段音频呢？之前的神经网络都是只能处理固定长度的输入，如果输入不固定怎么办； 拿CNN来看看，CNN是在不同的空间上共享参数；扩展一下，是否可以在时间维度上共享参数呢。这个就是递归神经网络的初心了，现在流行这个词。 如果只是需要在每个时间点根据序列中的事件（状态）做出预测，那么你就可以在每个时间点用同样的分类器，比较简单； 但是我们说的话是前面的词语对后面的词语都有影响，所以早期的思想是用递归地使用先前分类器的，记录下先前分类器的状态，所以你需要一个很深的神经网络，这样网络可能要几百，几千层； 现在的我们采用的是一种较为简单的分类器，一部分连接到输入层，一部分连接到过去的事件 为什么RNN的反向传播不使用梯度下降？梯度下降要从输出层一直计算到输入层，这些导数都会作用到w参数上，但是这个模型都是共用参数w的，梯度下降偏好于无关联的参数更新，这样每层的学到东西才会保留下来；如果RNN反向传播使用梯度下降，那么要么梯度变到无穷大，要么变到无穷小…就没办法训练了 如何解决梯度爆炸？梯度裁剪，超过一定范围，就进行缩放 如何解决梯度消失？梯度消失就是模型只记住了近期的事件，序列稍长久不管用了，如何解决这个问题呢，LSTM(长短记忆模型登场了) LSTM是啥？RNN的细胞单元有 两个输入 （过去的状态，当前的序列）；两个输出（当前的预测，当前的状态） LSTM的细胞需要做3件事情： w:将数据写到记忆中 r:读取记忆中的数据 d:忘记记忆中的数据 如何给LSTM的门下指令呢？设置权重范围【0,1】，如果是0全部不做，如果是1 全部做，0.6 部分做；这样的话权重范围就是个连续的函数，就可以求导，并且进行反向传播 实际上，每个指令都是由一个共享参数的逻辑回归分类器来控制的，并且还通过tanh来保障输出值在【-1,1】之间 如何对LSTM 进行正则化？可以进行L2 以及Droupout； 使用Dropout时要记住只能用在序列输入，以及预测输出，不能用在过去和未来的状态传递上 Beam搜索是啥？当你预测句子中下一个出现的单词时，你可以遍历所有的词汇库来找出最大概率的词； 更复杂一点是每次保存所有单词，在所有单词的基础上，继续预测，最后通过概率相乘来计算所有预测的概率，但是这个很奢侈； 所以改进的方法就是只保留每个时间步最有可能的几个单词，这就是束搜索","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"RNN","slug":"深度学习/RNN","permalink":"http://yoursite.com/categories/深度学习/RNN/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"序列","slug":"序列","permalink":"http://yoursite.com/tags/序列/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"RNN","slug":"深度学习/RNN","permalink":"http://yoursite.com/categories/深度学习/RNN/"}]},{"title":"使用TensorFlow训练一个 CBOW模型并可视化输出","slug":"使用TensorFlow训练一个 CBOW模型并可视化输出","date":"2018-06-17T12:19:32.000Z","updated":"2018-09-14T08:47:07.681Z","comments":true,"path":"2018/06/17/使用TensorFlow训练一个 CBOW模型并可视化输出/","link":"","permalink":"http://yoursite.com/2018/06/17/使用TensorFlow训练一个 CBOW模型并可视化输出/","excerpt":"","text":"要点思路一： 训练数据的变化： 如果原来的训练数据是 1data: [&apos;anarchism&apos;, &apos;originated&apos;, &apos;as&apos;, &apos;a&apos;, &apos;term&apos;, &apos;of&apos;, &apos;abuse&apos;, &apos;first&apos;] 就是用上下文来预测这个单词，比如单词originated，左右的skip_window = 1，那么可以产生以下预测： 123with skip_window = 1: batch: [&apos;anarchism&apos;, &apos;as&apos;] labels: [&apos;originated&apos;, &apos;originated&apos;] 例如整个句子可以在【skip_window：batch_size-skip_window】中随机选择 batch_size/num_skips个单词，然后预测这个单词周围的上下文,如下方式早出训练数据 123with num_skips = 2 and skip_window = 1: batch: [&apos;anarchism&apos;, &apos;as&apos;, &apos;a&apos;, &apos;of&apos;, &apos;of&apos;, &apos;first&apos;, &apos;term&apos;, &apos;abuse&apos;] labels: [&apos;originated&apos;, &apos;originated&apos;, &apos;term&apos;, &apos;term&apos;, &apos;abuse&apos;, &apos;abuse&apos;, &apos;of&apos;, &apos;of&apos;] 思路二： 利用上下文的单词的sum ,来预测单词 1data: [&apos;anarchism&apos;, &apos;originated&apos;, &apos;as&apos;, &apos;a&apos;, &apos;term&apos;, &apos;of&apos;, &apos;abuse&apos;, &apos;first&apos;] 比如左右上下文范围都是1 比如单词originated 123with skip_window = 1: batch: [&apos;anarchism&apos;+&apos;as&apos;] labels: [&apos;originated&apos;] 整个句子可以做出如下样子 123with num_skips = 2 and skip_window = 1: batch: [&apos;anarchism&apos;+&apos;as&apos;, &apos;originated&apos;+&apos;a&apos;, &apos;as&apos;+&apos;term&apos; ... ] labels: [&apos;originated&apos;,&apos;as&apos;, &apos;a&apos; ...]","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"RNN","slug":"深度学习/RNN","permalink":"http://yoursite.com/categories/深度学习/RNN/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"CBOW","slug":"CBOW","permalink":"http://yoursite.com/tags/CBOW/"},{"name":"词嵌入","slug":"词嵌入","permalink":"http://yoursite.com/tags/词嵌入/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"RNN","slug":"深度学习/RNN","permalink":"http://yoursite.com/categories/深度学习/RNN/"}]},{"title":"使用TensorFlow训练一个 skip-gram 模型并可视化输出","slug":"TensorFlow实现 skip-gram 模型并可视化输出","date":"2018-06-15T12:19:32.000Z","updated":"2018-09-07T05:38:20.444Z","comments":true,"path":"2018/06/15/TensorFlow实现 skip-gram 模型并可视化输出/","link":"","permalink":"http://yoursite.com/2018/06/15/TensorFlow实现 skip-gram 模型并可视化输出/","excerpt":"","text":"要点我们看深度学习如何处理文本的 1. 目标：训练一个skip-gram模型2.什么是Skip-Gram模型 和 CBOW 模型如果是用一个词语作为输入，来预测它周围的上下文，那这个模型叫做【Skip-gram】 而如果是拿一个词语的上下文作为输入，来预测这个词语本身，则是 【CBOW 】 CBOW对小型数据库比较合适，而Skip-Gram在大型语料中表现更好。 3.简单了解下Skip-Gram模型比如『她们 夸 吴彦祖 帅 到 没朋友』，如果输入 x 是『吴彦祖』，那么 y 可以是『她们』、『夸』、『帅』、『没朋友』这些词 我们的目的就是通过词嵌入 计算 目标和预测相同的概率 one-hot ——&gt; 词嵌入 ——&gt;softmax——&gt; 计算概率 4.简单了解下CBOW跟 Skip-gram 相似，只不过: Skip-gram 是预测一个词的上下文，而 CBOW 是用上下文预测这个词 网络结构如下 5.介绍下数据集使用的是Text8数据集，压缩后大小是29.9M，包含单词个数 17005207个，大约1700万 函数collections.CounterCounter类的目的是用来跟踪值出现的次数。它是一个无序的容器类型，以字典的键值对形式存储，其中元素作为key，其计数作为value；类似于java的map most_common([n])返回一个TopN列表。如果n没有被指定，则返回所有元素。当多个元素计数值相同时，排列是无确定顺序的。 12345words = [&apos;hello&apos;,&apos;world&apos;,&apos;hello&apos;,&apos;hello&apos;,&apos;me&apos;]collections.Counter(words)&gt;&gt;&gt;&gt; Counter(&#123;&apos;hello&apos;: 3, &apos;world&apos;: 1, &apos;me&apos;: 1&#125;)collections.Counter(words).most_common(2)&gt;&gt;&gt;&gt;&gt; [(&apos;hello&apos;, 3), (&apos;world&apos;, 1)] extend()Python List extend()方法,extend() 函数用于在列表末尾一次性追加另一个序列中的多个值（用新列表扩展原来的列表）。 dict()字典是另一种可变容器模型，且可存储任意类型对象。字典的每个键值 key=&gt;value 对用冒号 : 分割 1mydict = &#123;&apos;a&apos;: 1, &apos;b&apos;: 2, &apos;b&apos;: &apos;3&apos;&#125;; 如何反转键值对呢 1reversed_dict = dict(zip(mydict.values(),mydict.keys())) tf.truncated_normal与tf.random_normal区别就是truncated_normal的随机值范围是 两个标准差以内 (mean?2stddev,mean+2stddev)\\\\ stddev=\\sigma=\\sqrt{\\frac{\\sum_{i=0}^{N}(x_i-u)^2}{N}}12345tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) shape：1-D向量 mean :均值，默认为0，即正态分布中的μstddev :标准差，默认为1，即正态分布中的σseed : 种子，同一个seed下的分布值均相同； 1234567def tf.truncated_normal( shape, #一个一维整数张量 或 一个Python数组。 这个值决定输出张量的形状。 mean=0.0,#一个零维张量或 类型属于dtype的Python值. 这个值决定正态分布片段的平均值 stddev=1.0,# 一个零维张量或 类型属于dtype的Python值. 这个值决定正态分布片段的标准差 dtype=tf.float32,# 输出的类型. seed=None, # 一个Python整数. 被用来为正态分布创建一个随机种子. name=None)#操作的名字 (可选参数). tf.random_uniform从字面uniform看就是均匀分布的；random_uniform:均匀分布随机数，范围为[minval,maxval] 123tf.random_uniform(shape,minval=0,maxval=None,dtype=tf.float32) #例如 返回6*6的矩阵，产生于low和high之间，产生的值是均匀分布的tf.random_uniform((6, 6), minval=low,maxval=high,dtype=tf.float32))) tf.nn.embedding_lookup123tf.nn.embedding_lookup(params, ids, partition_strategy=&apos;mod&apos;, name=None, validate_indices=True, max_norm=None)partition_strategy 是&quot;mod&quot; 和 &quot;div&quot;两种参数 embedding_lookup(params, ids)其实就是按照ids，返回params中的ids行的向量。 比如说，ids=[1,3,2],就是返回params中第1,3,2行。返回结果为由params的1,3,2行向量组成的tensor。 tf.nn.sampled_softmax_loss先采样后计算交叉熵损失 12345678910111213sampled_softmax_loss( weights, biases, labels, inputs, num_sampled, #采样的个数 num_classes, #总的类别个数 num_true=1, sampled_values=None, remove_accidental_hits=True, partition_strategy='mod', name='sampled_softmax_loss') 余弦相似度 similarity = cos(\\theta) = { A \\cdot B \\over \\|A\\| \\cdot \\|B\\| } = A_{normalized} \\cdot B_{normalized}所以归一化所有的词嵌入向量后，方便计算余弦相似度啊，只要做矩阵乘法，就可以得出所有向量的向量的相似度 numpy.argsort元素从小到大排序，返回其索引 1234argsort(a, axis=-1, kind=&apos;quicksort&apos;, order=None)例子：np.argsort(-x) #按降序排列np.argsort(x) #按升序排列 扩展一下： 对list 排序用sort(). 对任何可迭代序列排序用sorted(). 代码12345678910111213%matplotlib inlinefrom __future__ import print_functionimport collectionsimport mathimport numpy as npimport osimport randomimport tensorflow as tfimport zipfilefrom matplotlib import pylabfrom six.moves import rangefrom six.moves.urllib.request import urlretrievefrom sklearn.manifold import TSNE C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters 1234567891011121314151617# 下载文本数据集url = 'http://mattmahoney.net/dc/'def maybe_download(filename, expected_bytes): \"\"\"Download a file if not present, and make sure it's the right size.\"\"\" if not os.path.exists(filename): filename, _ = urlretrieve(url + filename, filename) statinfo = os.stat(filename) if statinfo.st_size == expected_bytes: print('Found and verified %s' % filename) else: print(statinfo.st_size) raise Exception( 'Failed to verify ' + filename + '. Can you get to it with a browser?') return filenamefilename = maybe_download('text8.zip', 31344016) Found and verified text8.zip 123456789# 将数据集读成Stringdef read_data(filename): \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\" with zipfile.ZipFile(filename) as f: data = tf.compat.as_str(f.read(f.namelist()[0])).split() return data words = read_data(filename)print('Data size %d' % len(words)) Data size 17005207 12345678910111213141516171819202122232425262728293031323334# 弄个字典存放单词，去掉频率极少的单词，加入UNK 作为未见单词分类，# 先弄个5万个单词vocabulary_size = 50000def build_dataset(words): count = [['UNK', -1]] # extend() 函数用于在列表末尾一次性追加另一个序列中的多个值（用新列表扩展原来的列表）。 # count 中存放着最常用5万个单词，unk 作为第一个单词 count.extend(collections.Counter(words).most_common(vocabulary_size - 1)) dictionary = dict() for word, _ in count: # 把最常见5万的word 放入 dictionary dictionary[word] = len(dictionary) data = list() unk_count = 0 # 清点words中不在5万个单词内的unk的个数； # 把words文本表示成 dict 中的value # 比如 “ my name is sudoli” ==&gt; \"[12,45,48,unk]\" for word in words: if word in dictionary: index = dictionary[word] else: index = 0 # dictionary['UNK'] unk_count = unk_count + 1 data.append(index) count[0][1] = unk_count # 把键值对交换一下 reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) return data, count, dictionary, reverse_dictionarydata, count, dictionary, reverse_dictionary = build_dataset(words)print('Most common words (+UNK)', count[:5])print('Sample data', data[:10])del words # 减少内存损耗 Most common words (+UNK) [[&#39;UNK&#39;, 418391], (&#39;the&#39;, 1061396), (&#39;of&#39;, 593677), (&#39;and&#39;, 416629), (&#39;one&#39;, 411764)] Sample data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156] 12345sentens = [];for i in data[100:110]: sentens.append(reverse_dictionary[i]); print(sentens) [&#39;interpretations&#39;, &#39;of&#39;, &#39;what&#39;, &#39;this&#39;, &#39;means&#39;, &#39;anarchism&#39;, &#39;also&#39;, &#39;refers&#39;, &#39;to&#39;, &#39;related&#39;] 12345678910111213141516171819202122232425262728293031323334353637383940# 为skip-gram 生成一个训练批次data_index = 0# batch_size 批次大小# num_skips 以这个单词为中心选择多少个单词# skip_window 在多大的窗口中选择，关键词的两边的窗口中的所有单词==num_skips；所以有num_skips = 2 * skip_window,但是也可以不取满def generate_batch(batch_size, num_skips, skip_window): global data_index assert batch_size % num_skips == 0 assert num_skips &lt;= 2 * skip_window # batch 是个list batch = np.ndarray(shape=(batch_size), dtype=np.int32) #labels 是个矩阵，bacth_size *1 的矩阵 labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32) span = 2 * skip_window + 1 # [ skip_window target skip_window ] # 双向队列 buffer = collections.deque(maxlen=span) for _ in range(span): buffer.append(data[data_index]) data_index = (data_index + 1) % len(data) for i in range(batch_size // num_skips): target = skip_window # target label at the center of the buffer targets_to_avoid = [ skip_window ] for j in range(num_skips): while target in targets_to_avoid: target = random.randint(0, span - 1) targets_to_avoid.append(target) batch[i * num_skips + j] = buffer[skip_window] labels[i * num_skips + j, 0] = buffer[target] buffer.append(data[data_index]) data_index = (data_index + 1) % len(data) return batch, labelsprint('data:', [reverse_dictionary[di] for di in data[:12]])for num_skips, skip_window in [(2, 1), (4, 2)]: data_index = 0 batch, labels = generate_batch(batch_size=12, num_skips=num_skips, skip_window=skip_window) print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window)) print(' batch:', [reverse_dictionary[bi] for bi in batch]) print(' labels:', [reverse_dictionary[li] for li in labels.reshape(12)]) data: [&#39;anarchism&#39;, &#39;originated&#39;, &#39;as&#39;, &#39;a&#39;, &#39;term&#39;, &#39;of&#39;, &#39;abuse&#39;, &#39;first&#39;, &#39;used&#39;, &#39;against&#39;, &#39;early&#39;, &#39;working&#39;] with num_skips = 2 and skip_window = 1: batch: [&#39;originated&#39;, &#39;originated&#39;, &#39;as&#39;, &#39;as&#39;, &#39;a&#39;, &#39;a&#39;, &#39;term&#39;, &#39;term&#39;, &#39;of&#39;, &#39;of&#39;, &#39;abuse&#39;, &#39;abuse&#39;] labels: [&#39;anarchism&#39;, &#39;as&#39;, &#39;a&#39;, &#39;originated&#39;, &#39;term&#39;, &#39;as&#39;, &#39;a&#39;, &#39;of&#39;, &#39;abuse&#39;, &#39;term&#39;, &#39;first&#39;, &#39;of&#39;] with num_skips = 4 and skip_window = 2: batch: [&#39;as&#39;, &#39;as&#39;, &#39;as&#39;, &#39;as&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;term&#39;, &#39;term&#39;, &#39;term&#39;, &#39;term&#39;] labels: [&#39;anarchism&#39;, &#39;originated&#39;, &#39;term&#39;, &#39;a&#39;, &#39;originated&#39;, &#39;term&#39;, &#39;of&#39;, &#39;as&#39;, &#39;abuse&#39;, &#39;a&#39;, &#39;as&#39;, &#39;of&#39;] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 接下来就是核心部分了 skip-grambatch_size = 128embedding_size = 128 # 这个就是最终词嵌入向量的维度数skip_window = 1 # 这个就是左右窗口的要考虑的大小了num_skips = 2 # 总共预测关键词周围几个单词# 我们再随机构建一个验证集，验证集选择最常见的单词valid_size = 16 # Random set of words to evaluate similarity on.valid_window = 100 # Only pick dev samples in the head of the distribution.# 从100的窗口范围内，采样16个样本大小valid_examples = np.array(random.sample(range(valid_window), valid_size))num_sampled = 64 # 负采样样本个数graph = tf.Graph()# TF不区分CPU的设备号，设置为0即可；若设置GPU 对显存要求大with graph.as_default(), tf.device('/cpu:0'): # Input data. train_dataset = tf.placeholder(tf.int32, shape=[batch_size]) train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1]) valid_dataset = tf.constant(valid_examples, dtype=tf.int32) # Variables. # 均匀分布的【-1,1】的embeddings embeddings = tf.Variable( tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)) # 权重参数采用两个标准差内的truncated_normal，叫做权重归一化，防止梯度消失 softmax_weights = tf.Variable( tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size))) # 偏移量直接就是0 softmax_biases = tf.Variable(tf.zeros([vocabulary_size])) # Model. # Look up embeddings for inputs.就是把train_dataset中的数字变成embeddings 中128维向量 embed = tf.nn.embedding_lookup(embeddings, train_dataset) # Compute the softmax loss, using a sample of the negative labels each time. # 采用负采样计算softmax loss = tf.reduce_mean( tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed, labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size)) # Optimizer. # Note: The optimizer will optimize the softmax_weights AND the embeddings. # 优化器将更新softmax_weights 以及 embeddings.因为embeddings被定义为变量，minimize()默认修改所有的变量 # 这个就是风格画产生的原因之一 optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss) # 计算所有批次和valid_dataset 的余弦相似度 # 余弦相似度 # 求出每个embeddings每个向量的长度，就是求模 norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True)) # embeddings 每个向量进行归一化，将每个向量各自做归一化，就是去掉了向量的长度，只关注向量的方向 normalized_embeddings = embeddings / norm # 所以比较向量的时候只用了方向信息，余弦相似度 valid_embeddings = tf.nn.embedding_lookup( normalized_embeddings, valid_dataset) similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings)) WARNING:tensorflow:From C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version. Instructions for updating: Future major versions of TensorFlow will allow gradients to flow into the labels input on backprop by default. See tf.nn.softmax_cross_entropy_with_logits_v2. 1234567891011121314151617181920212223242526272829303132num_steps = 100001with tf.Session(graph=graph) as session: tf.global_variables_initializer().run() print('Initialized') average_loss = 0 for step in range(num_steps): batch_data, batch_labels = generate_batch( batch_size, num_skips, skip_window) feed_dict = &#123;train_dataset : batch_data, train_labels : batch_labels&#125; _, l = session.run([optimizer, loss], feed_dict=feed_dict) average_loss += l if step % 2000 == 0: if step &gt; 0: average_loss = average_loss / 2000 # 平均损失时对过去2000批次的损失的估计 print('Average loss at step %d: %f' % (step, average_loss)) average_loss = 0 # 这个计算代价很大 5万的矩阵归一化，后矩阵相乘； if step % 10000 == 0: sim = similarity.eval() for i in range(valid_size): valid_word = reverse_dictionary[valid_examples[i]] top_k = 8 # 选取to_k 的近似值,这里argsort是 nearest = (-sim[i, :]).argsort()[1:top_k+1] log = 'Nearest to %s:' % valid_word for k in range(top_k): close_word = reverse_dictionary[nearest[k]] log = '%s %s,' % (log, close_word) print(log) final_embeddings = normalized_embeddings.eval() Initialized Average loss at step 0: 7.906703 Nearest to is: bobble, venn, rodeo, ric, walks, shemini, graphite, instituting, Nearest to s: impart, understandably, waveguide, yakko, whittier, lingual, protestant, lautering, Nearest to who: haken, sol, javier, dressage, clay, almeida, fruiting, plans, Nearest to time: meninges, tley, attends, unimpressed, sep, acct, wrestlers, disabled, Nearest to four: repulsed, carrying, boeing, adjectival, zarathustra, locate, bittorrent, scilly, Nearest to or: starvation, headphones, statisticians, destitute, pragmatics, aten, hogs, chamada, Nearest to has: spenser, showings, cassiodorus, taste, afterwards, numeration, periods, judicial, Nearest to about: baker, lasting, great, spiny, hobbs, jiang, mistakenly, enemy, Nearest to such: insistent, awacs, accomplices, jokes, trivial, cigarettes, vista, enactments, Nearest to five: airspace, isomorphic, hahnemann, ddd, hallstatt, algonquian, substance, fantasia, Nearest to when: buffalo, formulas, homerun, karate, indic, andalus, subservient, klan, Nearest to there: sacks, yaoi, textrm, indigenous, inhibition, somerville, kournikova, repainted, Nearest to use: javan, islands, mengele, karim, carr, conses, syllogism, goodfellas, Nearest to i: regan, adagio, premiership, memos, payroll, mccormick, varicella, gow, Nearest to will: xiang, prosper, moldavian, stitch, geothermal, attracting, infection, livius, Nearest to no: commanded, massacre, gluck, philippine, hundred, chordata, dark, shor, Average loss at step 2000: 4.378162 Average loss at step 4000: 3.866741 Average loss at step 6000: 3.789374 Average loss at step 8000: 3.693026 Average loss at step 10000: 3.616336 Nearest to is: was, are, has, mjf, seconded, malenkov, balm, be, Nearest to s: his, and, my, haram, ghiorso, aloe, consequently, kusanagi, Nearest to who: he, they, still, and, proviso, then, fruiting, underwent, Nearest to time: unimpressed, him, intravenous, wrestlers, key, way, miyamoto, kahan, Nearest to four: three, eight, seven, six, five, two, nine, zero, Nearest to or: and, propanol, dissipation, zombies, than, overtraining, archaeologists, hearted, Nearest to has: had, is, was, have, cassiodorus, afterwards, numeration, taste, Nearest to about: docklands, mistakenly, raymond, rapcore, alcaeus, friends, fuelling, spiny, Nearest to such: known, conifers, robot, ok, adventurers, well, creator, accomplices, Nearest to five: eight, seven, nine, six, four, three, two, zero, Nearest to when: donohue, parson, bewildering, andalus, describes, saimei, buffalo, honorably, Nearest to there: it, they, sacks, repainted, he, haley, spitz, spiked, Nearest to use: crew, appending, mengele, bands, generators, cowardice, dtv, karim, Nearest to i: minute, constitutes, swnt, singaporean, uttered, peachtree, filaments, slaughterhouse, Nearest to will: xiang, would, can, to, must, prosper, moldavian, cdma, Nearest to no: massacre, named, machina, it, dried, minimized, dark, gluck, Average loss at step 12000: 3.605883 Average loss at step 14000: 3.577284 Average loss at step 16000: 3.407318 Average loss at step 18000: 3.461814 Average loss at step 20000: 3.540494 Nearest to is: was, are, has, mjf, be, rectangles, myrrh, were, Nearest to s: chew, gtpase, byron, ghiorso, bulfinch, expletive, aleksander, legnica, Nearest to who: he, then, which, they, also, often, still, married, Nearest to time: way, intravenous, unimpressed, wrestlers, key, bakunin, chernobyl, diatessaron, Nearest to four: three, five, six, two, seven, eight, nine, zero, Nearest to or: and, than, with, encouraging, mishna, dissipation, like, stingray, Nearest to has: had, is, have, was, multipart, afterwards, cassiodorus, pap, Nearest to about: mistakenly, docklands, fuelling, alcaeus, derived, heathen, next, indulging, Nearest to such: known, irrigation, conifers, well, robot, many, kanal, circulatory, Nearest to five: six, four, three, seven, zero, eight, two, nine, Nearest to when: since, before, donohue, but, parson, cryptographer, culver, reputations, Nearest to there: it, they, he, which, often, excluding, bunny, algebra, Nearest to use: crew, mengele, bands, appending, generators, dtv, deceive, cowardice, Nearest to i: ii, minute, constitutes, foregoing, filaments, mastered, we, fatal, Nearest to will: would, can, must, may, to, but, could, xiang, Nearest to no: massacre, dried, dark, it, a, gluck, topple, flagpole, Average loss at step 22000: 3.504591 Average loss at step 24000: 3.490641 Average loss at step 26000: 3.483303 Average loss at step 28000: 3.475509 Average loss at step 30000: 3.505294 Nearest to is: was, has, are, be, were, mjf, myrrh, became, Nearest to s: his, haram, antonin, byron, of, bends, harmonically, bey, Nearest to who: he, they, which, also, still, then, often, she, Nearest to time: unsympathetic, way, belief, key, intravenous, diatessaron, maligned, chernobyl, Nearest to four: five, six, eight, seven, three, two, nine, zero, Nearest to or: and, but, than, bakelite, like, mishna, skier, as, Nearest to has: had, have, is, was, praises, osvaldo, afterwards, having, Nearest to about: alcaeus, mistakenly, docklands, domestic, financial, reformation, devotees, friends, Nearest to such: known, well, these, irrigation, conifers, many, other, robot, Nearest to five: four, eight, seven, six, three, nine, zero, two, Nearest to when: before, but, if, after, was, since, while, fz, Nearest to there: they, it, he, often, still, usually, bunny, we, Nearest to use: crew, mengele, bands, goods, demography, maitreya, purify, deceive, Nearest to i: ii, we, minute, iii, constitutes, doubleday, foregoing, prestige, Nearest to will: can, would, may, must, could, should, to, cannot, Nearest to no: dried, any, dark, a, it, only, machina, there, Average loss at step 32000: 3.502657 Average loss at step 34000: 3.492980 Average loss at step 36000: 3.460367 Average loss at step 38000: 3.306287 Average loss at step 40000: 3.429836 Nearest to is: was, has, be, are, gaylord, myrrh, inr, cryptography, Nearest to s: his, bey, vinegar, her, while, haram, harmonically, disguise, Nearest to who: he, which, also, often, still, they, broadly, condensates, Nearest to time: way, year, unsympathetic, key, unimpressed, shays, poetical, maligned, Nearest to four: six, three, five, seven, eight, two, nine, one, Nearest to or: and, than, a, the, with, tony, querying, cca, Nearest to has: had, have, was, is, having, osvaldo, afterwards, inquisitorial, Nearest to about: alcaeus, domestic, mistakenly, docklands, koan, financial, on, before, Nearest to such: known, well, these, irrigation, many, willing, conifers, lahore, Nearest to five: seven, six, four, three, eight, nine, zero, two, Nearest to when: before, if, while, where, after, but, was, fz, Nearest to there: it, they, he, still, often, usually, these, which, Nearest to use: purify, mengele, goods, interfere, form, measure, bands, dtv, Nearest to i: ii, we, you, t, they, he, volumes, constitutes, Nearest to will: would, can, must, may, could, should, cannot, to, Nearest to no: any, dried, dark, it, profoundly, flip, than, theories, Average loss at step 42000: 3.443375 Average loss at step 44000: 3.449424 Average loss at step 46000: 3.455859 Average loss at step 48000: 3.352189 Average loss at step 50000: 3.383797 Nearest to is: was, are, myrrh, has, be, mjf, gaylord, lucian, Nearest to s: his, bey, ztas, stranded, antonin, and, ghanaian, briand, Nearest to who: he, which, she, also, manchu, then, there, often, Nearest to time: way, year, unsympathetic, period, key, maligned, shays, unimpressed, Nearest to four: six, eight, three, five, seven, nine, two, zero, Nearest to or: and, than, marcius, while, extrasolar, murray, like, landlocked, Nearest to has: had, have, was, is, does, having, applicable, osvaldo, Nearest to about: koan, mistakenly, alcaeus, docklands, nous, leni, how, meters, Nearest to such: well, these, known, many, irrigation, conifers, perish, guericke, Nearest to five: six, four, seven, eight, three, zero, nine, two, Nearest to when: while, if, after, before, where, but, since, although, Nearest to there: they, it, he, often, still, usually, these, bunny, Nearest to use: mengele, measure, portions, goods, purify, form, maitreya, interfere, Nearest to i: ii, we, you, they, foregoing, iii, he, t, Nearest to will: would, can, must, could, may, should, cannot, might, Nearest to no: any, dried, a, she, flip, distinguishable, ellefson, sharps, Average loss at step 52000: 3.436850 Average loss at step 54000: 3.430339 Average loss at step 56000: 3.442089 Average loss at step 58000: 3.393397 Average loss at step 60000: 3.393661 Nearest to is: was, has, are, although, myrrh, be, becomes, gaylord, Nearest to s: diff, decrypted, haram, px, ghiorso, bulfinch, bey, hne, Nearest to who: he, which, she, they, still, never, also, married, Nearest to time: way, unsympathetic, period, maligned, shays, place, year, comprehensively, Nearest to four: six, five, eight, three, seven, nine, zero, two, Nearest to or: and, than, but, including, like, ollie, manchukuo, olson, Nearest to has: had, have, is, was, having, plays, thinks, although, Nearest to about: koan, alcaeus, lasting, how, docklands, nous, what, domestic, Nearest to such: known, these, well, many, including, irrigation, lahore, venezia, Nearest to five: four, six, three, eight, seven, nine, zero, two, Nearest to when: if, before, after, while, where, although, because, though, Nearest to there: they, it, often, still, he, usually, she, now, Nearest to use: measure, mengele, portions, goods, infraclass, cause, most, norah, Nearest to i: we, ii, you, they, foregoing, t, doubleday, iii, Nearest to will: would, must, may, can, could, should, cannot, might, Nearest to no: any, sex, paintings, than, sharps, considerably, ellefson, arkham, Average loss at step 62000: 3.246356 Average loss at step 64000: 3.253968 Average loss at step 66000: 3.403710 Average loss at step 68000: 3.393377 Average loss at step 70000: 3.360259 Nearest to is: was, has, are, be, mjf, although, myrrh, became, Nearest to s: haram, diff, decrypted, madman, bey, ztas, isbn, whose, Nearest to who: he, which, never, they, married, she, still, manchu, Nearest to time: unsympathetic, way, comprehensively, maligned, year, place, season, shays, Nearest to four: six, five, eight, seven, three, nine, two, zero, Nearest to or: and, than, like, any, while, but, dissection, the, Nearest to has: had, have, is, was, although, having, applicable, borrower, Nearest to about: koan, alcaeus, regarding, lasting, over, mistakenly, docklands, derry, Nearest to such: these, known, many, well, certain, lahore, including, venezia, Nearest to five: six, eight, four, seven, three, nine, zero, two, Nearest to when: if, before, while, after, where, however, though, although, Nearest to there: they, it, still, he, often, usually, we, now, Nearest to use: measure, mengele, form, appear, dominoes, think, lesund, deceive, Nearest to i: we, you, ii, g, doubleday, foregoing, god, t, Nearest to will: would, must, could, may, can, should, might, cannot, Nearest to no: any, little, than, considerably, paintings, there, flagpole, flip, Average loss at step 72000: 3.373728 Average loss at step 74000: 3.351606 Average loss at step 76000: 3.323821 Average loss at step 78000: 3.353756 Average loss at step 80000: 3.377470 Nearest to is: was, has, are, becomes, although, be, became, includes, Nearest to s: haram, ztas, fealty, bey, dtp, justly, isbn, whose, Nearest to who: he, never, she, married, which, they, often, manchu, Nearest to time: unsympathetic, year, maligned, season, way, khanate, boas, period, Nearest to four: five, six, seven, eight, three, nine, two, zero, Nearest to or: and, while, per, than, dissection, van, restrain, pekka, Nearest to has: had, have, is, was, having, although, since, borrower, Nearest to about: alcaeus, koan, derry, indulging, lasting, regarding, over, docklands, Nearest to such: well, these, known, follows, certain, including, opposed, conifers, Nearest to five: six, four, seven, eight, three, nine, zero, two, Nearest to when: before, if, after, though, during, although, while, where, Nearest to there: it, they, he, usually, she, often, still, we, Nearest to use: measure, form, think, mengele, cause, dtv, treatment, analysing, Nearest to i: ii, you, we, iii, they, t, foregoing, doubleday, Nearest to will: would, could, can, must, may, should, might, cannot, Nearest to no: any, little, teng, flagpole, she, prespa, arkham, ellefson, Average loss at step 82000: 3.409646 Average loss at step 84000: 3.412236 Average loss at step 86000: 3.390080 Average loss at step 88000: 3.352896 Average loss at step 90000: 3.365889 Nearest to is: was, has, are, be, although, gaylord, becomes, myrrh, Nearest to s: his, whose, isbn, haram, bey, briand, isaac, barclays, Nearest to who: he, often, she, never, also, which, married, then, Nearest to time: unsympathetic, season, year, period, maligned, way, length, sort, Nearest to four: five, seven, six, eight, three, nine, two, one, Nearest to or: and, email, affreightment, taney, wideawake, pensacola, while, dissection, Nearest to has: had, have, is, was, having, since, although, bu, Nearest to about: koan, alcaeus, regarding, over, derry, docklands, indulging, on, Nearest to such: known, well, these, certain, follows, described, separate, many, Nearest to five: eight, seven, four, three, six, nine, zero, two, Nearest to when: if, before, while, after, although, where, until, though, Nearest to there: they, it, he, still, she, usually, we, now, Nearest to use: treatment, list, cause, measure, most, analysing, notion, many, Nearest to i: we, you, ii, iii, doubleday, g, t, minute, Nearest to will: would, could, can, must, may, should, might, cannot, Nearest to no: any, little, only, than, another, distinguishable, considerably, she, Average loss at step 92000: 3.398345 Average loss at step 94000: 3.252411 Average loss at step 96000: 3.359638 Average loss at step 98000: 3.247843 Average loss at step 100000: 3.358810 Nearest to is: was, has, becomes, became, be, although, gaylord, myrrh, Nearest to s: whose, his, haram, lebeau, wields, bey, isbn, isaac, Nearest to who: he, never, she, actually, often, and, which, still, Nearest to time: way, unsympathetic, step, khanate, year, maligned, season, sort, Nearest to four: six, seven, five, eight, three, two, nine, zero, Nearest to or: and, than, per, while, geq, merck, influencing, pekka, Nearest to has: had, have, is, was, since, having, borrower, kidney, Nearest to about: regarding, alcaeus, koan, indulging, derry, over, docklands, vigorous, Nearest to such: known, well, these, separate, certain, follows, many, including, Nearest to five: four, seven, six, eight, three, zero, nine, two, Nearest to when: if, while, before, although, where, after, though, until, Nearest to there: they, it, he, still, now, we, usually, sometimes, Nearest to use: treatment, measure, form, mengele, maximally, principles, rounding, cause, Nearest to i: you, we, ii, they, doubleday, iii, t, schaff, Nearest to will: would, must, can, could, should, may, might, cannot, Nearest to no: little, any, concerned, profoundly, steamships, considerably, xviii, she, 12345# 抽取400点到TSNE 来可视化num_points = 400tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')two_d_embeddings = tsne.fit_transform(final_embeddings[1:num_points+1, :]) 12345678910111213# 画出400个点的二维向量def plot(embeddings, labels): assert embeddings.shape[0] &gt;= len(labels), 'More labels than embeddings' pylab.figure(figsize=(15,15)) # in inches for i, label in enumerate(labels): x, y = embeddings[i,:] pylab.scatter(x, y) pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom') pylab.show()words = [reverse_dictionary[i] for i in range(1, num_points+1)]plot(two_d_embeddings, words)","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"RNN","slug":"深度学习/RNN","permalink":"http://yoursite.com/categories/深度学习/RNN/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"skip-gram","slug":"skip-gram","permalink":"http://yoursite.com/tags/skip-gram/"},{"name":"RNN","slug":"RNN","permalink":"http://yoursite.com/tags/RNN/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"RNN","slug":"深度学习/RNN","permalink":"http://yoursite.com/categories/深度学习/RNN/"}]},{"title":"文本和序列的深度模型01","slug":"文本和序列的深度模型01","date":"2018-06-10T12:19:32.000Z","updated":"2018-09-07T03:09:23.621Z","comments":true,"path":"2018/06/10/文本和序列的深度模型01/","link":"","permalink":"http://yoursite.com/2018/06/10/文本和序列的深度模型01/","excerpt":"","text":"要点我们看深度学习如何处理文本的 1. 例如文本分类的问题，判断一个文本是属于娱乐，体育，游戏，汽车，财经？这个问题估计今日头条会遇到吧，新华字典现已经收录20959个汉字、52万个词语，有些我们可能这辈子都用不到；但是对于文本分类来说，极少用到的词，才是重要的，常见词“我你他是”没啥用；例如“二十一三体综合症”，这个可能是篇医学文章，所以我们需要很多的训练样本 2.为什么要把词语转化成向量呢？我们有很多的词是近义词，虽然形式上有区别，但是意思是一样的，要知道词与词之间的关系 要找出词语的内在含义，并且有一个统一的数值化的标准来表示词语的内在含义 3.为什么采用非监督学习学习词向量？标记这些海量的文本似乎是不现实的，所以考虑采用非监督学习，用的核心理论就是 相似的词出现在相似的上下文中；这个既简单又实用 4.词嵌入的过程？词嵌入的过程就是寻找词之间内在关系的一种过程，例如52万个词语，那岂不是一个词要用52万维向量来表示，那太夸张了，其实词嵌入也是降维的过程，把词语一个个嵌入到一个低维空间中，在这个低维空间中，越靠近的点（词），意思越相近；通过无监督学习，你并不需要了解这些词语的实际含义，通过相似的词出现在相似的上下文中可以让机器学习到词语的内在含义，前提是文本量充足 5.经典模型word2vec？从字面上看就知道它是干啥的了，word转化成vec; 这个模型极其简单却效果极佳，只做了简单线性分类（逻辑回归） 把句子的词语嵌入到词向量空间，（刚开始是随机初始化的） 在句子中随机选一个词，选定这个词附近的窗口大小（上下文） 通过嵌入后的词向量预测该词语的上下文（上下文就是窗口中的词），词语的相似性通过余弦相似性度量，欧氏距离强调每个维度的差异，余弦相似性既可以度量近义词，也可以度量反义词，有归一化操作在里面； 负采样：在计算softmax时，ont-hot 向量52万维，太大了，所以对非目标词进行采样，就是选取一小部分变成一个小的one-hot 向量，大大加速了word2vec的softmax 计算 123456 graph LRInput[单词] --&gt; Embedding[词嵌入]Embedding --&gt; LinerModel[线性模型 wx+b]LinerModel--&gt; softmaxsoftmax--&gt;sampling[负采样]sampling --&gt; cross[与上下文单词交叉熵计算] 6.T-SNE在word2vec起到什么作用?你怎么判断你的词嵌入已经效果很好了呢，两种思路： 第一种，最近邻，找出某个词的最近邻，看看是不是其近义词 第二种，降维到2维空间，但是PCA 会损失很多关键信息，建议用T-SNE 如果向量空间维数过高，200维以上，或者更高，那么这里建议先使用PCA降维方法，降到50维左右以后，再使用t-SNE来继续降到二维或者三维。因为直接用t-SNE给高维向量空间做降维的话，效率会比较低 参考文档 7.什么是词嵌入后的类比属性？词语经过嵌入后，每一个单词都可以表示成一个向量，比如“漂亮” 的近义词 “美丽”，他们的词嵌入向量是非常接近的，余弦相似度可能接近1；而“高大” 与 “矮小” 余弦相似度可能接近-1 更加有趣的是： “猫咪” - “猫” +“狗” = “狗” “更高” -“高” + “强” = “更强”","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"RNN","slug":"深度学习/RNN","permalink":"http://yoursite.com/categories/深度学习/RNN/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"RNN","slug":"RNN","permalink":"http://yoursite.com/tags/RNN/"},{"name":"序列模型","slug":"序列模型","permalink":"http://yoursite.com/tags/序列模型/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"RNN","slug":"深度学习/RNN","permalink":"http://yoursite.com/categories/深度学习/RNN/"}]},{"title":"卷积神经网络的TensorFlow实践","slug":"卷积神经网络的TensorFlow实践","date":"2018-05-10T12:19:32.000Z","updated":"2018-09-05T06:11:33.873Z","comments":true,"path":"2018/05/10/卷积神经网络的TensorFlow实践/","link":"","permalink":"http://yoursite.com/2018/05/10/卷积神经网络的TensorFlow实践/","excerpt":"","text":"要点 本文主要示范如何使用神经网络的卷积 如何使用卷积核以及conv2d函数以及padding设置，stride设置 我们这次用了两个卷积层，和一个全连接层 这里我要说下的是padding same:填充，比如我的图像是[16,28,28,3] stride[1,2,2,1] filter[3,3,3,8] 那么结果是 【16,28/2,28/2，8】==》【16,14,14,8】保证了stride的缩放肯定是成比例的，stride的宽度步长是2，那么宽度的采样后就是28/2 valid 不填充，比如我的图像是[16,28,28,3] stride[1,2,2,1] filter[3,3,3,8] ，那么结果就是【16,25/3+1,25/3+1,8】==》【16,13,13,8】 看到没有，宽度变小了，不是原来的一半 函数tf.nn.conv2d12345678910tf.nn.conv2d( input, filter, strides, padding, use_cudnn_on_gpu=True, data_format='NHWC', dilations=[1, 1, 1, 1], name=None) Args Annotation 第一个参数input 指需要做卷积的输入图像，它要求是一个Tensor，具有[batch, in_height, in_width, in_channels]这样的shape，具体含义是[训练时一个batch的图片数量, 图片高度, 图片宽度, 图像通道数]，注意这是一个4维的Tensor，要求类型为float32和float64其中之一 第二个参数filter 相当于CNN中的卷积核，它要求是一个Tensor，具有[filter_height, filter_width, in_channels, out_channels]这样的shape，具体含义是[卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]，有一个地方需要注意，第三维in_channels，就是参数input的第四维 第三个参数strides 卷积时在图像每一维的步长，这是一个一维的向量，长度4，和input 是一一对应的；一般我都设置为【1,2,2,1】或者【1,1,1,1】； 第四个参数padding string类型的量，只能是”SAME”,”VALID”其中之一，这个值决定了不同的卷积方式 第五个参数 use_cudnn_on_gpu:bool类型，是否使用cudnn加速，默认为true 结果返回： 一个Tensor，这个输出，就是我们常说的feature map 作者：Niling 链接：https://www.jianshu.com/p/510bb4bc590f 來源：简书 简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。 tf.nn.max_pool()参数 value：需要池化的输入，一般池化层接在卷积层后面，所以输入通常是feature map，依然是[batch, height, width, channels]这样的shape ksize：池化窗口的大小，取一个四维向量，一般是[1, height, width, 1]，因为我们不想在batch和channels上做池化，所以这两个维度设为了1 strides：和卷积类似，窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1] padding：和卷积类似，可以取’VALID’ 或者’SAME’ use_cudnn_on_gpu：bool类型，是否使用cudnn加速，默认为true name：指定该操作的name 返回返回一个Tensor，类型不变，shape仍然是[batch, height, width, channels]这种形式 作者：我是谁的小超人 链接：https://www.jianshu.com/p/1d73fd1a256e 來源：简书 简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。 代码1234567# These are all the modules we'll be using later. Make sure you can import them# before proceeding further.from __future__ import print_functionimport numpy as npimport tensorflow as tffrom six.moves import cPickle as picklefrom six.moves import range C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters 1234567891011121314pickle_file = 'notMNIST.pickle'with open(pickle_file, 'rb') as f: save = pickle.load(f) train_dataset = save['train_dataset'] train_labels = save['train_labels'] valid_dataset = save['valid_dataset'] valid_labels = save['valid_labels'] test_dataset = save['test_dataset'] test_labels = save['test_labels'] del save # hint to help gc free up memory print('Training set', train_dataset.shape, train_labels.shape) print('Validation set', valid_dataset.shape, valid_labels.shape) print('Test set', test_dataset.shape, test_labels.shape) Training set (20000, 28, 28) (20000,) Validation set (1000, 28, 28) (1000,) Test set (1000, 28, 28) (1000,) 1234567891011121314151617181920# 这里我们把数据转化成 图像立方体：（宽，高，色道）# 把标签转化成one-hot 向量image_size = 28num_labels = 10num_channels = 1 # grayscaleimport numpy as npdef reformat(dataset, labels): dataset = dataset.reshape( (-1, image_size, image_size, num_channels)).astype(np.float32) # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...] labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32) return dataset, labelstrain_dataset, train_labels = reformat(train_dataset, train_labels)valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)test_dataset, test_labels = reformat(test_dataset, test_labels)print('Training set', train_dataset.shape, train_labels.shape)print('Validation set', valid_dataset.shape, valid_labels.shape)print('Test set', test_dataset.shape, test_labels.shape) Training set (20000, 28, 28, 1) (20000, 10) Validation set (1000, 28, 28, 1) (1000, 10) Test set (1000, 28, 28, 1) (1000, 10) 123def accuracy(predictions, labels): return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0]) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# 构建一个由两层卷积层以及一个完全层组成的神经网络# 卷积层计算代价高，所以要设置好卷积层的深度和宽度batch_size = 16patch_size = 5depth = 16num_hidden = 64graph = tf.Graph()with graph.as_default(): # Input data. tf_train_dataset = tf.placeholder( tf.float32, shape=(batch_size, image_size, image_size, num_channels)) tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels)) tf_valid_dataset = tf.constant(valid_dataset) tf_test_dataset = tf.constant(test_dataset) # Variables. # 第一层我们用5*5*1的过滤器，总共depth 个，就是说一共有16个过滤器 # 这里问个问题？为什么要设置方差是0.1 layer1_weights = tf.Variable(tf.truncated_normal( [patch_size, patch_size, num_channels, depth], stddev=0.1)) layer1_biases = tf.Variable(tf.zeros([depth])) # 第一层我们用5*5*16的过滤器，总共depth 个，就是说一共有16个过滤器 layer2_weights = tf.Variable(tf.truncated_normal( [patch_size, patch_size, depth, depth], stddev=0.1)) # 偏移量都是1 layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth])) # 第三层全连接层，产生一个64的节点的层 layer3_weights = tf.Variable(tf.truncated_normal( [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1)) layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden])) # 第四层全连接层，64个节点的层 layer4_weights = tf.Variable(tf.truncated_normal( [num_hidden, num_labels], stddev=0.1)) layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels])) # Model. 定义模型计算 def model(data): # 这个conv2d常用函数，详见我的函数说明 conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME') hidden = tf.nn.relu(conv + layer1_biases) conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME') hidden = tf.nn.relu(conv + layer2_biases) # 拿到shape,然后把shape 值变成list，方便后面reshape shape = hidden.get_shape().as_list() reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]]) hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases) return tf.matmul(hidden, layer4_weights) + layer4_biases # Training computation. logits = model(tf_train_dataset) loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) # Optimizer. optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss) # Predictions for the training, validation, and test data. train_prediction = tf.nn.softmax(logits) valid_prediction = tf.nn.softmax(model(tf_valid_dataset)) test_prediction = tf.nn.softmax(model(tf_test_dataset)) WARNING:tensorflow:From &lt;ipython-input-5-bb43204e2f1e&gt;:55: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version. Instructions for updating: Future major versions of TensorFlow will allow gradients to flow into the labels input on backprop by default. See tf.nn.softmax_cross_entropy_with_logits_v2. 123456789101112131415161718num_steps = 1001with tf.Session(graph=graph) as session: tf.global_variables_initializer().run() print('Initialized') for step in range(num_steps): offset = (step * batch_size) % (train_labels.shape[0] - batch_size) batch_data = train_dataset[offset:(offset + batch_size), :, :, :] batch_labels = train_labels[offset:(offset + batch_size), :] feed_dict = &#123;tf_train_dataset : batch_data, tf_train_labels : batch_labels&#125; _, l, predictions = session.run( [optimizer, loss, train_prediction], feed_dict=feed_dict) if (step % 50 == 0): print('Minibatch loss at step %d: %f' % (step, l)) print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels)) print('Validation accuracy: %.1f%%' % accuracy( valid_prediction.eval(), valid_labels)) print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels)) Initialized Minibatch loss at step 0: 2.871578 Minibatch accuracy: 12.5% Validation accuracy: 11.4% Minibatch loss at step 50: 1.603516 Minibatch accuracy: 37.5% Validation accuracy: 55.2% Minibatch loss at step 100: 1.132913 Minibatch accuracy: 62.5% Validation accuracy: 74.9% Minibatch loss at step 150: 0.959747 Minibatch accuracy: 68.8% Validation accuracy: 75.4% Minibatch loss at step 200: 0.925898 Minibatch accuracy: 68.8% Validation accuracy: 78.4% Minibatch loss at step 250: 0.961354 Minibatch accuracy: 75.0% Validation accuracy: 76.4% Minibatch loss at step 300: 0.885228 Minibatch accuracy: 68.8% Validation accuracy: 79.8% Minibatch loss at step 350: 0.329915 Minibatch accuracy: 87.5% Validation accuracy: 80.0% Minibatch loss at step 400: 0.525234 Minibatch accuracy: 93.8% Validation accuracy: 80.2% Minibatch loss at step 450: 0.728237 Minibatch accuracy: 75.0% Validation accuracy: 81.3% Minibatch loss at step 500: 0.781117 Minibatch accuracy: 75.0% Validation accuracy: 80.7% Minibatch loss at step 550: 0.725993 Minibatch accuracy: 75.0% Validation accuracy: 81.2% Minibatch loss at step 600: 0.588381 Minibatch accuracy: 81.2% Validation accuracy: 80.9% Minibatch loss at step 650: 0.313426 Minibatch accuracy: 87.5% Validation accuracy: 81.9% Minibatch loss at step 700: 0.076320 Minibatch accuracy: 100.0% Validation accuracy: 83.8% Minibatch loss at step 750: 0.580707 Minibatch accuracy: 87.5% Validation accuracy: 83.4% Minibatch loss at step 800: 0.641274 Minibatch accuracy: 75.0% Validation accuracy: 82.4% Minibatch loss at step 850: 0.665619 Minibatch accuracy: 75.0% Validation accuracy: 83.4% Minibatch loss at step 900: 0.026958 Minibatch accuracy: 100.0% Validation accuracy: 83.8% Minibatch loss at step 950: 0.789471 Minibatch accuracy: 81.2% Validation accuracy: 83.2% Minibatch loss at step 1000: 0.559735 Minibatch accuracy: 87.5% Validation accuracy: 83.9% Test accuracy: 89.4% 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 构建一个由两层卷积层以及一个完全层组成的神经网络# 卷积层计算代价高，所以要设置好卷积层的深度和宽度batch_size = 16patch_size = 5depth = 16num_hidden = 64graph = tf.Graph()with graph.as_default(): # Input data. tf_train_dataset = tf.placeholder( tf.float32, shape=(batch_size, image_size, image_size, num_channels)) tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels)) tf_valid_dataset = tf.constant(valid_dataset) tf_test_dataset = tf.constant(test_dataset) # Variables. # 第一层我们用5*5*1的过滤器，总共depth 个，就是说一共有16个过滤器 # 这里问个问题？为什么要设置方差是0.1 layer1_weights = tf.Variable(tf.truncated_normal( [patch_size, patch_size, num_channels, depth], stddev=0.1)) layer1_biases = tf.Variable(tf.zeros([depth])) # 第一层我们用5*5*16的过滤器，总共depth 个，就是说一共有16个过滤器 layer2_weights = tf.Variable(tf.truncated_normal( [patch_size, patch_size, depth, depth], stddev=0.1)) # 偏移量都是1 layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth])) # 第三层全连接层，产生一个64的节点的层 layer3_weights = tf.Variable(tf.truncated_normal( [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1)) layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden])) # 第四层全连接层，64个节点的层 layer4_weights = tf.Variable(tf.truncated_normal( [num_hidden, num_labels], stddev=0.1)) layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels])) # Model. 定义模型计算 def model(data): # 这个conv2d常用函数，详见我的函数说明 conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME') hidden = tf.nn.relu(conv + layer1_biases) print(hidden.get_shape) # conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME') # hidden = tf.nn.relu(conv + layer2_biases) # 添加了池化层 hidden = tf.nn.max_pool(hidden,[1,2,2,1],[1,2,2,1],padding='SAME') print(hidden.get_shape) # 拿到shape,然后把shape 值变成list，方便后面reshape shape = hidden.get_shape().as_list() reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]]) print(reshape.get_shape) hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases) return tf.matmul(hidden, layer4_weights) + layer4_biases # Training computation. logits = model(tf_train_dataset) loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) # Optimizer. optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss) # Predictions for the training, validation, and test data. train_prediction = tf.nn.softmax(logits) valid_prediction = tf.nn.softmax(model(tf_valid_dataset)) test_prediction = tf.nn.softmax(model(tf_test_dataset)) &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;Relu:0&#39; shape=(16, 14, 14, 16) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;MaxPool:0&#39; shape=(16, 7, 7, 16) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;Reshape:0&#39; shape=(16, 784) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;Relu_2:0&#39; shape=(1000, 14, 14, 16) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;MaxPool_1:0&#39; shape=(1000, 7, 7, 16) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;Reshape_1:0&#39; shape=(1000, 784) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;Relu_4:0&#39; shape=(1000, 14, 14, 16) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;MaxPool_2:0&#39; shape=(1000, 7, 7, 16) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;Reshape_2:0&#39; shape=(1000, 784) dtype=float32&gt;&gt; 12345678910111213141516171819num_steps = 1001with tf.Session(graph=graph) as session: tf.global_variables_initializer().run() print('Initialized') for step in range(num_steps): offset = (step * batch_size) % (train_labels.shape[0] - batch_size) batch_data = train_dataset[offset:(offset + batch_size), :, :, :] batch_labels = train_labels[offset:(offset + batch_size), :] feed_dict = &#123;tf_train_dataset : batch_data, tf_train_labels : batch_labels&#125; _, l, predictions = session.run( [optimizer, loss, train_prediction], feed_dict=feed_dict) if (step % 50 == 0): print('Minibatch loss at step %d: %f' % (step, l)) print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels)) print('Validation accuracy: %.1f%%' % accuracy( valid_prediction.eval(), valid_labels)) print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels)) Initialized Minibatch loss at step 0: 2.481559 Minibatch accuracy: 18.8% Validation accuracy: 9.8% Minibatch loss at step 50: 1.541600 Minibatch accuracy: 50.0% Validation accuracy: 51.1% Minibatch loss at step 100: 0.922436 Minibatch accuracy: 75.0% Validation accuracy: 75.2% Minibatch loss at step 150: 0.821155 Minibatch accuracy: 75.0% Validation accuracy: 79.4% Minibatch loss at step 200: 0.842788 Minibatch accuracy: 75.0% Validation accuracy: 79.3% Minibatch loss at step 250: 1.074179 Minibatch accuracy: 62.5% Validation accuracy: 79.5% Minibatch loss at step 300: 0.586223 Minibatch accuracy: 75.0% Validation accuracy: 80.2% Minibatch loss at step 350: 0.439200 Minibatch accuracy: 93.8% Validation accuracy: 80.4% Minibatch loss at step 400: 0.518278 Minibatch accuracy: 93.8% Validation accuracy: 81.0% Minibatch loss at step 450: 0.722640 Minibatch accuracy: 81.2% Validation accuracy: 81.7% Minibatch loss at step 500: 0.790449 Minibatch accuracy: 75.0% Validation accuracy: 81.8% Minibatch loss at step 550: 0.557499 Minibatch accuracy: 75.0% Validation accuracy: 80.9% Minibatch loss at step 600: 0.579446 Minibatch accuracy: 81.2% Validation accuracy: 81.9% Minibatch loss at step 650: 0.408497 Minibatch accuracy: 87.5% Validation accuracy: 82.1% Minibatch loss at step 700: 0.196559 Minibatch accuracy: 87.5% Validation accuracy: 82.5% Minibatch loss at step 750: 0.745032 Minibatch accuracy: 87.5% Validation accuracy: 83.3% Minibatch loss at step 800: 0.657614 Minibatch accuracy: 81.2% Validation accuracy: 82.0% Minibatch loss at step 850: 0.823133 Minibatch accuracy: 62.5% Validation accuracy: 82.5% Minibatch loss at step 900: 0.030666 Minibatch accuracy: 100.0% Validation accuracy: 83.5% Minibatch loss at step 950: 0.385080 Minibatch accuracy: 87.5% Validation accuracy: 82.5% Minibatch loss at step 1000: 0.597917 Minibatch accuracy: 87.5% Validation accuracy: 83.8% Test accuracy: 89.9% 12# 任务二：调试卷积网络，使其达到最佳效果# 可以参考LENET-5;添加dropout 还有学习速率衰减来试试 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293batch_size = 50patch_size = 5depth = 16num_hidden = 64graph = tf.Graph()with graph.as_default(): # Input data. tf_train_dataset = tf.placeholder( tf.float32, shape=(batch_size, image_size, image_size, num_channels)) tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels)) tf_valid_dataset = tf.constant(valid_dataset) tf_test_dataset = tf.constant(test_dataset) # Variables. # 第一层我们用5*5*1的过滤器，总共6 个，步长1==》[16,28,28,6] layer1_weights = tf.Variable(tf.truncated_normal( [5, 5, num_channels, 6], stddev=0.1)) layer1_biases = tf.Variable(tf.zeros([6])) # 第二层我们用2*2的最大池化，步长是2==》【16,14,14,6】 # 第三层卷积层【5,5】的过滤器16个，步长是1，valid ==》【16,10，10,16】 layer3_weights = tf.Variable(tf.truncated_normal( [5, 5, 6, 16], stddev=0.1)) # 偏移量都是1 layer3_biases = tf.Variable(tf.constant(1.0, shape=[16])) # 第四层最大池化，2*2的，步长2 ===》【16,5,5,16】 # 第五层卷积层 【5,5】的过滤器120个，步长1，valid ===》【16,1,1,120】 layer5_weights = tf.Variable(tf.truncated_normal( [5, 5,16,120], stddev=0.1)) layer5_biases = tf.Variable(tf.constant(1.0, shape=[120])) # 第六层全连接层，输出是84 layer6_weights = tf.Variable(tf.truncated_normal( [120, 84], stddev=0.1)) layer6_biases = tf.Variable(tf.constant(1.0, shape=[84])) # 第七层全连接层--输出层，输出是10 layer7_weights = tf.Variable(tf.truncated_normal( [84, 10], stddev=0.1)) layer7_biases = tf.Variable(tf.constant(1.0, shape=[10])) # Model. 定义模型计算 def model(data): # 这个conv2d常用函数，详见我的函数说明 # 第一层卷积 conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME') hidden = tf.nn.relu(conv + layer1_biases) # conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME') # hidden = tf.nn.relu(conv + layer2_biases) # 添加了池化层 # 第二层池化 print(hidden.get_shape) hidden = tf.nn.max_pool(hidden,[1,2,2,1],[1,2,2,1],padding='SAME') print(hidden.get_shape) # 第三层卷积，加上了dropout # conv = tf.nn.conv2d(tf.nn.dropout(hidden,keep_prob=0.5), layer3_weights, [1, 1, 1, 1], padding='VALID') conv = tf.nn.conv2d(hidden, layer3_weights, [1, 1, 1, 1], padding='VALID') hidden = conv + layer3_biases print(hidden.get_shape) # 第四层最大池化 hidden = tf.nn.max_pool(hidden,[1,2,2,1],[1,2,2,1],padding='SAME') print(hidden.get_shape) # 第五层卷积层 conv = tf.nn.conv2d(hidden, layer5_weights, [1, 1, 1, 1], padding='VALID') hidden = conv + layer5_biases print(hidden.get_shape) # 第六层全连接层 # 拿到shape,然后把shape 值变成list，方便后面reshape shape = hidden.get_shape().as_list() reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]]) print(reshape.get_shape) hidden = tf.nn.sigmoid(tf.matmul(reshape, layer6_weights) + layer6_biases) return tf.matmul(hidden, layer7_weights) + layer7_biases # Training computation. logits = model(tf_train_dataset) loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) # # Optimizer. # optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss) # 优化器采用GD+学习速率衰减 global_step = tf.Variable(0) # count the number of steps taken. learning_rate = tf.train.exponential_decay(0.05, global_step,decay_steps=500,decay_rate=0.96) optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step) # Predictions for the training, validation, and test data. train_prediction = tf.nn.softmax(logits) valid_prediction = tf.nn.softmax(model(tf_valid_dataset)) test_prediction = tf.nn.softmax(model(tf_test_dataset)) &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;Relu:0&#39; shape=(50, 28, 28, 6) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;MaxPool:0&#39; shape=(50, 14, 14, 6) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;add_1:0&#39; shape=(50, 10, 10, 16) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;MaxPool_1:0&#39; shape=(50, 5, 5, 16) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;add_2:0&#39; shape=(50, 1, 1, 120) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;Reshape:0&#39; shape=(50, 120) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;Relu_1:0&#39; shape=(1000, 28, 28, 6) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;MaxPool_2:0&#39; shape=(1000, 14, 14, 6) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;add_6:0&#39; shape=(1000, 10, 10, 16) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;MaxPool_3:0&#39; shape=(1000, 5, 5, 16) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;add_7:0&#39; shape=(1000, 1, 1, 120) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;Reshape_1:0&#39; shape=(1000, 120) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;Relu_2:0&#39; shape=(1000, 28, 28, 6) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;MaxPool_4:0&#39; shape=(1000, 14, 14, 6) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;add_11:0&#39; shape=(1000, 10, 10, 16) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;MaxPool_5:0&#39; shape=(1000, 5, 5, 16) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;add_12:0&#39; shape=(1000, 1, 1, 120) dtype=float32&gt;&gt; &lt;bound method Tensor.get_shape of &lt;tf.Tensor &#39;Reshape_2:0&#39; shape=(1000, 120) dtype=float32&gt;&gt; 12345678910111213141516171819num_steps = 5001with tf.Session(graph=graph) as session: tf.global_variables_initializer().run() print('Initialized') for step in range(num_steps): offset = (step * batch_size) % (train_labels.shape[0] - batch_size) batch_data = train_dataset[offset:(offset + batch_size), :, :, :] batch_labels = train_labels[offset:(offset + batch_size), :] feed_dict = &#123;tf_train_dataset : batch_data, tf_train_labels : batch_labels&#125; _, l, predictions = session.run( [optimizer, loss, train_prediction], feed_dict=feed_dict) if (step % 300 == 0): print('Minibatch loss at step %d: %f' % (step, l)) print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels)) print('Validation accuracy: %.1f%%' % accuracy( valid_prediction.eval(), valid_labels)) print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels)) Initialized Minibatch loss at step 0: 2.309445 Minibatch accuracy: 22.0% Validation accuracy: 10.0% Minibatch loss at step 300: 0.865154 Minibatch accuracy: 78.0% Validation accuracy: 78.5% Minibatch loss at step 600: 1.000986 Minibatch accuracy: 78.0% Validation accuracy: 80.1% Minibatch loss at step 900: 0.359938 Minibatch accuracy: 92.0% Validation accuracy: 82.1% Minibatch loss at step 1200: 0.665972 Minibatch accuracy: 82.0% Validation accuracy: 82.4% Minibatch loss at step 1500: 0.526232 Minibatch accuracy: 84.0% Validation accuracy: 83.3% Minibatch loss at step 1800: 0.601502 Minibatch accuracy: 84.0% Validation accuracy: 84.0% Minibatch loss at step 2100: 0.366273 Minibatch accuracy: 88.0% Validation accuracy: 85.1% Minibatch loss at step 2400: 0.402447 Minibatch accuracy: 88.0% Validation accuracy: 85.2% Minibatch loss at step 2700: 0.464672 Minibatch accuracy: 88.0% Validation accuracy: 85.0% Minibatch loss at step 3000: 0.415956 Minibatch accuracy: 90.0% Validation accuracy: 86.0% Minibatch loss at step 3300: 0.414384 Minibatch accuracy: 86.0% Validation accuracy: 86.8% Minibatch loss at step 3600: 0.384747 Minibatch accuracy: 88.0% Validation accuracy: 86.2% Minibatch loss at step 3900: 0.443196 Minibatch accuracy: 84.0% Validation accuracy: 86.2% Minibatch loss at step 4200: 0.301673 Minibatch accuracy: 92.0% Validation accuracy: 86.4% Minibatch loss at step 4500: 0.493804 Minibatch accuracy: 86.0% Validation accuracy: 87.1% Minibatch loss at step 4800: 0.311406 Minibatch accuracy: 92.0% Validation accuracy: 86.8% Test accuracy: 92.1%","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"卷积神经网络","slug":"深度学习/卷积神经网络","permalink":"http://yoursite.com/categories/深度学习/卷积神经网络/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://yoursite.com/tags/TensorFlow/"},{"name":"池化","slug":"池化","permalink":"http://yoursite.com/tags/池化/"},{"name":"卷积","slug":"卷积","permalink":"http://yoursite.com/tags/卷积/"},{"name":"LENET-5","slug":"LENET-5","permalink":"http://yoursite.com/tags/LENET-5/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"卷积神经网络","slug":"深度学习/卷积神经网络","permalink":"http://yoursite.com/categories/深度学习/卷积神经网络/"}]},{"title":"卷积神经网络","slug":"2018-05-03-卷积神经网络","date":"2018-05-03T12:19:32.000Z","updated":"2018-09-03T07:30:17.209Z","comments":true,"path":"2018/05/03/2018-05-03-卷积神经网络/","link":"","permalink":"http://yoursite.com/2018/05/03/2018-05-03-卷积神经网络/","excerpt":"","text":"要点颜色 思考一个问题，当你在识别手写数字时，数字的颜色重不重要？ 在轮廓识别的时候，颜色信息会耗费神经网络的训练资源，所以采用灰度更好 位置 思考：一张图像上有一只猫，但是猫的位置是变动的，猫的位置改变影响它是只猫的识别么？ 如果猫的位置信息也影响猫的识别，那么神经网络将会耗费大量的计算资源在位置记忆上 好的识别应该具有平移不变性，即使你位置再变，我同样的参数都可以识别你 词语的在句子中的位置 思考：句子中“老苏喜欢吃牛排，牛排还不能太老”；“牛排”在句子中的位置影响“牛排”的意思吗？ 不影响，我们说的话中的词语，大多数时候的意思是一样的，所有的词语要是有统一的权重就好了——权重共享 卷积 卷积的本质就是空间上共享参数的神经网络 神经网络的卷积过程，可以看成是一个金字塔的过程 金字塔的底部是输入图像（RGB） 通过卷积操作，不断压缩图像的维度，增加图像的深度 术语（lingo）池化 池化没有新的权重需要训练，但是有超参数要调整 池化大小 池化步长 操作分类 最大池化：对特征点周围像素进行最大值采样 平均池化：对特征点周围像素进行平均值采样 1*1卷积 卷积操作相当于在图像的一小块上运行了一个线性分类器 如果你在这个线性分类器前加上一个11的卷积，那就变成了一个小的*神经网络分类器了 1*1卷积使神经网络变得更深，更加低耗高效 计算代价较低，可以看成是矩阵相乘 Inception 就是冗余神经网络了，不再考虑每层到底用池化还是卷积，是用1x1的卷积还是3x3的卷积 在1x1的卷积后面把1x1 3x3 5x5 的卷积以及池化统统算一下 我认为就是全面的特征提取 经典的神经网络 LENET-5 1998年YANN LECUN 在字母识别中设计 ALEXNET 2012年赢得了ImageNet的物体识别挑战赛 参考资料卷积神经网络的各层","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"卷积神经网络","slug":"深度学习/卷积神经网络","permalink":"http://yoursite.com/categories/深度学习/卷积神经网络/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"池化","slug":"池化","permalink":"http://yoursite.com/tags/池化/"},{"name":"1x1","slug":"1x1","permalink":"http://yoursite.com/tags/1x1/"},{"name":"卷积","slug":"卷积","permalink":"http://yoursite.com/tags/卷积/"},{"name":"LENET-5","slug":"LENET-5","permalink":"http://yoursite.com/tags/LENET-5/"},{"name":"ALEXNET","slug":"ALEXNET","permalink":"http://yoursite.com/tags/ALEXNET/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"卷积神经网络","slug":"深度学习/卷积神经网络","permalink":"http://yoursite.com/categories/深度学习/卷积神经网络/"}]},{"title":"使用正则化去优化深度学习模型","slug":"使用正则化去优化深度学习模型","date":"2018-05-01T12:19:32.000Z","updated":"2018-09-04T07:03:37.536Z","comments":true,"path":"2018/05/01/使用正则化去优化深度学习模型/","link":"","permalink":"http://yoursite.com/2018/05/01/使用正则化去优化深度学习模型/","excerpt":"","text":"要点 了解正则化对模型的影响 训练数据采用notMNIST 采用L2 正则化 采用Dropout随机失活 采用学习速率衰减 我在增加神经网络层时遇到了梯度消失问题，就是loss 值算出来是nan;我有一种实用但是不能彻底解决梯度消失问题的策略； 首先RELU 可以帮助防止梯度消失 其次就是权重的初始化很重要，梯度消失是多个小于1的数相乘，所以如何避免呢，就是初始化时，假设前面一层有n个节点，那么我会这样初始化： relu : stddev=np.sqrt(\\frac{2.0}{n} ) \\\\ sigmod : stddev=np.sqrt(\\frac{1.0}{n} ) \\\\ tanh: stddev=np.sqrt(\\frac{1.0}{n} ） 函数 tf.nn.dropout 按照keep_prob失活神经元； 这种操作类似于缩放，但是记住缩放后，元素的和是不变的，所以不需要再手动乘上倍数 12345678910111213141516tf.nn.dropout( x, keep_prob, noise_shape=None, seed=None, name=None)Args:x: A floating point tensor.keep_prob: A scalar Tensor with the same type as x. The probability that each element is kept.noise_shape: A 1-D Tensor of type int32, representing the shape for randomly generated keep/drop flags.seed: A Python integer. Used to create random seeds. See tf.set_random_seed for behavior.name: A name for this operation (optional).Returns:A Tensor of the same shape of x. tf.train.exponential_decay 这个就是专门用来对学习速率进行衰减的 一句话概括这个函数，每decay_steps（步）就以decay_rate（率）乘以learning_rate 计算公式如下 12decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps) 123456789101112131415161718192021tf.train.exponential_decay( learning_rate, global_step, decay_steps, decay_rate, staircase=False, name=None)Args:learning_rate: A scalar float32 or float64 Tensor or a Python number. The initial learning rate.global_step: A scalar int32 or int64 Tensor or a Python number. Global step to use for the decay computation. Must not be negative.decay_steps: A scalar int32 or int64 Tensor or a Python number. Must be positive. See the decay computation above.decay_rate: A scalar float32 or float64 Tensor or a Python number. The decay rate.staircase: Boolean. If True decay the learning rate at discrete intervalsname: String. Optional name of the operation. Defaults to 'ExponentialDecay'.Returns:A scalar Tensor of the same type as learning_rate. The decayed learning rate.Raises:ValueError: if global_step is not supplied. 代码123456# These are all the modules we'll be using later. Make sure you can import them# before proceeding further.from __future__ import print_functionimport numpy as npimport tensorflow as tffrom six.moves import cPickle as pickle C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters 1234567891011121314pickle_file = 'notMNIST.pickle'with open(pickle_file, 'rb') as f: save = pickle.load(f) train_dataset = save['train_dataset'] train_labels = save['train_labels'] valid_dataset = save['valid_dataset'] valid_labels = save['valid_labels'] test_dataset = save['test_dataset'] test_labels = save['test_labels'] del save # hint to help gc free up memory print('Training set', train_dataset.shape, train_labels.shape) print('Validation set', valid_dataset.shape, valid_labels.shape) print('Test set', test_dataset.shape, test_labels.shape) Training set (20000, 28, 28) (20000,) Validation set (1000, 28, 28) (1000,) Test set (1000, 28, 28) (1000,) 12345678910111213141516# 把3维的训练数据转化为2维，即把28*28的图像像素矩阵转化成一维向量的转置 # 把标签转化为one-hot 向量image_size = 28num_labels = 10def reformat(dataset, labels): dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32) # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...] labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32) return dataset, labelstrain_dataset, train_labels = reformat(train_dataset, train_labels)valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)test_dataset, test_labels = reformat(test_dataset, test_labels)print('Training set', train_dataset.shape, train_labels.shape)print('Validation set', valid_dataset.shape, valid_labels.shape)print('Test set', test_dataset.shape, test_labels.shape) Training set (20000, 784) (20000, 10) Validation set (1000, 784) (1000, 10) Test set (1000, 784) (1000, 10) 1234567def accuracy(predictions, labels): # argmax axis = 0的时候返回每一列最大值的位置索引 # axis = 1的时候返回每一行最大值的位置索引 # axis = 2、3、4...，即为多维张量时，同理推断 # 得到true，fasle 的向量，然后统计1的个数，就是正确率 return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0]) 123import timelocaltime = time.asctime( time.localtime(time.time()) )print(localtime) Tue Sep 4 09:08:04 2018 1234567891011121314151617181920212223242526272829303132333435363738394041424344# 任务1，利用nn.l2_loss(t)来训练模型，包含了L2正则化# TODO:将全连接层转化成1024节点，并且有relu的隐藏层# 图的初始化操作batch_size = 128graph = tf.Graph()with graph.as_default(): # 输入数据，对于训练数据，我们使用一个占位符 # 在运行时，用一个批次 tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size)) tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels)) tf_valid_dataset = tf.constant(valid_dataset) tf_test_dataset = tf.constant(test_dataset) # 初始化变量 weights = tf.Variable( tf.truncated_normal([image_size * image_size, 1024])) biases = tf.Variable(tf.zeros([1024])) # 训练计算 hidden1 = tf.matmul(tf_train_dataset, weights) + biases hidden1 = tf.nn.relu(hidden1) weights1 = tf.Variable( tf.truncated_normal([1024, num_labels])) biases1 = tf.Variable(tf.zeros([num_labels])) logits = tf.matmul(hidden1, weights1) + biases1 loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))+\\ 1e-3 *(tf.nn.l2_loss(weights)+tf.nn.l2_loss(weights1)) # 优化器还是 optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss) # 计算预测值,这里的定义直接可以通过.evl()来调用 train_prediction = tf.nn.softmax(logits) valid_prediction = tf.nn.softmax( tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases),weights1) + biases1) test_prediction = tf.nn.softmax( tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases),weights1) + biases1) 12345678910111213141516171819202122232425262728num_steps = 3001with tf.Session(graph=graph) as session: tf.global_variables_initializer().run() print(\"Initialized\") starttime = time.clock() print('start time:',time.asctime( time.localtime(time.time()) )) for step in range(num_steps): # 用偏移量来选择数据，10一批次， 101个数据，当取到12批次，offset = 110%101 = 9,可以保证批次有效 # Note:我们可以用更好的随机策略选择批次 offset = (step * batch_size) % (train_labels.shape[0] - batch_size) # 创建一个批次. batch_data = train_dataset[offset:(offset + batch_size), :] batch_labels = train_labels[offset:(offset + batch_size), :] # 准备一个字典，来存放批数据，以便于 placeholder 来放数据 # 数据类型 numpy 的数组 feed_dict = &#123;tf_train_dataset : batch_data, tf_train_labels : batch_labels&#125; _, l, predictions = session.run( [optimizer, loss, train_prediction], feed_dict=feed_dict) if (step % 500 == 0): print(\"Minibatch loss at step %d: %f\" % (step, l)) print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)) print(\"Validation accuracy: %.1f%%\" % accuracy( valid_prediction.eval(), valid_labels)) print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels)) print('end time:',time.asctime( time.localtime(time.time()) )) elapsed = (time.clock() - starttime) print(\"Time used:\",elapsed) Initialized start time: Tue Sep 4 09:45:29 2018 Minibatch loss at step 0: 677.508362 Minibatch accuracy: 6.2% Validation accuracy: 10.6% Test accuracy: 11.5% Minibatch loss at step 500: 342.153320 Minibatch accuracy: 76.6% Validation accuracy: 74.5% Test accuracy: 82.2% Minibatch loss at step 1000: 336.932465 Minibatch accuracy: 75.0% Validation accuracy: 74.6% Test accuracy: 83.3% Minibatch loss at step 1500: 315.919678 Minibatch accuracy: 84.4% Validation accuracy: 75.6% Test accuracy: 84.2% Minibatch loss at step 2000: 310.289490 Minibatch accuracy: 85.9% Validation accuracy: 75.8% Test accuracy: 84.5% Minibatch loss at step 2500: 308.458069 Minibatch accuracy: 85.2% Validation accuracy: 76.7% Test accuracy: 84.8% Minibatch loss at step 3000: 304.995117 Minibatch accuracy: 84.4% Validation accuracy: 76.5% Test accuracy: 84.7% end time: Tue Sep 4 09:46:01 2018 Time used: 31.499022337961833 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475# TODO:制作过拟合情况,通过减少训练批次的大小# 下面训练随机梯度下降# 将数据保存到常量节点，创建一个占位节点,每次用数据代入占位节点# 图的初始化操作batch_size = 50graph = tf.Graph()with graph.as_default(): # 输入数据，对于训练数据，我们使用一个占位符 # 在运行时，用一个批次 tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size)) tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels)) tf_valid_dataset = tf.constant(valid_dataset) tf_test_dataset = tf.constant(test_dataset) # 初始化变量 weights = tf.Variable( tf.truncated_normal([image_size * image_size, 1024])) biases = tf.Variable(tf.zeros([1024])) # 训练计算 hidden1 = tf.matmul(tf_train_dataset, weights) + biases hidden1 = tf.nn.relu(hidden1) weights1 = tf.Variable( tf.truncated_normal([1024, num_labels])) biases1 = tf.Variable(tf.zeros([num_labels])) logits = tf.matmul(hidden1, weights1) + biases1 loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))+\\ 0 *(tf.nn.l2_loss(weights)+tf.nn.l2_loss(weights1)) # 优化器还是 optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss) # 计算预测值,这里的定义直接可以通过.evl()来调用 train_prediction = tf.nn.softmax(logits) valid_prediction = tf.nn.softmax( tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases),weights1) + biases1) test_prediction = tf.nn.softmax( tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases),weights1) + biases1)num_steps = 300with tf.Session(graph=graph) as session: tf.global_variables_initializer().run() print(\"Initialized\") starttime = time.clock() print('start time:',time.asctime( time.localtime(time.time()) )) for step in range(num_steps): # 用偏移量来选择数据，10一批次， 101个数据，当取到12批次，offset = 110%101 = 9,可以保证批次有效 # Note:我们可以用更好的随机策略选择批次 offset = (step * batch_size) % (train_labels.shape[0] - batch_size) # 创建一个批次. batch_data = train_dataset[offset:(offset + batch_size), :] batch_labels = train_labels[offset:(offset + batch_size), :] # 准备一个字典，来存放批数据，以便于 placeholder 来放数据 # 数据类型 numpy 的数组 feed_dict = &#123;tf_train_dataset : batch_data, tf_train_labels : batch_labels&#125; _, l, predictions = session.run( [optimizer, loss, train_prediction], feed_dict=feed_dict) if (step % 30 == 0): print(\"Minibatch loss at step %d: %f\" % (step, l)) print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)) print(\"Validation accuracy: %.1f%%\" % accuracy( valid_prediction.eval(), valid_labels)) print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels)) print('end time:',time.asctime( time.localtime(time.time()) )) elapsed = (time.clock() - starttime) print(\"Time used:\",elapsed) Initialized start time: Tue Sep 4 09:51:58 2018 Minibatch loss at step 0: 301.258759 Minibatch accuracy: 4.0% Validation accuracy: 21.8% Test accuracy: 22.0% Minibatch loss at step 30: 135.621140 Minibatch accuracy: 68.0% Validation accuracy: 75.7% Test accuracy: 80.1% Minibatch loss at step 60: 62.916607 Minibatch accuracy: 80.0% Validation accuracy: 75.6% Test accuracy: 80.8% Minibatch loss at step 90: 75.922493 Minibatch accuracy: 68.0% Validation accuracy: 72.6% Test accuracy: 77.4% Minibatch loss at step 120: 126.814140 Minibatch accuracy: 70.0% Validation accuracy: 72.4% Test accuracy: 80.6% Minibatch loss at step 150: 41.706875 Minibatch accuracy: 72.0% Validation accuracy: 76.6% Test accuracy: 83.5% Minibatch loss at step 180: 46.694637 Minibatch accuracy: 78.0% Validation accuracy: 75.7% Test accuracy: 81.1% Minibatch loss at step 210: 101.218369 Minibatch accuracy: 80.0% Validation accuracy: 75.6% Test accuracy: 82.2% Minibatch loss at step 240: 22.329721 Minibatch accuracy: 76.0% Validation accuracy: 78.1% Test accuracy: 83.4% Minibatch loss at step 270: 29.404995 Minibatch accuracy: 70.0% Validation accuracy: 79.2% Test accuracy: 83.8% end time: Tue Sep 4 09:52:01 2018 Time used: 2.9922106035760407 123456789101112131415161718192021222324252627282930313233343536373839404142# task 3,加入Dropout 随机失活# TODO: 使用nn.dropout来处理，但是只是用于训练时，评估时不需要用batch_size = 128graph = tf.Graph()with graph.as_default(): # 输入数据，对于训练数据，我们使用一个占位符 # 在运行时，用一个批次 tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size)) tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels)) tf_valid_dataset = tf.constant(valid_dataset) tf_test_dataset = tf.constant(test_dataset) # 初始化变量 weights = tf.Variable( tf.truncated_normal([image_size * image_size, 1024])) biases = tf.Variable(tf.zeros([1024])) # 训练计算 hidden1 = tf.matmul(tf_train_dataset, weights) + biases hidden1 = tf.nn.relu(hidden1) weights1 = tf.Variable( tf.truncated_normal([1024, num_labels])) biases1 = tf.Variable(tf.zeros([num_labels])) # 采用dropout 随机失活 logits = tf.matmul(tf.nn.dropout(hidden1,0.5), weights1) + biases1 loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) # 优化器还是 optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss) # 计算预测值,这里的定义直接可以通过.evl()来调用 train_prediction = tf.nn.softmax(logits) valid_prediction = tf.nn.softmax( tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases),weights1) + biases1) test_prediction = tf.nn.softmax( tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases),weights1) + biases1) 123456789101112131415161718192021222324252627282930313233343536num_steps = 3001def accuracy(predictions, labels): # argmax axis = 0的时候返回每一列最大值的位置索引 # axis = 1的时候返回每一行最大值的位置索引 # axis = 2、3、4...，即为多维张量时，同理推断 # 得到true，fasle 的向量，然后统计1的个数，就是正确率 return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])with tf.Session(graph=graph) as session: tf.global_variables_initializer().run() print(\"Initialized\") starttime = time.clock() print('start time:',time.asctime( time.localtime(time.time()) )) for step in range(num_steps): # 用偏移量来选择数据，10一批次， 101个数据，当取到12批次，offset = 110%101 = 9,可以保证批次有效 # Note:我们可以用更好的随机策略选择批次 offset = (step * batch_size) % (train_labels.shape[0] - batch_size) # 创建一个批次. batch_data = train_dataset[offset:(offset + batch_size), :] batch_labels = train_labels[offset:(offset + batch_size), :] # 准备一个字典，来存放批数据，以便于 placeholder 来放数据 # 数据类型 numpy 的数组 feed_dict = &#123;tf_train_dataset : batch_data, tf_train_labels : batch_labels&#125; _, l, predictions = session.run( [optimizer, loss, train_prediction], feed_dict=feed_dict) if (step % 500 == 0): print(\"Minibatch loss at step %d: %f\" % (step, l)) print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)) print(\"Validation accuracy: %.1f%%\" % accuracy( valid_prediction.eval(), valid_labels)) print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels)) print('end time:',time.asctime( time.localtime(time.time()) )) elapsed = (time.clock() - starttime) print(\"Time used:\",elapsed) Initialized start time: Tue Sep 4 12:01:16 2018 Minibatch loss at step 0: 431.407959 Minibatch accuracy: 14.8% Validation accuracy: 25.6% Test accuracy: 28.3% Minibatch loss at step 500: 13.565708 Minibatch accuracy: 79.7% Validation accuracy: 82.1% Test accuracy: 86.1% Minibatch loss at step 1000: 12.175354 Minibatch accuracy: 75.0% Validation accuracy: 82.2% Test accuracy: 88.3% Minibatch loss at step 1500: 4.373288 Minibatch accuracy: 78.1% Validation accuracy: 81.5% Test accuracy: 87.0% Minibatch loss at step 2000: 4.806540 Minibatch accuracy: 83.6% Validation accuracy: 81.6% Test accuracy: 87.6% Minibatch loss at step 2500: 7.298935 Minibatch accuracy: 79.7% Validation accuracy: 81.0% Test accuracy: 89.0% Minibatch loss at step 3000: 1.783429 Minibatch accuracy: 85.2% Validation accuracy: 82.0% Test accuracy: 89.3% end time: Tue Sep 4 12:01:46 2018 Time used: 30.266830585829666 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# task4 ,调整训练精确度，最高据说达到了97%# TODO:两种思路，一种是学习速率衰减# 另一种是增加多层# 我先来试试学习速率衰减+dropout +正则化：准确率89%batch_size = 128graph = tf.Graph()with graph.as_default(): # 输入数据，对于训练数据，我们使用一个占位符 # 在运行时，用一个批次 tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size)) tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels)) tf_valid_dataset = tf.constant(valid_dataset) tf_test_dataset = tf.constant(test_dataset) # 第一层,层数多了会出现梯度消失，所有初始化时很有讲究，就是初始化为前一层所有节点的个数 w1 = tf.Variable(tf.truncated_normal([image_size * image_size, 1024],stddev=np.sqrt(2.0/(image_size * image_size) ))) b1 = tf.Variable(tf.zeros([1024])) a1 = tf.nn.relu(tf.matmul(tf_train_dataset, w1) + b1) # 第二层 w2 = tf.Variable(tf.truncated_normal([1024, 512],stddev=np.sqrt(2.0/1024 ))) b2 = tf.Variable(tf.zeros([512])) a2 = tf.nn.tanh(tf.matmul(a1, w2) + b2) # # 第三层 w3 = tf.Variable(tf.truncated_normal([512, 256],stddev=np.sqrt(2.0/512 ))) b3 = tf.Variable(tf.zeros([256])) a3 = tf.nn.relu(tf.matmul(a2, w3) + b3) # # 第四层 w4 = tf.Variable(tf.truncated_normal([256, 128],stddev=np.sqrt(2.0/256 ))) b4 = tf.Variable(tf.zeros([128])) a4 = tf.nn.relu(tf.matmul(a3, w4) + b4) # # 第五层 w5 = tf.Variable(tf.truncated_normal([128, num_labels],stddev=np.sqrt(2.0/128 ))) b5 = tf.Variable(tf.zeros([num_labels])) # 特别注意，最后一层不要用激活函数，会梯度消失的，最后都是全连接层 logits = tf.matmul(a4, w5) + b5 loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) # 优化器还是GD+学习速率衰减 global_step = tf.Variable(0) # count the number of steps taken. learning_rate = tf.train.exponential_decay(0.5, global_step,decay_steps=500,decay_rate=0.96) optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step) # optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss) # 计算预测值,这里的定义直接可以通过.evl()来调用 train_prediction = tf.nn.softmax(logits) p1 = tf.nn.relu(tf.matmul(tf_valid_dataset, w1) + b1) p1 = tf.nn.relu(tf.matmul(p1, w2) + b2) p1 = tf.nn.relu(tf.matmul(p1, w3) + b3) p1 = tf.nn.relu(tf.matmul(p1, w4) + b4) p1 = tf.nn.relu(tf.matmul(p1, w5) + b5) valid_prediction = tf.nn.softmax(p1) t1 = tf.nn.relu(tf.matmul(tf_test_dataset, w1) + b1) t1 = tf.nn.relu(tf.matmul(t1, w2) + b2) t1 = tf.nn.relu(tf.matmul(t1, w3) + b3) t1 = tf.nn.relu(tf.matmul(t1, w4) + b4) t1 = tf.nn.relu(tf.matmul(t1, w5) + b5) test_prediction = tf.nn.softmax(t1) 1234567891011121314151617181920212223242526272829num_steps = 6000with tf.Session(graph=graph) as session: tf.global_variables_initializer().run() print(\"Initialized\") starttime = time.clock() print('start time:',time.asctime( time.localtime(time.time()) )) for step in range(num_steps): # 用偏移量来选择数据，10一批次， 101个数据，当取到12批次，offset = 110%101 = 9,可以保证批次有效 # Note:我们可以用更好的随机策略选择批次 offset = (step * batch_size) % (train_labels.shape[0] - batch_size) # 创建一个批次. batch_data = train_dataset[offset:(offset + batch_size), :] batch_labels = train_labels[offset:(offset + batch_size), :] # 准备一个字典，来存放批数据，以便于 placeholder 来放数据 # 数据类型 numpy 的数组 feed_dict = &#123;tf_train_dataset : batch_data, tf_train_labels : batch_labels&#125; _, l, predictions = session.run( [optimizer, loss, train_prediction], feed_dict=feed_dict) if (step % 500 == 0): print(\"Minibatch loss at step %d: %f\" % (step, l)) print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)) print(\"Validation accuracy: %.1f%%\" % accuracy( valid_prediction.eval(), valid_labels)) print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels)) print('end time:',time.asctime( time.localtime(time.time()) )) elapsed = (time.clock() - starttime) print(\"Time used:\",elapsed) Initialized start time: Tue Sep 4 14:47:32 2018 Minibatch loss at step 0: 2.305960 Minibatch accuracy: 14.8% Validation accuracy: 39.5% Test accuracy: 39.8% Minibatch loss at step 500: 0.361337 Minibatch accuracy: 89.8% Validation accuracy: 84.6% Test accuracy: 90.0% Minibatch loss at step 1000: 0.176127 Minibatch accuracy: 92.2% Validation accuracy: 84.7% Test accuracy: 89.9% Minibatch loss at step 1500: 0.134203 Minibatch accuracy: 95.3% Validation accuracy: 83.9% Test accuracy: 89.3% Minibatch loss at step 2000: 0.032851 Minibatch accuracy: 99.2% Validation accuracy: 86.0% Test accuracy: 91.0% Minibatch loss at step 2500: 0.023428 Minibatch accuracy: 100.0% Validation accuracy: 86.1% Test accuracy: 90.8% Minibatch loss at step 3000: 0.051366 Minibatch accuracy: 98.4% Validation accuracy: 86.1% Test accuracy: 91.7% Minibatch loss at step 3500: 0.026836 Minibatch accuracy: 99.2% Validation accuracy: 86.1% Test accuracy: 91.9% Minibatch loss at step 4000: 0.002356 Minibatch accuracy: 100.0% Validation accuracy: 86.1% Test accuracy: 91.4% Minibatch loss at step 4500: 0.002357 Minibatch accuracy: 100.0% Validation accuracy: 86.3% Test accuracy: 91.6% Minibatch loss at step 5000: 0.000582 Minibatch accuracy: 100.0% Validation accuracy: 86.7% Test accuracy: 91.1% Minibatch loss at step 5500: 0.024401 Minibatch accuracy: 99.2% Validation accuracy: 86.4% Test accuracy: 90.7% end time: Tue Sep 4 14:49:17 2018 Time used: 105.25818238910142","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://yoursite.com/tags/TensorFlow/"},{"name":"L2","slug":"L2","permalink":"http://yoursite.com/tags/L2/"},{"name":"Dropout","slug":"Dropout","permalink":"http://yoursite.com/tags/Dropout/"},{"name":"正则化","slug":"正则化","permalink":"http://yoursite.com/tags/正则化/"},{"name":"梯度消失","slug":"梯度消失","permalink":"http://yoursite.com/tags/梯度消失/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}]},{"title":"深度学习的学习速率曲线","slug":"22深度学习的学习速率曲线","date":"2018-04-10T12:19:32.000Z","updated":"2018-08-29T08:57:12.190Z","comments":true,"path":"2018/04/10/22深度学习的学习速率曲线/","link":"","permalink":"http://yoursite.com/2018/04/10/22深度学习的学习速率曲线/","excerpt":"","text":"深度学习的学习速率 问个问题，学习速率越高，越能快速训练好模型对不对？ 显然是错的，如果你降低学习速率，有时反而能更快的收敛模型，有时候是适得其反，留在鞍点上 可以观察损失曲线 1.你的损失函数下降的快与慢与你的模型好坏无关 2.较低的学习曲线反而能更快收敛 最后非常重要的一点，如果你的模型出现问题，首先降低学习速率","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"学习速率","slug":"学习速率","permalink":"http://yoursite.com/tags/学习速率/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}]},{"title":"深度学习的优化函数","slug":"21深度学习的优化函数","date":"2018-04-08T12:19:32.000Z","updated":"2018-09-03T01:04:56.781Z","comments":true,"path":"2018/04/08/21深度学习的优化函数/","link":"","permalink":"http://yoursite.com/2018/04/08/21深度学习的优化函数/","excerpt":"","text":"MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]} }); 深度学习的优化函数 深度学习产生的一个重要原因就是数据计算的庞大规模 梯度下降GD是一种很好的优化损失函数的策略，运行效果如下图蓝线所示 如此庞大的数据集如果运行梯度下降来最优化损失函数，将会非常的耗时 所以我们这里将会使用估计的方法，实话说这是个相当糟糕的估计，很差很差 我们随机从数据集中抽取很小的一部分数据[1,1000]的平均损失; 这里强调下随机，这个我认为是这个方法有效的核心——随机梯度下降（SGD）上图的紫色线 SGD的每次都不大准确，但是它很快所以要执行更多次数来弥补它的不精确性 针对他的精确性提高有动量梯度下降法以及ADAM 算法 动量梯度下降 V_dw=0.9*V_dw+0.1*dW \\\\ V_db=0.9*V_db+0.1*db 其实就是把前面梯度的移动平均值计算了出来，用平均值代替前一批数据的梯度 这个动量梯度下降非常有用，也能帮助模型收敛 学习速率衰减，越接近目标损失时，越是要用较小的学习速率 第一种 设置衰减率，decayrate称为衰减率，epochnum为代数，a0为初始学习率） \\alpha=\\frac{1}{1+decayrate*epochnum} 第二种是设置指数 \\alpha=0.95^{epochnum}*\\alpha0 还有手动减少，离散衰减等 AdaGrad——首选优化算法 它是SGD的优化版本 使用了动量防止过拟合 使用了学习速率衰减（自动衰减） 准确率比使用动量的SGD低一点","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"机器学习","slug":"深度学习/机器学习","permalink":"http://yoursite.com/categories/深度学习/机器学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"优化函数","slug":"优化函数","permalink":"http://yoursite.com/tags/优化函数/"},{"name":"SGD，GD","slug":"SGD，GD","permalink":"http://yoursite.com/tags/SGD，GD/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"机器学习","slug":"深度学习/机器学习","permalink":"http://yoursite.com/categories/深度学习/机器学习/"}]},{"title":"深度神经网络的基本概念——随机梯度下降与梯度下降","slug":"4使用梯度下降和随机梯度下降训练一个全连接网络","date":"2018-04-03T12:19:32.000Z","updated":"2018-09-03T06:21:37.338Z","comments":true,"path":"2018/04/03/4使用梯度下降和随机梯度下降训练一个全连接网络/","link":"","permalink":"http://yoursite.com/2018/04/03/4使用梯度下降和随机梯度下降训练一个全连接网络/","excerpt":"","text":"要点使用梯度下降和随机梯度下降训练一个全连接网络 数据集 数据集采用的是notMNIST数据集，这个更加像真实的数据集，不如MNIST 数据集干净，更加有难 数据集有训练集 500K ，测试集 19000，这个规模在PC机上跑很快 标签为A 到 J (10个类别) 每个图像特征 28*28 像素 原数据地 TensorFlow 的工作流程如下： 12345678910 graph TDInput[节点:输入] --&gt; GraphVar[节点:变量] --&gt; Graphoper[节点:操作] --&gt; GraphGraph[初始化Graph]--&gt;init(with graph.as_default)init--&gt;run(具体执行 session.run) run--&gt;res(拿到graph的执行结果with tf.Session graph=graph as session:) 用到的TensorFlow函数: tf.truncated_normal 从一个正态分布片段中输出随机数值，只保留两个标准差以内的值，超出的值会被弃掉重新生成 1234567def tf.truncated_normal( shape, #一个一维整数张量 或 一个Python数组。 这个值决定输出张量的形状。 mean=0.0,#一个零维张量或 类型属于dtype的Python值. 这个值决定正态分布片段的平均值 stddev=1.0,# 一个零维张量或 类型属于dtype的Python值. 这个值决定正态分布片段的标准差 dtype=tf.float32,# 输出的类型. seed=None, # 一个Python整数. 被用来为正态分布创建一个随机种子. name=None)#操作的名字 (可选参数). reduce_mean 123456tf.reduce_mean( input_tensor, # 需要求平均值的张量。应该存在数字类型。 axis=None, # 需要求平均值的维度. 如果没有设置（默认情况），所有的维度都会被减值。 keep_dims=False, # 如果为真，维持减少的维度长度为1. name=None, # 操作的名字(可选值) reduction_indices=None) #旧的**axis**参数的名字(已弃用) 可跨越维度的计算张量各元素的平均值 例子 12import numpy as npimport tensorflow as tf 1a = np.arange(20).reshape(4,5) 1234array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]]) 12# 对列求平均b = tf.reduce_mean(a,0) 1sess = tf.Session() 1sess.run(b) 1array([ 7, 8, 9, 10, 11]) 12# 对行求平均c = tf.reduce_mean(a,1) 1sess.run(c) 1array([ 2, 7, 12, 17]) softmax_cross_entropy_with_logits 123456def softmax_cross_entropy_with_logits( _sentinel=None, labels=None, # labels one-hot 向量 logits=None, # 预测值 one-hot 向量 dim=-1, name=None ): 12345def sparse_softmax_cross_entropy_with_logits( _sentinel=None, labels=None, # 原始标签 logits=None, # 预测值 one-hot 向量 name=None ): 例子 1234567891011121314151617181920212223242526272829303132333435363738394041import tensorflow as tf # 神经网络的输出logits=tf.constant([[1.0,2.0,3.0],[1.0,2.0,3.0],[1.0,2.0,3.0]]) # 对输出做softmax操作y=tf.nn.softmax(logits) # 真实数据标签，one hot形式y_=tf.constant([[0.0,0.0,1.0],[0.0,0.0,1.0],[0.0,0.0,1.0]]) # 将标签稠密化dense_y=tf.argmax(y_,1) # dense_y = [2 2 2]# 采用普通方式计算交叉熵cross_entropy = -tf.reduce_sum(y_*tf.log(y))# 使用softmax_cross_entropy_with_logits方法计算交叉熵cross_entropy2=tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))# 使用sparse_softmax_cross_entropy_with_logits方法计算交叉熵cross_entropy3=tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=dense_y)) with tf.Session() as sess: softmax=sess.run(y) c_e = sess.run(cross_entropy) c_e2 = sess.run(cross_entropy2) c_e3 = sess.run(cross_entropy3) print(\"step1:softmax result=\") print(softmax) print(\"y_ = \") print(sess.run(y_)) print(\"tf.log(y) = \") print(sess.run(tf.log(y))) print(\"dense_y =\") print(sess.run(dense_y)) print(\"step2:cross_entropy result=\") print(c_e) print(\"Function(softmax_cross_entropy_with_logits) result=\") print(c_e2) print(\"Function(sparse_softmax_cross_entropy_with_logits) result=\") print(c_e3)作者：泊牧链接：https://www.jianshu.com/p/3b084ec9ed80來源：简书简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。 np.argmax tf.argmax 返回最大值的索引，常用于softmax 后的结果预测以及ont-hot 向量的处理 1234tf.argmax(input, axis=None,# axis = 0的时候返回每一列最大值的位置索引 ;axis = 1的时候返回每一行最大值的位置索引 ;axis = 2、3、4...，即为多维张量时，同理推断 name=None, dimension=None) 代码1234567# These are all the modules we'll be using later. Make sure you can import them# before proceeding further.from __future__ import print_functionimport numpy as npimport tensorflow as tffrom six.moves import cPickle as picklefrom six.moves import range 123456789101112131415161718192021# 数据集采用的是notminist 数据集，这个更加像真实的数据集，不如mnist 数据集干净，更加有难度# 数据集有训练集 500K ，测试集 19000，这个规模在PC机上跑很快# 标签为A 到 J (10个类别)# 每个图像特征 28*28 像素# 原数据地址 http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html# 导入训练集合，用的是notMNISTpickle_file = 'notMNIST.pickle'with open(pickle_file, 'rb') as f: save = pickle.load(f) # 之前按键值对存放的，现在按键值对取出来 train_dataset = save['train_dataset'] train_labels = save['train_labels'] valid_dataset = save['valid_dataset'] valid_labels = save['valid_labels'] test_dataset = save['test_dataset'] test_labels = save['test_labels'] del save # hint to help gc free up memory print('Training set', train_dataset.shape, train_labels.shape) print('Validation set', valid_dataset.shape, valid_labels.shape) print('Test set', test_dataset.shape, test_labels.shape) Training set (20000, 28, 28) (20000,) Validation set (1000, 28, 28) (1000,) Test set (1000, 28, 28) (1000,) 12345678910111213141516# 把3维的训练数据转化为2维，即把28*28的图像像素矩阵转化成一维向量的转置 # 把标签转化为one-hot 向量image_size = 28num_labels = 10def reformat(dataset, labels): dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32) # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...] labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32) return dataset, labelstrain_dataset, train_labels = reformat(train_dataset, train_labels)valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)test_dataset, test_labels = reformat(test_dataset, test_labels)print('Training set', train_dataset.shape, train_labels.shape)print('Validation set', valid_dataset.shape, valid_labels.shape)print('Test set', test_dataset.shape, test_labels.shape) Training set (20000, 784) (20000, 10) Validation set (1000, 784) (1000, 10) Test set (1000, 784) (1000, 10) 123456789101112131415161718192021222324252627282930313233343536373839# 首先我们用梯度下降来训练多项式回归模型# 使用梯度下降的话，数目不能太多，不然计算很耗时# 我们这里用10000的数据子集train_subset = 10000graph = tf.Graph()with graph.as_default(): # 输入数据，常量 # 这是图的输入节点 tf_train_dataset = tf.constant(train_dataset[:train_subset, :]) tf_train_labels = tf.constant(train_labels[:train_subset]) tf_valid_dataset = tf.constant(valid_dataset) tf_test_dataset = tf.constant(test_dataset) # Variables.变量 # 变量是我们模型训练中需要改变的量：权重和偏移 # 权重使用truncated进行初始化，并且归一化参数，这个是为了防止梯度爆炸和消失 # 偏移全部初始化为0 weights = tf.Variable( tf.truncated_normal([image_size * image_size, num_labels])) biases = tf.Variable(tf.zeros([num_labels])) # 训练模型 # 模型很简单，特征 * 权重 + 偏移 # 使用softmax 以及交叉熵来计算损失函数 logits = tf.matmul(tf_train_dataset, weights) + biases loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) # 优化器 # GD 梯度下降. optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss) # 用softmax 预测结果 # 计算出验证集和测试集的预测结果 train_prediction = tf.nn.softmax(logits) valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases) test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases) 123import timelocaltime = time.asctime( time.localtime(time.time()) )print(localtime) Fri Aug 31 16:01:56 2018 1234567891011121314151617181920212223242526272829303132# 接下来让我们迭代运行num_steps = 801def accuracy(predictions, labels): # argmax axis = 0的时候返回每一列最大值的位置索引 # axis = 1的时候返回每一行最大值的位置索引 # axis = 2、3、4...，即为多维张量时，同理推断 # 得到true，fasle 的向量，然后统计1的个数，就是正确率 return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])with tf.Session(graph=graph) as session: # 一次性的初始化操作，把graph 中的所有变量一次性初始化好 tf.global_variables_initializer().run() print('Initialized') starttime = time.clock() print('start time:',time.asctime( time.localtime(time.time()) )) for step in range(num_steps): # 通过session.run 计算 optimizer，loss,以及 预测结果 _, l, predictions = session.run([optimizer, loss, train_prediction]) # 每一百步输入损失函数， if (step % 100 == 0): print('Loss at step %d: %f' % (step, l)) print('Training accuracy: %.1f%%' % accuracy( predictions, train_labels[:train_subset])) # 这里不能使用tensor 因为accuracy里面调用的是numpy；numpy使用的是np的ndarray #在验证集合上调用.eval() 等价于 session.run()方法，但是返回值是one-hot 的预测结果，它有图的所有信息 print('Validation accuracy: %.1f%%' % accuracy( valid_prediction.eval(), valid_labels)) print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels)) print('end time:',time.asctime( time.localtime(time.time()) )) elapsed = (time.clock() - starttime) print(\"Time used:\",elapsed) Initialized start time: Fri Aug 31 16:09:36 2018 Loss at step 0: 17.316351 Training accuracy: 9.8% Validation accuracy: 13.1% Loss at step 100: 2.382646 Training accuracy: 72.3% Validation accuracy: 73.9% Loss at step 200: 1.911187 Training accuracy: 74.5% Validation accuracy: 75.0% Loss at step 300: 1.647586 Training accuracy: 75.9% Validation accuracy: 75.3% Loss at step 400: 1.472351 Training accuracy: 76.6% Validation accuracy: 75.6% Loss at step 500: 1.344603 Training accuracy: 77.3% Validation accuracy: 75.5% Loss at step 600: 1.245956 Training accuracy: 77.9% Validation accuracy: 76.1% Loss at step 700: 1.166571 Training accuracy: 78.6% Validation accuracy: 76.0% Loss at step 800: 1.100793 Training accuracy: 78.9% Validation accuracy: 75.9% Test accuracy: 81.9% end time: Fri Aug 31 16:10:02 2018 Time used: 26.420718226680037 12345678910111213141516171819202122232425262728293031323334# 下面训练随机梯度下降# 将数据保存到常量节点，创建一个占位节点,每次用数据代入占位节点# 图的初始化操作batch_size = 128graph = tf.Graph()with graph.as_default(): # 输入数据，对于训练数据，我们使用一个占位符 # 在运行时，用一个批次 tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size)) tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels)) tf_valid_dataset = tf.constant(valid_dataset) tf_test_dataset = tf.constant(test_dataset) # 初始化变量 weights = tf.Variable( tf.truncated_normal([image_size * image_size, num_labels])) biases = tf.Variable(tf.zeros([num_labels])) # 训练计算 logits = tf.matmul(tf_train_dataset, weights) + biases loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) # 优化器还是 optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss) # 计算预测值 train_prediction = tf.nn.softmax(logits) valid_prediction = tf.nn.softmax( tf.matmul(tf_valid_dataset, weights) + biases) test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases) 12345678910111213141516171819202122232425262728num_steps = 3001with tf.Session(graph=graph) as session: tf.global_variables_initializer().run() print(\"Initialized\") starttime = time.clock() print('start time:',time.asctime( time.localtime(time.time()) )) for step in range(num_steps): # 用偏移量来选择数据，10一批次， 101个数据，当取到12批次，offset = 110%101 = 9,可以保证批次有效 # Note:我们可以用更好的随机策略选择批次 offset = (step * batch_size) % (train_labels.shape[0] - batch_size) # 创建一个批次. batch_data = train_dataset[offset:(offset + batch_size), :] batch_labels = train_labels[offset:(offset + batch_size), :] # 准备一个字典，来存放批数据，以便于 placeholder 来放数据 # 数据类型 numpy 的数组 feed_dict = &#123;tf_train_dataset : batch_data, tf_train_labels : batch_labels&#125; _, l, predictions = session.run( [optimizer, loss, train_prediction], feed_dict=feed_dict) if (step % 500 == 0): print(\"Minibatch loss at step %d: %f\" % (step, l)) print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)) print(\"Validation accuracy: %.1f%%\" % accuracy( valid_prediction.eval(), valid_labels)) print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels)) print('end time:',time.asctime( time.localtime(time.time()) )) elapsed = (time.clock() - starttime) print(\"Time used:\",elapsed) Initialized start time: Fri Aug 31 18:31:58 2018 Minibatch loss at step 0: 17.747850 Minibatch accuracy: 14.1% Validation accuracy: 14.1% Minibatch loss at step 500: 1.452345 Minibatch accuracy: 76.6% Validation accuracy: 76.6% Minibatch loss at step 1000: 1.574866 Minibatch accuracy: 76.6% Validation accuracy: 77.6% Minibatch loss at step 1500: 0.657050 Minibatch accuracy: 84.4% Validation accuracy: 77.0% Minibatch loss at step 2000: 0.710017 Minibatch accuracy: 83.6% Validation accuracy: 78.5% Minibatch loss at step 2500: 1.029955 Minibatch accuracy: 77.3% Validation accuracy: 78.4% Minibatch loss at step 3000: 0.693648 Minibatch accuracy: 84.4% Validation accuracy: 78.4% Test accuracy: 84.5% end time: Fri Aug 31 18:32:01 2018 Time used: 2.942584584146971","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://yoursite.com/tags/TensorFlow/"},{"name":"SGD","slug":"SGD","permalink":"http://yoursite.com/tags/SGD/"},{"name":"GD","slug":"GD","permalink":"http://yoursite.com/tags/GD/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}]},{"title":"多层神经网络","slug":"1.多层神经网络TensorFlow ReLUs","date":"2018-04-03T12:19:32.000Z","updated":"2018-08-30T09:58:08.940Z","comments":true,"path":"2018/04/03/1.多层神经网络TensorFlow ReLUs/","link":"","permalink":"http://yoursite.com/2018/04/03/1.多层神经网络TensorFlow ReLUs/","excerpt":"","text":"深度神经网络 神经神经网络如何对非线性模型进行预测？ 通过激活函数，激活函数可以使使神经网络适应非线性的拟合，以解决更复杂的问题 介绍个非常常用的激活函数——RELU (rectified linear unit）懒惰工程师最常用的函数 代码下面用 ReLU 函数把一个线性单层网络转变成非线性多层网络 这个代码很简单，过程如下图所示： 这段模拟了一个预测的过程 初始化好权重，偏差，以及输入特征，输入特征是个3*4矩阵 经过计算后还是一个3*3 矩阵然后经过relu 处理 relu 处理后还是个3*3的矩阵，然后经过第二次权重和偏差得到输出 1import tensorflow as tf 12C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters 12345678910111213# 定义输出层output = None# 假设隐藏层权重 4*3 的矩阵，输入4个节点，就是4个特征，输出3个节点hidden_layer_weights = [ [0.1, 0.2, 0.4], [0.4, 0.6, 0.6], [0.5, 0.9, 0.1], [0.8, 0.2, 0.8]]# 假设输出权重 3*2 的矩阵，输入3个节点，输出2个节点out_weights = [ [0.1, 0.6], [0.2, 0.1], [0.7, 0.9]] 1234567# Weights and biasesweights = [ tf.Variable(hidden_layer_weights), tf.Variable(out_weights)]biases = [ tf.Variable(tf.zeros(3)), tf.Variable(tf.zeros(2))] 12# Input 输入特征 3*4 的矩阵features = tf.Variable([[1.0, 2.0, 3.0, 4.0], [-1.0, -2.0, -3.0, -4.0], [11.0, 12.0, 13.0, 14.0]]) 12345# TODO: Create Modelhidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])hidden_layer = tf.nn.relu(hidden_layer)output = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1]) 12345# TODO: Print session results# 提个问题，为什么一定要通过sess.run 的形式？with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(sess.run(output)) 123[[ 5.11 8.440001] [ 0. 0. ] [24.010002 38.239998]]","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"TensorFlow","slug":"深度学习/TensorFlow","permalink":"http://yoursite.com/categories/深度学习/TensorFlow/"},{"name":"RELU","slug":"深度学习/TensorFlow/RELU","permalink":"http://yoursite.com/categories/深度学习/TensorFlow/RELU/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"多层神经网络","slug":"多层神经网络","permalink":"http://yoursite.com/tags/多层神经网络/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://yoursite.com/tags/TensorFlow/"},{"name":"RELU","slug":"RELU","permalink":"http://yoursite.com/tags/RELU/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"TensorFlow","slug":"深度学习/TensorFlow","permalink":"http://yoursite.com/categories/深度学习/TensorFlow/"},{"name":"RELU","slug":"深度学习/TensorFlow/RELU","permalink":"http://yoursite.com/categories/深度学习/TensorFlow/RELU/"}]},{"title":"深度神经网络的基本概念","slug":"2.深度神经网络的基本概念","date":"2018-04-03T12:19:32.000Z","updated":"2018-08-31T00:37:50.906Z","comments":true,"path":"2018/04/03/2.深度神经网络的基本概念/","link":"","permalink":"http://yoursite.com/2018/04/03/2.深度神经网络的基本概念/","excerpt":"","text":"深度神经网络线性模型的局限性 假设你有28x28的图片,经过一个简单的线性变换 只有一层W和b,然后预测10类，那么你的参数个数就有28x28x10+10 = 7850个参数 GPU就是专门用来应对大型矩阵相乘，成本低，计算速度快 线性模型可以很好表示特征之间相加的关系，但是不能表示特征之间相乘的关系 线性运行非常的稳定：y=wx ,x的微小变化不会引起y的巨大变化，所以模型中使用线性计算使模型保持稳定，还要加入非线性计算，使模型能够预测非线性的情况 链式法则 深度神经网络之间能够传递导数的秘诀就是——链式法则 g(f(x))' = g'(f(x))*f(x)'反向传播 预测的过程是前向传播 模型调整参数的过程是反向传播； 核心就是用了链式法则，这里不展开讲反向传播了； 但是要记住的是反正传播相对于前向传播需要2倍的计算和空间，当你要改变模型的大小并放入内存中的时候就要考虑这个问题了 神经网络的训练过程 我们可以增加很多隐藏层来学习复杂的特征 例如图片，第一层就是线边点，第二层是一些鼻子，眼睛的轮廓…..第N层出现了人脸","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"链式法则","slug":"链式法则","permalink":"http://yoursite.com/tags/链式法则/"},{"name":"反向传播","slug":"反向传播","permalink":"http://yoursite.com/tags/反向传播/"},{"name":"线性模型优势","slug":"线性模型优势","permalink":"http://yoursite.com/tags/线性模型优势/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}]},{"title":"深度神经网络的基本概念——高方差03","slug":"3.深度神经网络的基本概念——高方差问题","date":"2018-04-03T12:19:32.000Z","updated":"2018-08-31T01:41:21.377Z","comments":true,"path":"2018/04/03/3.深度神经网络的基本概念——高方差问题/","link":"","permalink":"http://yoursite.com/2018/04/03/3.深度神经网络的基本概念——高方差问题/","excerpt":"","text":"要点深度神经网络兴起的先决条件 历史久远：深度学习框架可以追溯到1980年福岛邦彦提出的新认知机 大量的训练数据：以前获取大量的数据很难，现在变得越来越容易 硬件条件：超高速计算机的诞生使得需要大量计算的深度神经网络的训练时间大大降低 紧身裤问题 如果你有一条非常合身的紧身裤，那么这个裤子就很难穿上——紧身裤用来比喻模型的模型的过拟合 解决方案常见的几种： 早停法——观察误差曲线，当出现过拟合情况时就停止训练 正则化——给参数加上一些束缚，而且不改变模型的结构 L1 正则化——降低 |权重|，使得有些接近0的参数变成0 L2 正则化——降低权重^2，把权重趋向于0，但不是0 Dropout——随机失活 Jeffery Hinton 提出的，把层与层之间的传递值，选择一定的概率把这些值置为0 所以神经网络就必须学会冗余备份各种学到的信息 这个训练过程感觉起来很低效，但是它把模型变得像多个神经网络组合成的，反正更加坚固 如果Dropout都对你的神经网络没有什么效果，那你就该换个更大的网络了 前向传播计算时，失活的值是0，但是未失活的值要除以失活概率（试想一下概率是1的情况），才可以正确计算预测值","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"过拟合，正则化","slug":"过拟合，正则化","permalink":"http://yoursite.com/tags/过拟合，正则化/"},{"name":"误差曲线，L1","slug":"误差曲线，L1","permalink":"http://yoursite.com/tags/误差曲线，L1/"},{"name":"L2","slug":"L2","permalink":"http://yoursite.com/tags/L2/"},{"name":"Dropout","slug":"Dropout","permalink":"http://yoursite.com/tags/Dropout/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}]},{"title":"模型的性能评估","slug":"20 型性能评估","date":"2018-04-02T12:19:32.000Z","updated":"2018-08-30T01:18:04.409Z","comments":true,"path":"2018/04/02/20 型性能评估/","link":"","permalink":"http://yoursite.com/2018/04/02/20 型性能评估/","excerpt":"","text":"模型性能评估 如何衡量评估你的模型的表现，问题就解决了一半了 为什么需要训练集，验证集，测试集？ 模型即使在训练集上达到了百分百的准确率，但是我们要使用的是其泛化的能力，即：预测新事物的能力 如何判断其泛化能力，我们要使用模型未记忆（训练）的数据，但是经过一次次的训练测试，测试集达到了很好的效果，但是在放入生产预测时效果又不好了，why？. 因为经过一次次的预测，模型会对测试集产生一些记忆，虽然只有一点点，但是次数多了，就会产生很大的影响. 解决方法，就是在测试集里面再划分出一个集合，用于最后的模型评估 介绍一个平台——Kaggle Challenge，机器学习的竞赛平台. 这里的数据分成三个部分，一个是训练集，一个公开的验证集，一个不公开的测试集. 有些人公开的验证集上做的很好，但是测试集上的效果不佳，很有可能是验证集测试了很多遍； 好的做法是，将公开的验证集划分出一块作为测试集 如何划分 训练集，验证集，测试集 我一般在数据集不多的情况下划分为 6:2:2 如何判断验证集有效呢，要超过30个样本（经验法则）的改变才足以说明 总共有3000个样本，效果从80%-&gt;81% 总共有3000个样本，效果从80%-&gt;80.5% 0.01*3000 = 30 说明有效 0.005 * 3000 = 15 说明有可能是噪声引起的 验证集的大小 一般要求&gt;30000个样本，那么30个有效变化，占了（30/30000 = 0.001 = 0.1%）,这样可以肉眼可见有效的变化 比起交叉验证，我更加倾向于获取更多的数据","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"机器学习","slug":"深度学习/机器学习","permalink":"http://yoursite.com/categories/深度学习/机器学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"Kaggle","slug":"Kaggle","permalink":"http://yoursite.com/tags/Kaggle/"},{"name":"模型评估","slug":"模型评估","permalink":"http://yoursite.com/tags/模型评估/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"机器学习","slug":"深度学习/机器学习","permalink":"http://yoursite.com/categories/深度学习/机器学习/"}]},{"title":"sklearn的模型保存08","slug":"test_09_保存模型_save","date":"2018-03-12T12:19:32.000Z","updated":"2018-08-29T08:58:11.142Z","comments":true,"path":"2018/03/12/test_09_保存模型_save/","link":"","permalink":"http://yoursite.com/2018/03/12/test_09_保存模型_save/","excerpt":"","text":"12from sklearn import svmfrom sklearn import datasets 标题1234clf = svm.SVC()iris = datasets.load_iris()X, y = iris.data, iris.targetclf.fit(X,y) SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto&#39;, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) 12345# method 1:pickleimport pickle# 保存模型with open('save/clf.pickle','wb') as f: pickle.dump(clf,f) 1234# 载入模型with open('save/clf.pickle','rb') as f: clf2 = pickle.load(f)clf2.predict(X[0:4]) 1234# method 2:joblib 第二种方法 from sklearn.externals import joblib# save 更加快，相对于原生的python的picklejoblib.dump(clf,'save/clf.pkl') 123# restore 载入模型clf3 = joblib.load('save/clf.pkl')clf3.predict(X[0:4]) array([0, 0, 0, 0])","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"模型保存","slug":"模型保存","permalink":"http://yoursite.com/tags/模型保存/"},{"name":"pickle","slug":"pickle","permalink":"http://yoursite.com/tags/pickle/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}]},{"title":"sklearn的学习曲线_过拟合欠拟合大杀器07","slug":"test-07-sklearn_学习曲线_过拟合欠拟合大杀器07","date":"2018-03-10T12:19:32.000Z","updated":"2018-08-24T10:59:42.940Z","comments":true,"path":"2018/03/10/test-07-sklearn_学习曲线_过拟合欠拟合大杀器07/","link":"","permalink":"http://yoursite.com/2018/03/10/test-07-sklearn_学习曲线_过拟合欠拟合大杀器07/","excerpt":"","text":"要点 正常的训练过程会出现过拟合以及欠拟合等情况，所以要识别这些情况，以下图为例： 左图欠拟合，中间正好，右图过拟合 一句话说明学习曲线learning_curve——通过增加训练数据，来观察模型的获益效果 一般学习曲线长什么样? 朴素贝叶斯大致收敛到一个较低的分数 支持向量机（SVM）是样本越多越好 我们使用手写数字数据集 分别用朴素贝叶斯和SVM 来训练模型 参考文档 验证曲线: 绘制分数以评估模型 代码123456from sklearn.model_selection import learning_curve #学习曲线模块from sklearn.datasets import load_digits #digits数据集from sklearn.svm import SVC #Support Vector Classifierimport matplotlib.pyplot as plt #可视化模块from sklearn.model_selection import ShuffleSplit # 专业的数据集分割包import numpy as np 1234# 手写数字，0-9，共1797样本，每个样本由64个特征组合（8*8），每个特征值用0-16表示digits = load_digits()X = digits.datay = digits.target 1234567# 迭代次数100次，交叉验证集分成100个，测试集比例0.2cv = ShuffleSplit(n_splits=100, test_size=0.2)#平均每一轮所得到的平均方差(共5轮，分别为样本10%、25%、50%、75%、100%),采用均方差度量损失，train_sizes, train_scores, test_scores= learning_curve( # SVM 中的参数，一个C 大间距，一个gammam, gamma 不是高斯半径 是 1/(2*sigma^2),都是越大越容易过拟合 SVC(gamma=0.0001), X, y, cv=cv, train_sizes=np.linspace(.1, 1.0, 5),n_jobs=5) 123# axis=1 对行进行操作，由于上面的cv=分成了20份，所以 train_scores 是一个20个元素的数组train_scores_mean = np.mean(train_scores, axis=1)test_scores_mean = np.mean(test_scores, axis=1) 123456789101112# train_sizes 横坐标，train_loss_mean 纵坐标，形状‘o-’,红色plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training\")plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation\")plt.xlabel(\"Training examples\")plt.ylabel(\"Loss\")# 将图标签——图例放到最佳的位置plt.legend(loc=\"best\")title = \"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"plt.title(title) 1Text(0.5,1,&apos;Learning Curves (SVM, RBF kernel, $\\\\gamma=0.001$)&apos;) 123456789# 接下来用朴素贝叶斯分类器from sklearn.naive_bayes import GaussianNB# 迭代次数100次，交叉验证集分成100个，测试集比例0.2cv = ShuffleSplit(n_splits=100, test_size=0.2)#平均每一轮所得到的平均方差(共5轮，分别为样本10%、25%、50%、75%、100%),采用均方差度量损失，train_sizes, train_scores, test_scores= learning_curve( # SVM 中的参数，一个C 大间距，一个gammam, gamma 不是高斯半径 是 1/(2*sigma^2),都是越大越容易过拟合 GaussianNB(), X, y, cv=cv, train_sizes=np.linspace(.1, 1.0, 5),n_jobs=2) 123# axis=1 对行进行操作，由于上面的cv=分成了20份，所以 train_scores 是一个20个元素的数组train_scores_mean = np.mean(train_scores, axis=1)test_scores_mean = np.mean(test_scores, axis=1) 123456789101112# train_sizes 横坐标，train_loss_mean 纵坐标，形状‘o-’,红色plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training\")plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation\")plt.xlabel(\"Training examples\")plt.ylabel(\"Loss\")# 将图标签——图例放到最佳的位置plt.legend(loc=\"best\")title = \"Learning Curves (Naive Bayes)\"plt.title(title) 1Text(0.5,1,&apos;Learning Curves (Naive Bayes)&apos;)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"分类","slug":"机器学习/sklearn/分类","permalink":"http://yoursite.com/categories/机器学习/sklearn/分类/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"学习曲线","slug":"学习曲线","permalink":"http://yoursite.com/tags/学习曲线/"},{"name":"过拟合","slug":"过拟合","permalink":"http://yoursite.com/tags/过拟合/"},{"name":"欠拟合","slug":"欠拟合","permalink":"http://yoursite.com/tags/欠拟合/"},{"name":"手写数字","slug":"手写数字","permalink":"http://yoursite.com/tags/手写数字/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"分类","slug":"机器学习/sklearn/分类","permalink":"http://yoursite.com/categories/机器学习/sklearn/分类/"}]},{"title":"sklearn的cross_validation交叉验证06","slug":"test-06-sklearn_cross_validation_的交叉验证06","date":"2018-03-09T12:19:32.000Z","updated":"2018-08-24T09:44:16.777Z","comments":true,"path":"2018/03/09/test-06-sklearn_cross_validation_的交叉验证06/","link":"","permalink":"http://yoursite.com/2018/03/09/test-06-sklearn_cross_validation_的交叉验证06/","excerpt":"","text":"要点 正常的训练过程会出现过拟合以及欠拟合等情况，所以要识别这些情况，首先你要懂得是交叉验证 本文我们用到的是利用 scikit-learn 包中的 train_test_split 可以快速划分数据集 我们使用鸢尾花数据集，首先采用0.25的划分来看看训练结果 然后我们 交叉验证（CV 缩写）)，即是设置不同（”hyperparameters(超参数)”）K近邻中的K来使模型达到最佳状态 我最常用的方法调用 cross_val_score，通过不同的划分集合来判断最好的参数取值 这里用的评估指标是accuracy以及neg_mean_squared_error更多指标 如果你有多个模型要链接成一个，把特征选择、归一化和分类合并成一个过程，那么要用 Pipeline（管道）和 FeatureUnion（特征联合）: 合并的评估器. 参考文档 3.1. 交叉验证：评估估算器的表现 代码123from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.neighbors import KNeighborsClassifier 123iris = load_iris()X = iris.dataY = iris.target 123# random_state 用来设置seed 的，seed 是确保相同划分的一种设置，比如可以这么写random_state = 3 # test_size 是测试集的划分大小，默认是0.25X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.25) 1234# K 近邻算法，常见分类算法，这里不是邻居越多越好，越少过拟合，越多欠拟合knn = KNeighborsClassifier(n_neighbors=6)knn.fit(X_train,Y_train)print(\"&gt; 没有使用交叉验证的得分： \",knn.score(X_test,Y_test)) 1&gt; 没有使用交叉验证的得分： 0.9210526315789473 12# K折交叉验证模块from sklearn.model_selection import cross_val_score 1234knn = KNeighborsClassifier(n_neighbors=6)# 把数据集分成5组，每组都是训练集合测试集scores = cross_val_score(knn,X,Y,cv = 5,scoring=\"accuracy\")print(\"&gt; 使用交叉验证的准确度： \",scores) 1&gt; 使用交叉验证的准确度： [0.96666667 1. 0.96666667 0.96666667 1. ] 12import numpy as npprint(\"&gt; 测试集的划分大小是： \",np.shape(X_test)[0]/(np.shape(X_train)[0]+np.shape(X_test)[0])) 1&gt; 测试集的划分大小是： 0.25333333333333335 12# 求下平均值print(\"&gt; 使用交叉验证的准确度平均值： \",scores.mean()) 1&gt; 使用交叉验证的准确度平均值： 0.9800000000000001 1234567891011# 现在切入正题，如何调整超参数 ，以n_neighbors为例k_scores = []k_losses = []for i in range(1,31): knn = KNeighborsClassifier(n_neighbors=i) # 把数据集分成10组，每组都是训练集合测试集 scores = cross_val_score(knn,X,Y,cv = 10,scoring=\"accuracy\") # 如果是回归问题使用neg_mean_squared_error,就是均方差，但是是负的，所以要加- loss = -cross_val_score(knn,X,Y,cv = 10,scoring=\"neg_mean_squared_error\") k_scores.append(scores.mean()) k_losses.append(loss.mean()) 12345# 画图画出来import matplotlib.pyplot as pltplt.plot(range(1,31),k_scores)plt.xlabel(\"Value of K for KNN\")plt.ylabel(\"cross-validated Accuracy\") 1Text(0,0.5,&apos;cross-validated Accuracy&apos;) 123plt.plot(range(1,31),k_losses)plt.xlabel(\"Value of K for KNN\")plt.ylabel(\"cross-validated loss\") 1Text(0,0.5,&apos;cross-validated loss&apos;) 12345678910111213# 如果你有归一化的过程，那么怎么使用交叉验证呢，你需要的pipelinefrom sklearn.pipeline import make_pipelinefrom sklearn import preprocessingk_scores = []k_losses = []for i in range(1,31): knn = make_pipeline(preprocessing.StandardScaler(), KNeighborsClassifier(n_neighbors=i)) # 把数据集分成10组，每组都是训练集合测试集 scores = cross_val_score(knn,X,Y,cv = 10,scoring=\"accuracy\") # 如果是回归问题使用neg_mean_squared_error,就是均方差，但是是负的，所以要加- loss = -cross_val_score(knn,X,Y,cv = 10,scoring=\"neg_mean_squared_error\") k_scores.append(scores.mean()) k_losses.append(loss.mean()) 12345# 画图画出来import matplotlib.pyplot as pltplt.plot(range(1,31),k_scores)plt.xlabel(\"Value of K for KNN\")plt.ylabel(\"cross-validated Accuracy\") 1Text(0,0.5,&apos;cross-validated Accuracy&apos;)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"分类","slug":"机器学习/sklearn/分类","permalink":"http://yoursite.com/categories/机器学习/sklearn/分类/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"分类","slug":"分类","permalink":"http://yoursite.com/tags/分类/"},{"name":"交叉验证","slug":"交叉验证","permalink":"http://yoursite.com/tags/交叉验证/"},{"name":"cross_validation","slug":"cross-validation","permalink":"http://yoursite.com/tags/cross-validation/"},{"name":"鸢尾花","slug":"鸢尾花","permalink":"http://yoursite.com/tags/鸢尾花/"},{"name":"K近邻","slug":"K近邻","permalink":"http://yoursite.com/tags/K近邻/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"分类","slug":"机器学习/sklearn/分类","permalink":"http://yoursite.com/categories/机器学习/sklearn/分类/"}]},{"title":"sklearn的归一化05","slug":"test-05-sklearn的归一化05","date":"2018-03-08T12:19:32.000Z","updated":"2018-08-24T09:13:57.085Z","comments":true,"path":"2018/03/08/test-05-sklearn的归一化05/","link":"","permalink":"http://yoursite.com/2018/03/08/test-05-sklearn的归一化05/","excerpt":"","text":"要点 我们通过make_classification创建每个类都是正态分布的多类数据集；如果你要聚类数据集，请使用 make_blobs(blobs 斑点的意思)；如果创建多标签分类器使用make_multilabel_classification 通过3D 图可视化了数据集的分布 将数据进行归一化 函数 scale 为数组形状的数据集的标准化提供了一个快捷实现 将特征缩放至特定范围内, 可以分别使用 MinMaxScaler 范围 [0,1] 和 MaxAbsScaler 范围 [-1,1]实现 通过SVC 模型分类 SVC的优势 高纬度空间有效 数据维度n ,样本数量时m, n&gt;&gt;m 时有效 使用训练集的子集，高效利用内存 使用不同的核函数，我常用的是线性核 以及高斯核svm.SVC(kernel=&#39;linear&#39;, C=1).fit(X_train, y_train) 比较使用归一化和没有使用归一化后的效果 代码12345678910111213141516# 标准化数据模块from sklearn import preprocessing import numpy as np# 将资料分割成train与test的模块from sklearn.model_selection import train_test_split# 生成适合做classification资料的模块from sklearn.datasets.samples_generator import make_classification # Support Vector Machine中的Support Vector Classifierfrom sklearn.svm import SVC # 可视化数据的模块import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D 1234567train_X,train_Y = make_classification(n_samples=300, n_features=3, n_redundant=0, n_informative=2, random_state=22, n_clusters_per_class=1, scale=100) 123456fig = plt.figure()# 创建一个三维的绘图工程ax = fig.add_subplot(111, projection='3d')ax.scatter(train_X[:,0],train_X[:,1],train_X[:,2],c=train_Y) 1&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x5299630&gt; 12# 分类的数据，如果只展示2维，所以可以把标签当成颜色分类出来plt.scatter(train_X[:,0],train_X[:,1],c=train_Y) 1&lt;matplotlib.collections.PathCollection at 0x13865278&gt; 12# minmax_scale 特征基于最大最小值进行缩放，范围[0,1] ,修改范围使用 minmax_scale(train_X,feature_range=(min, max))normalise_X = preprocessing.minmax_scale(train_X) 1234# 均值，对列求均值print(\"均值：\" ,normalise_X.mean(axis = 0))# 方差，对列求方差print(\"方差：\" ,normalise_X.std(axis = 0)) 12均值： [0.43318427 0.53885262 0.40536198]方差： [0.16968632 0.14949669 0.22150377] 123456# scale 最常见的归一化sclaed_X = preprocessing.scale(train_X)# 均值，对列求均值print(\"均值：\" ,sclaed_X.mean(axis = 0))# 方差，对列求方差print(\"方差：\" ,sclaed_X.std(axis = 0)) 12均值： [-4.14483263e-17 -2.82366723e-16 2.44249065e-17]方差： [1. 1. 1.] 1234567# 使用没有归一化后的数据X_train, X_test, Y_train, Y_test = train_test_split( train_X, train_Y, test_size=0.3)clf = SVC()clf.fit(X_train,Y_train)print(\"没有使用归一化后的得分：\",clf.score(X_test,Y_test)) 1没有使用归一化后的得分： 0.4666666666666667 1234567# 使用归一化后的数据X_train, X_test, Y_train, Y_test = train_test_split( normalise_X, train_Y, test_size=0.3)# 采用SVM 的分类器clf = SVC()clf.fit(X_train,Y_train)print(\"使用minmax_scale归一化后的得分：\",clf.score(X_test,Y_test)) 1使用minmax_scale归一化后的得分： 0.9333333333333333 1234567# 使用归一化后的数据X_train, X_test, Y_train, Y_test = train_test_split( sclaed_X, train_Y, test_size=0.3)# 采用SVM 的分类器clf = SVC()clf.fit(X_train,Y_train)print(\"使用scale归一化后的得分：\",clf.score(X_test,Y_test)) 1使用scale归一化后的得分： 0.9444444444444444","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"回归","slug":"机器学习/sklearn/回归","permalink":"http://yoursite.com/categories/机器学习/sklearn/回归/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"归一化","slug":"归一化","permalink":"http://yoursite.com/tags/归一化/"},{"name":"scale","slug":"scale","permalink":"http://yoursite.com/tags/scale/"},{"name":"minmax_scale","slug":"minmax-scale","permalink":"http://yoursite.com/tags/minmax-scale/"},{"name":"SVC","slug":"SVC","permalink":"http://yoursite.com/tags/SVC/"},{"name":"分类","slug":"分类","permalink":"http://yoursite.com/tags/分类/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"回归","slug":"机器学习/sklearn/回归","permalink":"http://yoursite.com/categories/机器学习/sklearn/回归/"}]},{"title":"sklearn的LinearRegression线性回归04","slug":"test_04_模型参数","date":"2018-03-06T12:19:32.000Z","updated":"2018-08-29T08:14:07.627Z","comments":true,"path":"2018/03/06/test_04_模型参数/","link":"","permalink":"http://yoursite.com/2018/03/06/test_04_模型参数/","excerpt":"","text":"参考英文API 我没有找到中文的API，如果哪位找到了，请告诉我 要点 linearmodel中的 model 中有很多常用的属性，我以LinearRegression为例子，这个model中的属性有`coef（斜率）以及intercept_（截距） 还有get_params()` （模型中所有的参数） ; 代码12from sklearn import datasetsfrom sklearn.linear_model import LinearRegression 12# 读入波士顿房价的数据boston_data = datasets.load_boston() 1234# 读入数据特征data_X = boston_data.data# 读入标签data_Y = boston_data.target 1234# 定义模型model = LinearRegression()# 训练模型model.fit(data_X,data_Y) 1LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False) 123# 线性回归模型常用参数 coef_ (斜率) intercept_(截距)# 模型的斜率model.coef_ 1234array([-1.07170557e-01, 4.63952195e-02, 2.08602395e-02, 2.68856140e+00, -1.77957587e+01, 3.80475246e+00, 7.51061703e-04, -1.47575880e+00, 3.05655038e-01, -1.23293463e-02, -9.53463555e-01, 9.39251272e-03, -5.25466633e-01]) 12# 模型的截距model.intercept_ 136.49110328036133 12# 输出模型中所有参数model.get_params() 1&#123;&apos;copy_X&apos;: True, &apos;fit_intercept&apos;: True, &apos;n_jobs&apos;: 1, &apos;normalize&apos;: False&#125; 12# 模型预测model.predict(data_X[0:2,]) 1array([30.00821269, 25.0298606 ]) 1model.score(data_X,data_Y) 10.7406077428649427","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"回归","slug":"机器学习/sklearn/回归","permalink":"http://yoursite.com/categories/机器学习/sklearn/回归/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"LinearRegression","slug":"LinearRegression","permalink":"http://yoursite.com/tags/LinearRegression/"},{"name":"linear_model","slug":"linear-model","permalink":"http://yoursite.com/tags/linear-model/"},{"name":"线性回归","slug":"线性回归","permalink":"http://yoursite.com/tags/线性回归/"},{"name":"参数","slug":"参数","permalink":"http://yoursite.com/tags/参数/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"回归","slug":"机器学习/sklearn/回归","permalink":"http://yoursite.com/categories/机器学习/sklearn/回归/"}]},{"title":"sklearn的可视化自己的数据集03","slug":"test_03_可视化_自己创建数据集","date":"2018-03-01T12:19:32.000Z","updated":"2018-08-29T08:14:33.374Z","comments":true,"path":"2018/03/01/test_03_可视化_自己创建数据集/","link":"","permalink":"http://yoursite.com/2018/03/01/test_03_可视化_自己创建数据集/","excerpt":"","text":"引入包 sklearn的数据集包，这次自己创建数据集 sklearn线性回归包 matplotlib画图包 3D 画图包 要点我们自己通过make_regression 构造数据集 构造数据集，样本个数100个，每个特征3维，标签维度1，噪音1度 sklearn 数据集 代码1234from sklearn import datasetsfrom sklearn.linear_model import LinearRegressionimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3D 1X,Y = datasets.make_regression(n_samples=100,n_features=2,n_targets=1,noise=1) 123456789fig = plt.figure()# 创建一个三维的绘图工程ax = fig.add_subplot(111, projection='3d')# 用散点图scatter 把3维数据放进去ax.scatter(X[:,0],X[:,1],X[:,2],c=Y)ax.set_xlabel('X Label')ax.set_ylabel('Y Label')ax.set_zlabel('Z Label') 12 Text(0.0937963,0.0125663,&#39;Z Label&#39;) 1plt.show() 12345678910111213# 如果数据超过3维，如何可视化呢# 通过PCA 降维 或者 LDA 就可以了# LDA 的图形模型是一个三层贝叶斯模型# 以鸢尾花数据集为例，特征是4维的# 引入PCA 包 以及 LDA(隐 Dirichlet 分配)from sklearn.decomposition import PCAfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysisiris = datasets.load_iris()X = iris.datay = iris.targettarget_names = iris.target_names 1target_names array([&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;], dtype=&#39;&lt;U10&#39;) 123# PCA将数据降低为2维pca = PCA(n_components=2)X_r = pca.fit(X).transform(X) 123# LDA 将数据降低为2维lda = LinearDiscriminantAnalysis(n_components=2)X_r2 = lda.fit(X, y).transform(X) 12print('explained variance ratio (first two components): %s' % str(pca.explained_variance_ratio_)) explained variance ratio (first two components): [0.92461621 0.05301557] 123456789plt.figure()colors = ['y', 'r', 'b']lw = 2for color, i, target_name in zip(colors, [0, 1, 2], target_names): plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, lw=lw, label=target_name)plt.legend(loc='best')plt.title('PCA of IRIS dataset') Text(0.5,1,&#39;PCA of IRIS dataset&#39;) 123456789plt.figure()for color, i, target_name in zip(colors, [0, 1, 2], target_names): plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=.8, color=color, label=target_name)plt.legend(loc='best')plt.title('LDA of IRIS dataset')plt.show()","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"数据集","slug":"数据集","permalink":"http://yoursite.com/tags/数据集/"},{"name":"可视化","slug":"可视化","permalink":"http://yoursite.com/tags/可视化/"},{"name":"3D","slug":"3D","permalink":"http://yoursite.com/tags/3D/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}]},{"title":"sklearn的波士顿房价数据集01","slug":"test_02_database_波士顿房价","date":"2018-02-01T12:19:32.000Z","updated":"2018-08-29T08:58:29.771Z","comments":true,"path":"2018/02/01/test_02_database_波士顿房价/","link":"","permalink":"http://yoursite.com/2018/02/01/test_02_database_波士顿房价/","excerpt":"","text":"引入包这里我们用到两个包 一个是sklearn 的自带的数据集合 一个是sklearn 的线性回归 12from sklearn import datasetsfrom sklearn.linear_model import LinearRegression 格式非常的固定 只要是自带的数据集，都是load_XXX() 数据特征都是.data;标签都是.target 定义模型，一般一句话超级简单 训练模型，都是XX.fit(特征，标签) 预测结果xx.predict(特征) 评价模型.score(特征，标签) 大小[0,1]越接近1越好 评价指标有很多种 sklearn API 12# 读入波士顿房价的数据boston_data = datasets.load_boston() 1234# 读入数据特征data_X = boston_data.data# 读入标签data_Y = boston_data.target 12# 定义模型model = LinearRegression() 12# 训练模型model.fit(data_X,data_Y) 输出： LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False) 1model.predict(data_X[0:4,]) 输出： array([30.00821269, 25.0298606 , 30.5702317 , 28.60814055]) 1data_Y[:4,] 输出： array([24. , 21.6, 34.7, 33.4]) 1model.score(data_X[0:4,],data_Y[:4])","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"波士顿房价","slug":"波士顿房价","permalink":"http://yoursite.com/tags/波士顿房价/"},{"name":"数据集","slug":"数据集","permalink":"http://yoursite.com/tags/数据集/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}]},{"title":"sklearn的鸢尾花瓣数据集02","slug":"test_01_花瓣_通用模型","date":"2018-02-01T12:19:32.000Z","updated":"2018-08-29T08:43:45.783Z","comments":true,"path":"2018/02/01/test_01_花瓣_通用模型/","link":"","permalink":"http://yoursite.com/2018/02/01/test_01_花瓣_通用模型/","excerpt":"","text":"鸢尾花瓣数据集引入包这里我们用到两个包，一个是sklearn 的自带的数据集合一个是sklearn 的数据集分割工具一个是k近邻分类器KNeighborsClassifierdas 格式非常的固定 只要是自带的数据集，都是load_XXX() 数据特征都是.data;标签都是.target 定义模型，一般一句话超级简单 训练模型，都是XX.fit(特征，标签) 预测结果xx.predict(特征) 评价模型.score(特征，标签) 大小[0,1]越接近1越好 评价指标有很多种 sklearn API 12345import numpy as np# 数据库可以用于TensorFlowfrom sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.neighbors import KNeighborsClassifier C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20. &quot;This module will be removed in 0.20.&quot;, DeprecationWarning) 123iris = datasets.load_iris()iris_X = iris.datairis_Y = iris.target 12# 读入花瓣数据，花瓣数据的特征是4维的，# 标签是3类的 0， 1 ， 2 1iris_X[:2,:] array([[5.1, 3.5, 1.4, 0.2], [4.9, 3. , 1.4, 0.2]]) 1iris_Y array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) 12# 将数据分成训练集 和测试集，采用train_test_split,数据会被打乱,比例就是后面的0.3X_train,X_test, Y_train,Y_test = train_test_split(iris_X,iris_Y,test_size=0.3) 1Y_train array([1, 1, 2, 1, 2, 0, 1, 0, 1, 0, 1, 2, 1, 1, 1, 0, 0, 2, 2, 0, 2, 0, 1, 1, 1, 1, 1, 0, 2, 2, 2, 1, 0, 1, 2, 2, 0, 1, 0, 2, 0, 1, 1, 1, 0, 0, 2, 2, 0, 1, 2, 1, 0, 0, 1, 2, 0, 0, 0, 0, 2, 1, 2, 0, 2, 0, 0, 2, 0, 1, 0, 1, 1, 1, 2, 2, 0, 1, 0, 2, 2, 2, 2, 0, 2, 2, 2, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 1, 0, 2, 2, 0, 2, 1, 2]) 123# 定义分类器knn = KNeighborsClassifier()knn.fit(X_train,Y_train) KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=1, n_neighbors=5, p=2, weights=&#39;uniform&#39;) 1knn.predict(X_test) array([0, 2, 1, 2, 0, 0, 2, 1, 2, 0, 2, 0, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 0, 2, 0, 1, 1, 1, 0, 2, 0, 1, 2, 1, 1, 0, 2]) 1Y_test array([0, 2, 2, 2, 0, 0, 2, 1, 2, 0, 2, 0, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 0, 2, 0, 1, 1, 1, 0, 2, 0, 1, 2, 1, 1, 0, 2]) 1knn.score(X_test,Y_test) 0.9777777777777777 一句话说明K近邻就是找到K个和目标最近的训练集中的点(最常用的是欧式距离)，用少数服从多数来预测目标。k 不是越小越好，也不是越大越好；k越小,过拟合，k越大欠拟合，所以后面要引入交叉验证来调整K实例与每一个训练点的距离（这里的复杂度为0(n)比较大，所以要使用kd树等结构kd树基本思想是，若 A 点距离 B 点非常远，B 点距离 C 点非常近， 可知 A 点与C点很遥远，不需要明确计算它们的距离。参考资料一文搞懂k近邻（k-NN）算法sklearn API","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"分类","slug":"分类","permalink":"http://yoursite.com/tags/分类/"},{"name":"鸢尾","slug":"鸢尾","permalink":"http://yoursite.com/tags/鸢尾/"},{"name":"花瓣识别花","slug":"花瓣识别花","permalink":"http://yoursite.com/tags/花瓣识别花/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}]},{"title":"notminist数据预处理示范LogisticRegression训练","slug":"notminist数据预处理示范LogisticRegression训练","date":"2018-01-06T12:19:32.000Z","updated":"2018-08-29T08:55:46.308Z","comments":true,"path":"2018/01/06/notminist数据预处理示范LogisticRegression训练/","link":"","permalink":"http://yoursite.com/2018/01/06/notminist数据预处理示范LogisticRegression训练/","excerpt":"","text":"[TOC] 要点我们这里采用notMNIST数据集，重点是对数据进行预处理，涉及内容包括： 图像展示 图像转化为训练数据 数据拆分成训练数据以及验证数据 检查数据的平衡性，shuffle后的数据是否可信 大量数据时如何快速检查重叠程度——矩阵操作 采用传统的机器学习训练一个简单的分类器 绘制学习曲线 notMNIST 入门123456789101112131415# 作业目的:用简单线性分类训练图像分类器# 准备工作:下载数据集# These are all the modules we'll be using later. Make sure you can import them# before proceeding further.from __future__ import print_functionimport imageioimport matplotlib.pyplot as pltimport numpy as npimport osimport sysimport tarfilefrom IPython.display import display, Imagefrom sklearn.linear_model import LogisticRegressionfrom six.moves.urllib.request import urlretrievefrom six.moves import cPickle as pickle 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 数据集采用的是notminist 数据集，这个更加像真实的数据集，不如mnist 数据集干净，更加有难度# 数据集有训练集 500K ，测试集 19000，这个规模在PC机上跑很快# 标签为A 到 J (10个类别)# 每个图像特征 28*28 像素# 原数据地址 http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html# 代码中的链接貌似下不了了\"\"\"\"\"\"\"\"\"\"\"\"\"将数据下载到本地\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"url = 'https://commondatastorage.googleapis.com/books1000/'last_percent_reported = Nonedata_root = '.' # Change me to store data elsewhere# 这个是用来汇报下载过程的，下载了多少def download_progress_hook(count, blockSize, totalSize): \"\"\"A hook to report the progress of a download. This is mostly intended for users with slow internet connections. Reports every 5% change in download progress. \"\"\" global last_percent_reported percent = int(count * blockSize * 100 / totalSize) if last_percent_reported != percent: if percent % 5 == 0: sys.stdout.write(\"%s%%\" % percent) sys.stdout.flush() else: sys.stdout.write(\".\") sys.stdout.flush() last_percent_reported = percent#这个是根据url 来下载文件def maybe_download(filename, expected_bytes, force=False): \"\"\"Download a file if not present, and make sure it's the right size.\"\"\" dest_filename = os.path.join(data_root, filename) if force or not os.path.exists(dest_filename): print('Attempting to download:', filename) filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook) print('\\nDownload Complete!') statinfo = os.stat(dest_filename) if statinfo.st_size == expected_bytes: print('Found and verified', dest_filename) else: raise Exception( 'Failed to verify ' + dest_filename + '. Can you get to it with a browser?') return dest_filename# 如果需要下载启用下面代码# train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)# test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"下载结束\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" 1&apos;下载结束&apos; 12train_filename = 'notMNIST_large.tar.gz'test_filename = \"notMNIST_small.tar.gz\" 12345678910111213141516171819202122232425262728293031323334\"\"\"\"\"\"\"\"\"\"\"\"\" 接下来解压这个文件，里面有目录，标记着A 到J\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"# 使用上面train_filename test_filename 来解压# 解压非常的耗时间# 解压后输出目录 train_folders test_foldersnum_classes = 10np.random.seed(133)# 解压tar.gz 文件def maybe_extract(filename, force=False): root = os.path.splitext(os.path.splitext(filename)[0])[0] # remove .tar.gz if os.path.isdir(root) and not force: # You may override by setting force=True. print('%s already present - Skipping extraction of %s.' % (root, filename)) else: print('Extracting data for %s. This may take a while. Please wait.' % root) tar = tarfile.open(filename) sys.stdout.flush() tar.extractall(data_root) tar.close() data_folders = [ os.path.join(root, d) for d in sorted(os.listdir(root)) if os.path.isdir(os.path.join(root, d))] if len(data_folders) != num_classes: raise Exception( 'Expected %d folders, one per class. Found %d instead.' % ( num_classes, len(data_folders))) print(data_folders) return data_folders# train_folders = maybe_extract(train_filename)# test_folders = maybe_extract(test_filename)\"\"\" 这个解压部分就结束了 \"\"\" 1&apos; 这个解压部分就结束了 &apos; 12345# 解压结束后可以得到这个目录列表from typing import Listtrain_folders: List[str] = ['notMNIST_large/A', 'notMNIST_large/B', 'notMNIST_large/C', 'notMNIST_large/D', 'notMNIST_large/E', 'notMNIST_large/F', 'notMNIST_large/G', 'notMNIST_large/H', 'notMNIST_large/I', 'notMNIST_large/J']test_folders = ['notMNIST_small/A', 'notMNIST_small/B', 'notMNIST_small/C', 'notMNIST_small/D', 'notMNIST_small/E', 'notMNIST_small/F', 'notMNIST_small/G', 'notMNIST_small/H', 'notMNIST_small/I', 'notMNIST_small/J'] 问题 1: 展示一些图像12345678910111213141516171819202122232425\"\"\"第一个作业 显示图像\"\"\"# 可以使用 IPython.display.# TODO:第一个作业 通过 IPython.display 展示图像# display(Image(\"notMNIST_large/A/a2F6b28udHRm.png\"))# 从每个类中随机选取几张图片展示def showimages(num_per_class:int,train_folders): plot_images = [] # 展示的图片 plt.figure() # 定义画图 index = 1 # 定义小图的索引 for _ in range(num_per_class): # 拿到list 中的元素 for folder in train_folders: # 列出所有子文件夹和子文件 image_files = os.listdir(folder) # 分成num_per_class 行，len(train_folders)列 plt.subplot(num_per_class,len(train_folders),index) img = plt.imread(os.path.join(folder, image_files[np.random.randint(len(image_files))])) plt.imshow(img,cmap='gray') index = index +1 plt.axis('off') plt.show() showimages(5,train_folders) 1234\"\"\"第二种用pillow\"\"\"from PIL import Imageim = Image.open(\"notMNIST_large/A/a2F6b28udHRm.png\")im 123\"\"\"第三种用MATLAB 的plt\"\"\"img = plt.imread(\"notMNIST_large/A/a2F6b28udHRm.png\")plt.imshow(img) 1&lt;matplotlib.image.AxesImage at 0x11e5e4a8&gt; 图像数据转换成 3D 数组12345\"\"\"\" 这里开始预处理数据集 \"\"\"\"\"# 1. 分割数据：由于数据没有办法全部放入内存，将数据集按照每个类分别放到磁盘中# 2. 组合训练数据：将每个类的数据再组合成计算机内存可放的集合# 3. 把图片处理成3维矩阵：我们把整个数值转化成 float 3维数组 (image index, x, y) 具体代码如下：dataset[num_images, :, :] = image_data# 4. 归一化：把数据处理成 均值为0 ，方差是 0.5 更好 1&apos;&quot; 这里开始预处理数据集 &apos; 12image_size = 28 # Pixel width and height.pixel_depth = 255.0 # Number of levels per pixel. 1234567891011121314151617181920212223242526272829303132333435363738394041# 1. 把每个标签的图像通过imageio.imread(image_file).astype(float)变成二维矩阵载入内存中# 2. 进行归一化处理(x-128)/255 【0,255】 -&gt;[-128,128] -&gt;[-0.5,0.5]# 3. 然后打包成[num_images, :, :]# folder = A....C..Fdef load_letter(folder, min_num_images): \"\"\"Load the data for a single letter label.\"\"\" image_files = os.listdir(folder) dataset = np.ndarray(shape=(len(image_files), image_size, image_size), dtype=np.float32) print(folder) num_images = 0 for image in image_files: image_file = os.path.join(folder, image) try: # 这部分就是进行归一化处理了 (x-128)/255 那么结果就在 【0,255】 -&gt;[-128,128] -&gt;[-0.5,0.5] image_data = (imageio.imread(image_file).astype(float) - pixel_depth / 2) / pixel_depth # 判断分辨率，大小不一致报错 if image_data.shape != (image_size, image_size): # 触发异常后，后面的代码就不会再执行 raise Exception('Unexpected image shape: %s' % str(image_data.shape)) # 这个就是最关键的代码，把图像数据加入到dataset中 dataset[num_images, :, :] = image_data num_images = num_images + 1 except (IOError, ValueError) as e: print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.') dataset = dataset[0:num_images, :, :] # 检查下是不是所有的图像都已经放好了 if num_images &lt; min_num_images: raise Exception('Many fewer images than expected: %d &lt; %d' % (num_images, min_num_images)) print('Full dataset tensor:', dataset.shape) # 计算均值 print('Mean:', np.mean(dataset)) # 计算标准差 print('Standard deviation:', np.std(dataset)) return dataset 1234567891011121314151617181920212223242526272829303132# 持久化每个标签的数据集# 重命名成\" folder.pickle\"# 使用方法为 pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)def maybe_pickle(data_folders, min_num_images_per_class, force=False): dataset_names = [] for folder in data_folders: # 把每个文件夹命名成A.pickle set_filename = folder + '.pickle' # 把数据集名称加入dataset_names dataset_names.append(set_filename) # 避免多次执行的重复操作 if os.path.exists(set_filename) and not force: # You may override by setting force=True. print('%s already present - Skipping pickling.' % set_filename) else: print('Pickling %s.' % set_filename) dataset = load_letter(folder, min_num_images_per_class) try: with open(set_filename, 'wb') as f: # 保存数据集pickle.dump # 将 obj 持久化保存到文件 tmp.txt 中 # pickle.dump(obj, open(\"tmp.txt\", \"w\")) # obj: 要持久化保存的对象； # file: 一个拥有 write() 方法的对象，并且这个 write() 方法能接收一个字符串作为参数。这个对象可以是一个以写模式打开的文件对象或者一个 # StringIO 对象，或者其他自定义的满足条件的对象。 # protocol: 这是一个可选的参数，默认为 3 ，如果设置为 4 ,则要求python 版本是3以上 # 有压缩功能 pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL) except Exception as e: print('Unable to save data to', set_filename, ':', e) return dataset_names 123# 调用方法处理数据处理,放心不会重复执行train_datasets = maybe_pickle(train_folders, 45000)test_datasets = maybe_pickle(test_folders, 1800) 1234567891011121314151617181920notMNIST_large/A.pickle already present - Skipping pickling.notMNIST_large/B.pickle already present - Skipping pickling.notMNIST_large/C.pickle already present - Skipping pickling.notMNIST_large/D.pickle already present - Skipping pickling.notMNIST_large/E.pickle already present - Skipping pickling.notMNIST_large/F.pickle already present - Skipping pickling.notMNIST_large/G.pickle already present - Skipping pickling.notMNIST_large/H.pickle already present - Skipping pickling.notMNIST_large/I.pickle already present - Skipping pickling.notMNIST_large/J.pickle already present - Skipping pickling.notMNIST_small/A.pickle already present - Skipping pickling.notMNIST_small/B.pickle already present - Skipping pickling.notMNIST_small/C.pickle already present - Skipping pickling.notMNIST_small/D.pickle already present - Skipping pickling.notMNIST_small/E.pickle already present - Skipping pickling.notMNIST_small/F.pickle already present - Skipping pickling.notMNIST_small/G.pickle already present - Skipping pickling.notMNIST_small/H.pickle already present - Skipping pickling.notMNIST_small/I.pickle already present - Skipping pickling.notMNIST_small/J.pickle already present - Skipping pickling. 问题 2: 验证归一化的图像1234567# TODO:第二个作业 通过展示归一化后的图像以及标签 提示：你可以使用matplotlib.pyplot来展示图像# 如何把pickle 载入内存中pickle_file_path = \"notMNIST_large/B.pickle\"f = open(pickle_file_path, 'rb')letter_set = pickle.load(f)index = np.random.randint(0,45000)plt.imshow(letter_set[index,:,:]) 1&lt;matplotlib.image.AxesImage at 0xf86ae10&gt; 12for pickle_file in train_datasets: print(pickle_file) 12345678910notMNIST_large/A.picklenotMNIST_large/B.picklenotMNIST_large/C.picklenotMNIST_large/D.picklenotMNIST_large/E.picklenotMNIST_large/F.picklenotMNIST_large/G.picklenotMNIST_large/H.picklenotMNIST_large/I.picklenotMNIST_large/J.pickle 12345678910111213141516171819202122232425# 多展示一些归一化后的pickle图像,# \"\"\"：num_per_class:int 每个类展示几个图像train_datasets 就是pickle 的名称的list\"\"\"def showimages_pickle(num_per_class:int,train_datasets): plot_images = [] # 展示的图片 plt.figure() # 定义画图 index = 1 # 定义小图的索引 for _ in range(num_per_class): # 拿到list 中的元素 for pickle_file in train_datasets: # 载入pickle 的文件 f = open(pickle_file, 'rb') image_data_per_class = pickle.load(f) # 分成num_per_class 行，len(train_folders)列 plt.subplot(num_per_class,len(train_datasets),index) plt.imshow( image_data_per_class[np.random.randint(np.shape(image_data_per_class)[0])],cmap='gray') index = index +1 plt.axis('off') plt.show() showimages_pickle(5,train_datasets) 123# 第二种 通过IPython.display展示index = np.random.randint(0,45000)# 但是IPython.display，我不知道如何载入图像，不知道谁懂 问题 3: 验证数据平衡123456789101112131415161718192021# TODO:第三个作业 检查各个类之间的数据是否平衡# 我认为平衡就是每个类的数据量是否一致# 可能理解错误了？# 把所有的数据集依次读出来，然后建立用柱状图表示出来labels = []num_of_labels = []# enumerate 枚举 可返回两个参数，一个是索引，一个是文件for label, pickle_file in enumerate(train_datasets): try: with open(pickle_file, 'rb') as f: letter_set = pickle.load(f) num = np.shape(letter_set)[0] num_of_labels.append(num) labels.append(label) except Exception as e: print('Unable to process data from', pickle_file, ':', e) raiseplt.bar(range(len(num_of_labels)), num_of_labels,color='rgb',tick_label=labels) 1&lt;BarContainer object of 10 artists&gt; 拆分数据集成批12345678910# 由于计算的的配置没有办法装下所有的配置，所以只能部分拿出来，组合成自己想要的数据集大小，并且要根据需要调整数据集的大小# 初始化变量 dataset (3维数组)，labels(1维数组)用于存放数据，def make_arrays(nb_rows, img_size): if nb_rows: # ndarray 创建3维数组 dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32) labels = np.ndarray(nb_rows, dtype=np.int32) else: dataset, labels = None, None return dataset, labels 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 第一个参数用于 A.pickle B.pickle# 第二个参数用于 设置训练集的大小# 比如总共选择 train_size = 200000 条数据，那么就是每个类 选择 （200000/ 10类）条数据，组合集合def merge_datasets(pickle_files, train_size, valid_size=0): num_classes = len(pickle_files) # 如果valid_size = 0，那么就是生成valid_dataset =None valid_dataset, valid_labels = make_arrays(valid_size, image_size) train_dataset, train_labels = make_arrays(train_size, image_size) # 测试集的大小 ，注意除法是 // # 验证集的大小 vsize_per_class = valid_size // num_classes tsize_per_class = train_size // num_classes start_v, start_t = 0, 0 # end_v, end_t = vsize_per_class, tsize_per_class end_l = vsize_per_class + tsize_per_class # enumerate 枚举 可返回两个参数，一个是索引，一个是文件 for label, pickle_file in enumerate(pickle_files): try: with open(pickle_file, 'rb') as f: letter_set = pickle.load(f) # let's shuffle the letters to have random validation and training set # 随机洗牌 np.random.shuffle(letter_set) # 把数据分成两部分，第一部分给 验证集valid 第二部分 测试集 train，每个都选一部分，组成最终数据集 if valid_dataset is not None: valid_letter = letter_set[:vsize_per_class, :, :] valid_dataset[start_v:end_v, :, :] = valid_letter valid_labels[start_v:end_v] = label start_v += vsize_per_class end_v += vsize_per_class train_letter = letter_set[vsize_per_class:end_l, :, :] train_dataset[start_t:end_t, :, :] = train_letter train_labels[start_t:end_t] = label start_t += tsize_per_class end_t += tsize_per_class except Exception as e: print('Unable to process data from', pickle_file, ':', e) raise return valid_dataset, valid_labels, train_dataset, train_labels 123456789101112131415161718192021222324252627from typing import Optionalfrom numpy.core.multiarray import ndarraytrain_size = 20000valid_size = 1000test_size = 1000valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets( train_datasets, train_size, valid_size)_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)print('Training:', train_dataset.shape, train_labels.shape)print('Validation:', valid_dataset.shape, valid_labels.shape)print('Testing:', test_dataset.shape, test_labels.shape)# 下面就是对做好的数据集进行洗牌def randomize(dataset, labels): # 获取label的排列 permutation = np.random.permutation(labels.shape[0]) # 用这个排列数来排序dataset 以及 label shuffled_dataset = dataset[permutation,:,:] shuffled_labels = labels[permutation] return shuffled_dataset, shuffled_labelstrain_dataset, train_labels = randomize(train_dataset, train_labels)test_dataset, test_labels = randomize(test_dataset, test_labels)valid_dataset, valid_labels = randomize(valid_dataset, valid_labels) 123Training: (20000, 28, 28) (20000,)Validation: (1000, 28, 28) (1000,)Testing: (1000, 28, 28) (1000,) 问题 4: 样本乱序与验证12345678910111213141516# Problem 4# TODO:第四个作业 说服自己洗牌后的数据很好# 接下来验证下数据平衡性，然后柱状图展示# 思路获取标签中的每一类的个数，然后画出柱状图# a = np.where(valid_labels==3) 过滤def showbalance(labels): num_of_labels = [] for label in range(10): # 这里我有个疑问，为什么label_per_class.shape 不能用？？？！！！ label_per_class = np.where(labels==label) num_of_labels.append(np.shape(label_per_class)[1]) plt.bar(range(len(num_of_labels)), num_of_labels,color='rgbcy',tick_label=range(len(num_of_labels))) return num_of_labelsshowbalance(valid_labels) 1[100, 100, 100, 100, 100, 100, 100, 100, 100, 100] 12345678910111213141516# TODO:第四个作业 数据是否已经随机打乱，打乱后的标签是否正确# 思路随机选择num_start个起始点，然后连续选择num_pic张图片和标签然后展示def show_pic_and_label(dataset,labels,num_start,num_pic): plt.figure() index = 1; for i in range(num_start): start = np.random.randint(len(labels)-num_pic) for j in range(num_pic): plt.subplot(num_start,num_pic,index) plt.imshow(dataset[start+j,:,:],cmap='gray') plt.title(labels[start+j]) index = index +1 plt.axis('off') plt.show() show_pic_and_label(train_dataset, train_labels,5,20) 1234567891011121314151617181920212223242526# 把数据存储起来准备使用# 合成路径 os.path.join# 1.open(pickle_file, 'wb')# 2.把数据集组合成键值对# 3.pickle.dump(save, f, pickle.HIGHEST_PROTOCOL) 存放数据pickle_file = os.path.join(data_root, 'notMNIST.pickle')try: f = open(pickle_file, 'wb') save = &#123; 'train_dataset': train_dataset, 'train_labels': train_labels, 'valid_dataset': valid_dataset, 'valid_labels': valid_labels, 'test_dataset': test_dataset, 'test_labels': test_labels, &#125; pickle.dump(save, f, pickle.HIGHEST_PROTOCOL) f.close()except Exception as e: print('Unable to save data to', pickle_file, ':', e) raise# 操作系统获取文件声明，可以拿到文件大小statinfo = os.stat(pickle_file)print('Compressed pickle size:', statinfo.st_size) 1Compressed pickle size: 69080502 123import timelocaltime = time.asctime( time.localtime(time.time()) )print( time.asctime( time.localtime(time.time()) )) 1Tue Aug 28 15:21:49 2018 问题 5: 寻找重叠样本1234567891011121314151617181920212223242526272829303132333435# TODO:作业五 测量训练数据 与 交叉验证数据 间的重合程度# 重合度高容易导致过拟合问题# 计算向量间的欧式距离，判断图像是否相似，# dist = numpy.sqrt(numpy.sum(numpy.square(vec1 - vec2)))# dist = numpy.linalg.norm(vec1 - vec2)# 余弦相似性 cosine_sim = np.inner(X, Y) / np.inner(np.abs(X), np.abs(Y))def overlap(train_dataset,valid_dataset,threshold=0,num_duplicate_to_show = 5): print(\"开始---&gt;\",) num_overlap = 0 # 覆盖个数 print_pic_num = 0; # 打印图片个数 index = 1; plt.figure() for i in range(train_dataset.shape[0]): for j in range(valid_dataset.shape[0]): dist = np.linalg.norm(train_dataset[i,:,:]-valid_dataset[j,:,:]) if dist &lt;= threshold: num_overlap = num_overlap + 1 if(print_pic_num&lt;=num_duplicate_to_show): # 画train_dataset第一个图 plt.subplot(num_duplicate_to_show,2,index) plt.title(\"train_dataset\") plt.imshow(train_dataset[i,:,:],cmap='gray') index = index +1 # 画valid_dataset的一样图 plt.subplot(num_duplicate_to_show,2,index) plt.title(\"valid_dataset\") plt.imshow(valid_dataset[j,:,:],cmap='gray') print_pic_num = print_pic_num + 1 index = index +1 print(\"重合数据个数是：\",num_overlap) print(\"train_dataset重合率是：\",num_overlap/train_dataset.shape[0]) print(\"valid_dataset重合率是：\",num_overlap/valid_dataset.shape[0]) return num_overlapoverlap(train_dataset,valid_dataset,threshold=0,num_duplicate_to_show = 5) 1开始---&gt; 12C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py:107: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance. In a future version, a new instance will always be created and returned. Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance. warnings.warn(message, mplDeprecation, stacklevel=1) 123重合数据个数是： 213540train_dataset重合率是： 1.0677valid_dataset重合率是： 21.354 1213540 123456789101112131415161718192021222324252627282930313233343536373839404142def overlap_cos_matrix(source_dataset,target_dataset,threshold=1,num_duplicate_to_show = 5,selfcheck = False): print(\"开始时间：\",time.asctime( time.localtime(time.time()) )) X = np.reshape(source_dataset,(source_dataset.shape[0],-1)) Y = np.reshape(target_dataset,(target_dataset.shape[0],-1)) assert (X.shape[1] == Y.shape[1]) # 计算余弦相似度，注意这里是矩阵相乘 cosine_sim = np.dot(X, Y.T) / np.inner(np.abs(X), np.abs(Y)) assert (cosine_sim.shape == (X.shape[0],Y.shape[0])) print(cosine_sim.shape) # 判断cosine_sim中的元素是否有等于threshold的，如果有就是相等 num_source = 0 num_target = 0 print_pic_num = 1; plt.figure() print(\"矩阵计算结束：\",time.asctime( time.localtime(time.time()) )) for i in range(X.shape[0]): # 第i 行中所有重复的图片索引 dup_indices = np.where(cosine_sim[i,:] &gt;= threshold) for j in dup_indices[0]: if i==j and selfcheck: continue else: num_source = num_source +1 if(print_pic_num&lt;=num_duplicate_to_show): # 画train_dataset第一个图 plt.subplot(num_duplicate_to_show,2,print_pic_num) plt.imshow(source_dataset[i,:,:],cmap='gray') print_pic_num = print_pic_num +1 # 画valid_dataset的一样图 plt.subplot(num_duplicate_to_show,2,print_pic_num) plt.imshow(target_dataset[j,:,:],cmap='gray') print_pic_num = print_pic_num + 1 break print(\"结束时间：\",time.asctime( time.localtime(time.time()) )) print(\"重合数据个数是：\",num_source) print(\"train_dataset重合率是：\",num_source/source_dataset.shape[0]) # print(\"valid_dataset重合率是：\",num_overlap/valid_dataset.shape[0]) plt.axis('off') plt.show() return num_source overlap_cos_matrix(valid_dataset,train_dataset,threshold=1,num_duplicate_to_show = 10,selfcheck = False) 123456开始时间： Tue Aug 28 15:55:19 2018(1000, 20000)矩阵计算结束： Tue Aug 28 15:55:20 2018结束时间： Tue Aug 28 15:55:20 2018重合数据个数是： 47train_dataset重合率是： 0.047 147 问题 6: 训练一个简单的机器学习模型1234# TODO:作业六 验证集自己内部的重复图片有多少？res = overlap_cos_matrix(valid_dataset,valid_dataset,threshold=1,num_duplicate_to_show = 10,selfcheck = True)# 每个图像都和自己重叠res = res - valid_dataset.shape[0] 123456开始时间： Tue Aug 28 15:55:23 2018(1000, 1000)矩阵计算结束： Tue Aug 28 15:55:23 2018结束时间： Tue Aug 28 15:55:23 2018重合数据个数是： 16train_dataset重合率是： 0.016 12345678910111213141516# 扩展一下如何判断两个图是否相似呢，可以通过下面的代码——直方图相似度def difference(hist1,hist2): sum1 = 0 for i in range(len(hist1)): if (hist1[i] == hist2[i]): sum1 += 1 else: sum1 += 1 - float(abs(hist1[i] - hist2[i]))/ max(hist1[i], hist2[i]) return sum1/len(hist1)def similary_calculate(img1, img2): img1_reshape = np.reshape(img1, (28 * 28, 1)) img1_reshape = np.reshape(img2, (28 * 28, 1)) hist1 = list(img1_reshape.getdata()) hist2 = list(img1_reshape.getdata()) return difference(hist1, hist2) 1234567891011# TODO:作业七 创建一个消过毒的测试以及验证集 比较你们在后续作业的准确性# 思路一: 在目前基础上做 # 1. 首先trainset的pickle检查重复的，如果重复的删除# 2. validateset的pickle检查重复的，如果重复的删除# 3. test的pickle 检查重复的，重复的删除# 4. train 和 valid 中重复的，把valid 删除# 5. train 和test 中重复的，把test 删除# 思路二：从头做# 1. 把图像数据按类查找重复的，每个图像和其他所有图像匹配，重复就删除# 2. 重新执行 转换3D数据，归一化，拆分成组合成新批次 1234567891011121314# TODO:作业八 using 50, 100, 1000 and 5000 training samples，用sklearn 训练一个模型# 提示使用sklearn 的 linear_model.LogisticRegression()# 思路一：直接用学习曲线，不用上面分好的验证集# from sklearn.model_selection import learning_curve #学习曲线模块# from sklearn.linear_model import LogisticRegression# from sklearn.model_selection import ShuffleSplit # 专业的数据集分割包# # X = train_dataset.reshape(train_dataset.shape[0],-1)# y = train_labels# # train_sizes, train_scores, test_scores= learning_curve(# model, X, y,cv=5 train_sizes=[50,100,1000,5000],n_jobs=5)# # train_scores 123456789101112131415161718192021222324252627282930# 思路二：自己做预测，自己做学习曲线，用上面分好的验证集from sklearn.model_selection import learning_curve #学习曲线模块from sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import ShuffleSplit # 专业的数据集分割包# train_dataset, train_labels = randomize(train_dataset, train_labels)# test_dataset, test_labels = randomize(test_dataset, test_labels)# valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)train_size=[50,100,1000,5000,8000,15000,20000]train_scores = []valid_scores = []test_scores = []for size in train_size: # 每次循环都要新建模型，solver适合数据较多的情况，比较快 model = LogisticRegression(solver= 'saga',multi_class='multinomial') # 由于train_dataset中数据已经打乱了，所以按顺序拿 X = train_dataset[0:size,:].reshape(size,-1) y = train_labels[0:size] X_valid = valid_dataset.reshape(valid_dataset.shape[0],-1) y_valid = valid_labels X_test = test_dataset.reshape(test_dataset.shape[0],-1) y_test = test_labels model.fit(X,y) train_scores.append(model.score(X,y)) valid_scores.append(model.score(X_valid,y_valid)) test_scores.append(model.score(X_test,y_test)) 12C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge &quot;the coef_ did not converge&quot;, ConvergenceWarning) 123456789101112131415161718# axis=1 对行进行操作，由于上面的cv=分成了20份，所以 train_scores 是一个20个元素的数组# train_scores_mean = np.mean(train_scores, axis=1)# valid_scores_mean = np.mean(valid_scores, axis=1)# test_scores_mean = np.mean(test_scores, axis=1)# train_sizes 横坐标，train_loss_mean 纵坐标，形状‘o-’,红色plt.plot(train_size, train_scores, 'o-', color=\"r\", label=\"Training\")plt.plot(train_size, valid_scores, 'o-', color=\"b\", label=\"Cross-validation\")plt.plot(train_size, test_scores, 'o-', color=\"g\", label=\"test\")plt.xlabel(\"Training examples\")plt.ylabel(\"Loss\")# 将图标签——图例放到最佳的位置plt.legend(loc=\"best\")title = \"Learning Curves\"plt.title(title) 1Text(0.5,1,&apos;Learning Curves&apos;)","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"机器学习","slug":"深度学习/机器学习","permalink":"http://yoursite.com/categories/深度学习/机器学习/"},{"name":"notminist","slug":"深度学习/机器学习/notminist","permalink":"http://yoursite.com/categories/深度学习/机器学习/notminist/"},{"name":"分类","slug":"深度学习/机器学习/notminist/分类","permalink":"http://yoursite.com/categories/深度学习/机器学习/notminist/分类/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/tags/深度学习/"},{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"学习曲线","slug":"学习曲线","permalink":"http://yoursite.com/tags/学习曲线/"},{"name":"notminist","slug":"notminist","permalink":"http://yoursite.com/tags/notminist/"},{"name":"数据清洗，数据预处理","slug":"数据清洗，数据预处理","permalink":"http://yoursite.com/tags/数据清洗，数据预处理/"},{"name":"LogisticRegression","slug":"LogisticRegression","permalink":"http://yoursite.com/tags/LogisticRegression/"}],"keywords":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"},{"name":"机器学习","slug":"深度学习/机器学习","permalink":"http://yoursite.com/categories/深度学习/机器学习/"},{"name":"notminist","slug":"深度学习/机器学习/notminist","permalink":"http://yoursite.com/categories/深度学习/机器学习/notminist/"},{"name":"分类","slug":"深度学习/机器学习/notminist/分类","permalink":"http://yoursite.com/categories/深度学习/机器学习/notminist/分类/"}]},{"title":"搭建免费博客HEXO+GitHub","slug":"搭建免费博客HEXO+GitHub","date":"2017-08-21T12:19:32.000Z","updated":"2018-08-29T08:49:46.693Z","comments":true,"path":"2017/08/21/搭建免费博客HEXO+GitHub/","link":"","permalink":"http://yoursite.com/2017/08/21/搭建免费博客HEXO+GitHub/","excerpt":"","text":"原料 Node.js ——简单的说就是运行在服务端的 JavaScript, 所以这个构建后端服务的 . Nexo —— 一款基于Node.js的静态博客框架，这个是台湾人创建的 GitHub Pages —— GitHub全球最大的Gay站，我们用的是GitHub中的仓库，因为它是免费的.. 步骤创建Github仓库 创建Github仓库安装Git 安装Git 可以直接安装GitHub Desktop创建SSH秘钥 创建SSH秘钥 配置git的用户名和邮箱 右键打开gitBash,12git config --global user.name &quot;你的GitHub用户名&quot;git config --global user.email &quot;你的GitHub注册邮箱&quot; 看本地有秘钥没 1cd ~/. ssh 本地创建秘钥 1ssh-keygen -t rsa -C &quot;your_email@example.com&quot; 中间会提示你是否需要设置密码，可输可不输 上传到GitHub复制公钥到系统粘贴板中 1clip &lt; ~/.ssh/id_rsa.pub +测试 1ssh -T git@github.com 如果提示你 yes /no? 那就是yes 安装Node.js 安装Node.js 下载地址:官网 安装Hexo 安装Hexo 安装nexo 1npm install hexo-cli -g 安装部署工具 1npm install hexo-deployer-git --save 初始化 1hexo init 启动 12345hexo generatehexo server可以一句话hexo g -d 常用命令现在来介绍常用的Hexo 命令 1234567891011121314151617npm install hexo -g #安装Hexonpm update hexo -g #升级 hexo init #初始化博客命令简写hexo n &quot;我的博客&quot; == hexo new &quot;我的博客&quot; #新建文章hexo g == hexo generate #生成hexo s == hexo server #启动服务预览hexo d == hexo deploy #部署hexo server #Hexo会监视文件变动并自动更新，无须重启服务器hexo server -s #静态模式hexo server -p 5000 #更改端口hexo server -i 192.168.1.1 #自定义 IPhexo clean #清除缓存，若是网页正常情况下可以忽略这条命令刚刚的三个命令依次是新建一篇博客文章、生成网页、在本地预览的操作。 浏览器访问地址http://localhost:4000 上传到Github 上传到Github 配置根目录下 _config.yml 1234deploy:type: gitrepository: git@github.com:username/username.github.io.gitbranch: master 上传github 123hexo clean hexo g hexo d 最后一条命令是部署到github访问 http://xxxx.github.io 绑定域名 绑定域名 更换主题 更换主题 Themes 官网 如果你不喜欢Hexo默认的主题，可以更换不同的主题，主题传送门：Themes 我自己使用的是Next主题，可以在blog目录中的themes文件夹中查看你自己主题是什么。现在把默认主题更改成Next主题，在blog目录中（就是命令行的位置处于blog目录）打开命令行输入：1git clone https://github.com/iissnan/hexo-theme-next themes/next 发布文章 发布文章 命令行 1hexo n &quot;博客名字&quot; 直接做好markdown 文件，放在nexo的source_posts目录下 1source\\_posts OSS服务器 OSS服务器 为啥要用对象存储服务（Object Storage Service，简称OSS）？ 1.费用很低，甚至免费 2.图片加载快 我用的是阿里云的OSS如果你不会写markdown 如果你不会写markdownAPI 参考","categories":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/categories/其他/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"},{"name":"搭建博客","slug":"搭建博客","permalink":"http://yoursite.com/tags/搭建博客/"},{"name":"免费","slug":"免费","permalink":"http://yoursite.com/tags/免费/"}],"keywords":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/categories/其他/"}]}]}