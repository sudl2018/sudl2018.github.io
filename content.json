{"meta":{"title":"Hexo","subtitle":null,"description":null,"author":"John Doe","url":"http://yoursite.com"},"pages":[],"posts":[{"title":"���п��ܵ�����������","slug":"329.找出所有可能的完整二叉树","date":"2018-08-26T06:06:32.000Z","updated":"2018-08-27T00:53:00.287Z","comments":true,"path":"2018/08/26/329.找出所有可能的完整二叉树/","link":"","permalink":"http://yoursite.com/2018/08/26/329.找出所有可能的完整二叉树/","excerpt":"","text":"�����ҳ����п��ܵ�����������All Possible Full Binary Trees ������������һ�������������ÿ�����ǡ���� 0 �� 2 ���ӽ�㡣 ���ذ��� N ���������п����������������б� �𰸵�ÿ��Ԫ�ض���һ���������ĸ���㡣 ����ÿ������ÿ������������� node.val=0�� ����԰��κ�˳�򷵻����������б� ע������������������ȫ������ ˼·һ��������������������п��ܽڵ㣬����������� �ҳ����е������Ǳ����кܶ���ڵ㣬�����Ǹ����ڵ��list �Ҳ�֪������ж��ٿ��������Բ���һ�����г����и��ڵ㣬���������ȿ��ǣ� �Ӹ��ڵ㹹���������к������ dfs�����Ӹ��ڵ㿪ʼ��ÿ�εݹ� ���ǰ������ӽڵ�������е�ĳ���ڵ������п��ܽڵ㶼Ҫ���ԣ�for ѭ����������ʹ��for ѭ��Ҫ�����ظ����ʵ����� ��ÿһ�ֿ��ܽ����������ӵ������ ����һ12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; List&lt;TreeNode&gt; res = new LinkedList&lt;&gt;(); int N = 0; public List&lt;TreeNode&gt; allPossibleFBT(int N) &#123; if(N%2==0)&#123; return res; &#125; this.N = N; TreeNode root = new TreeNode(0); LinkedList&lt;TreeNode&gt; nodes = new LinkedList&lt;&gt;(); nodes.add(root); helper(1,nodes,root); return res; &#125; public void helper(int count,LinkedList&lt;TreeNode&gt; nodes,TreeNode root)&#123; if(count == N)&#123; if(root != null)&#123; // Ҫ�ÿ�������Ϊԭ��root �ᱻ�Ҳ�ͣ�ĸı� TreeNode new_root = new TreeNode(0); copyTree(root,new_root); res.add(new_root); &#125; return; &#125; if(count &gt; N)&#123; return; &#125; //����һ��Ҫ��poll ,��Ϊ�����е�Ԫ���Ѿ���ʹ�ù��Ľڵ㣬����ٴα����ʾͻ�����ظ����� //����ڵ�һ�������ʹ����Ǿ�Ҫ�Ƴ� while(nodes.size()!=0)&#123; TreeNode node = nodes.poll(); node.left = new TreeNode(0); node.right = new TreeNode(0); // ��ʣ��û�з��ʵĽڵ㶼����list�У��Ա����´η��� LinkedList&lt;TreeNode&gt; nodelist = new LinkedList&lt;&gt;(nodes); nodelist.add(node.left); nodelist.add(node.right); helper(count+2,nodelist,root); node.left = null; node.right = null; &#125; &#125; //���һ���� public void copyTree(TreeNode node,TreeNode new_node)&#123; if(node == null)&#123; return; &#125; if(node.left != null)&#123; new_node.left = new TreeNode(0); copyTree(node.left,new_node.left); &#125; if(node.right != null)&#123; new_node.right = new TreeNode(0); copyTree(node.right,new_node.right); &#125; &#125; &#125; ˼·������̬�滮�� ��������������������Ҳ�������������� �����������п��ܺ������������п��ܵ���Ͼ��Ǵ� �ӽڵ�����1��ʼ���죬һֱ���쵽N ����һ1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public List&lt;TreeNode&gt; allPossibleFBT(int N) &#123; // ����洢���䣬������е����������� Map&lt;Integer,List&lt;TreeNode&gt;&gt; mem = new HashMap&lt;&gt;(); // ��ʼ������0��ʼ��N�����Բ���Ҫ��0��ʼ��Ϊʲô����Ҫ��0��ʼ��������� for(int i=0;i&lt;=N;i++)&#123; mem.put(i,new LinkedList&lt;TreeNode&gt;()); &#125; // ��ʼ��N=0û�нڵ㣬N=1һ�����ڵ� mem.get(1).add(new TreeNode(0)); // ��N=2��ʼ���� for(int i=2;i&lt;=N;i++)&#123; // ���������ܽڵ����,�����������������������������ڵ�����������0���ڵ㣬����mem������N==0Ҳ����,�������ڵ������Χ[0,i-1] for(int l=0;l&lt;i;l++)&#123; List&lt;TreeNode&gt; lefts = mem.get(l); List&lt;TreeNode&gt; rights = mem.get(i-l-1);//�ܹ�i���ڵ㣬���l����һ�����ڵ�1����ô�ұ�i-l-1 // ������������Ͼ�������,for ѭ��ȷ�������������������������ǿգ������� N == 0��for�Ͳ�ִ���� for(TreeNode left:lefts)&#123; for(TreeNode right:rights)&#123; TreeNode root = new TreeNode(0); // ������������������������������������ root.left = left; root.right = right; mem.get(i).add(root); &#125; &#125; &#125; &#125; return mem.get(N); &#125;&#125;","categories":[{"name":"�㷨","slug":"�㷨","permalink":"http://yoursite.com/categories/�㷨/"},{"name":"������","slug":"�㷨/������","permalink":"http://yoursite.com/categories/�㷨/������/"}],"tags":[{"name":"�㷨","slug":"�㷨","permalink":"http://yoursite.com/tags/�㷨/"},{"name":"������","slug":"������","permalink":"http://yoursite.com/tags/������/"},{"name":"����������","slug":"����������","permalink":"http://yoursite.com/tags/����������/"},{"name":"medium","slug":"medium","permalink":"http://yoursite.com/tags/medium/"},{"name":"894","slug":"894","permalink":"http://yoursite.com/tags/894/"}],"keywords":[{"name":"�㷨","slug":"�㷨","permalink":"http://yoursite.com/categories/�㷨/"},{"name":"������","slug":"�㷨/������","permalink":"http://yoursite.com/categories/�㷨/������/"}]},{"title":"sklearn的学习曲线_过拟合欠拟合大杀器07","slug":"test-07-sklearn_学习曲线_过拟合欠拟合大杀器07","date":"2018-03-10T12:19:32.000Z","updated":"2018-08-24T10:56:56.235Z","comments":true,"path":"2018/03/10/test-07-sklearn_学习曲线_过拟合欠拟合大杀器07/","link":"","permalink":"http://yoursite.com/2018/03/10/test-07-sklearn_学习曲线_过拟合欠拟合大杀器07/","excerpt":"","text":"要点 正常的训练过程会出现过拟合以及欠拟合等情况，所以要识别这些情况，以下图为例： 左图欠拟合，中间正好，右图过拟合 一句话说明学习曲线learning_curve——通过增加训练数据，来观察模型的获益效果 一般学习曲线长什么样? 朴素贝叶斯大致收敛到一个较低的分数 支持向量机（SVM）是样本越多越好 我们使用手写数字数据集 分别用朴素贝叶斯和SVM 来训练模型 参考文档 验证曲线: 绘制分数以评估模型 代码123456from sklearn.model_selection import learning_curve #学习曲线模块from sklearn.datasets import load_digits #digits数据集from sklearn.svm import SVC #Support Vector Classifierimport matplotlib.pyplot as plt #可视化模块from sklearn.model_selection import ShuffleSplit # 专业的数据集分割包import numpy as np 1234# 手写数字，0-9，共1797样本，每个样本由64个特征组合（8*8），每个特征值用0-16表示digits = load_digits()X = digits.datay = digits.target 1234567# 迭代次数100次，交叉验证集分成100个，测试集比例0.2cv = ShuffleSplit(n_splits=100, test_size=0.2)#平均每一轮所得到的平均方差(共5轮，分别为样本10%、25%、50%、75%、100%),采用均方差度量损失，train_sizes, train_scores, test_scores= learning_curve( # SVM 中的参数，一个C 大间距，一个gammam, gamma 不是高斯半径 是 1/(2*sigma^2),都是越大越容易过拟合 SVC(gamma=0.0001), X, y, cv=cv, train_sizes=np.linspace(.1, 1.0, 5),n_jobs=5) 123# axis=1 对行进行操作，由于上面的cv=分成了20份，所以 train_scores 是一个20个元素的数组train_scores_mean = np.mean(train_scores, axis=1)test_scores_mean = np.mean(test_scores, axis=1) 123456789101112# train_sizes 横坐标，train_loss_mean 纵坐标，形状‘o-’,红色plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training\")plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation\")plt.xlabel(\"Training examples\")plt.ylabel(\"Loss\")# 将图标签——图例放到最佳的位置plt.legend(loc=\"best\")title = \"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"plt.title(title) 1Text(0.5,1,&apos;Learning Curves (SVM, RBF kernel, $\\\\gamma=0.001$)&apos;) 123456789# 接下来用朴素贝叶斯分类器from sklearn.naive_bayes import GaussianNB# 迭代次数100次，交叉验证集分成100个，测试集比例0.2cv = ShuffleSplit(n_splits=100, test_size=0.2)#平均每一轮所得到的平均方差(共5轮，分别为样本10%、25%、50%、75%、100%),采用均方差度量损失，train_sizes, train_scores, test_scores= learning_curve( # SVM 中的参数，一个C 大间距，一个gammam, gamma 不是高斯半径 是 1/(2*sigma^2),都是越大越容易过拟合 GaussianNB(), X, y, cv=cv, train_sizes=np.linspace(.1, 1.0, 5),n_jobs=2) 123# axis=1 对行进行操作，由于上面的cv=分成了20份，所以 train_scores 是一个20个元素的数组train_scores_mean = np.mean(train_scores, axis=1)test_scores_mean = np.mean(test_scores, axis=1) 123456789101112# train_sizes 横坐标，train_loss_mean 纵坐标，形状‘o-’,红色plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training\")plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation\")plt.xlabel(\"Training examples\")plt.ylabel(\"Loss\")# 将图标签——图例放到最佳的位置plt.legend(loc=\"best\")title = \"Learning Curves (Naive Bayes)\"plt.title(title) 1Text(0.5,1,&apos;Learning Curves (Naive Bayes)&apos;)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"分类","slug":"机器学习/sklearn/分类","permalink":"http://yoursite.com/categories/机器学习/sklearn/分类/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"学习曲线","slug":"学习曲线","permalink":"http://yoursite.com/tags/学习曲线/"},{"name":"过拟合","slug":"过拟合","permalink":"http://yoursite.com/tags/过拟合/"},{"name":"欠拟合","slug":"欠拟合","permalink":"http://yoursite.com/tags/欠拟合/"},{"name":"手写数字","slug":"手写数字","permalink":"http://yoursite.com/tags/手写数字/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"分类","slug":"机器学习/sklearn/分类","permalink":"http://yoursite.com/categories/机器学习/sklearn/分类/"}]},{"title":"sklearn的cross_validation交叉验证06","slug":"test-06-sklearn_cross_validation_的交叉验证06","date":"2018-03-09T12:19:32.000Z","updated":"2018-08-24T09:44:16.777Z","comments":true,"path":"2018/03/09/test-06-sklearn_cross_validation_的交叉验证06/","link":"","permalink":"http://yoursite.com/2018/03/09/test-06-sklearn_cross_validation_的交叉验证06/","excerpt":"","text":"要点 正常的训练过程会出现过拟合以及欠拟合等情况，所以要识别这些情况，首先你要懂得是交叉验证 本文我们用到的是利用 scikit-learn 包中的 train_test_split 可以快速划分数据集 我们使用鸢尾花数据集，首先采用0.25的划分来看看训练结果 然后我们 交叉验证（CV 缩写）)，即是设置不同（”hyperparameters(超参数)”）K近邻中的K来使模型达到最佳状态 我最常用的方法调用 cross_val_score，通过不同的划分集合来判断最好的参数取值 这里用的评估指标是accuracy以及neg_mean_squared_error更多指标 如果你有多个模型要链接成一个，把特征选择、归一化和分类合并成一个过程，那么要用 Pipeline（管道）和 FeatureUnion（特征联合）: 合并的评估器. 参考文档 3.1. 交叉验证：评估估算器的表现 代码123from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.neighbors import KNeighborsClassifier 123iris = load_iris()X = iris.dataY = iris.target 123# random_state 用来设置seed 的，seed 是确保相同划分的一种设置，比如可以这么写random_state = 3 # test_size 是测试集的划分大小，默认是0.25X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.25) 1234# K 近邻算法，常见分类算法，这里不是邻居越多越好，越少过拟合，越多欠拟合knn = KNeighborsClassifier(n_neighbors=6)knn.fit(X_train,Y_train)print(\"&gt; 没有使用交叉验证的得分： \",knn.score(X_test,Y_test)) 1&gt; 没有使用交叉验证的得分： 0.9210526315789473 12# K折交叉验证模块from sklearn.model_selection import cross_val_score 1234knn = KNeighborsClassifier(n_neighbors=6)# 把数据集分成5组，每组都是训练集合测试集scores = cross_val_score(knn,X,Y,cv = 5,scoring=\"accuracy\")print(\"&gt; 使用交叉验证的准确度： \",scores) 1&gt; 使用交叉验证的准确度： [0.96666667 1. 0.96666667 0.96666667 1. ] 12import numpy as npprint(\"&gt; 测试集的划分大小是： \",np.shape(X_test)[0]/(np.shape(X_train)[0]+np.shape(X_test)[0])) 1&gt; 测试集的划分大小是： 0.25333333333333335 12# 求下平均值print(\"&gt; 使用交叉验证的准确度平均值： \",scores.mean()) 1&gt; 使用交叉验证的准确度平均值： 0.9800000000000001 1234567891011# 现在切入正题，如何调整超参数 ，以n_neighbors为例k_scores = []k_losses = []for i in range(1,31): knn = KNeighborsClassifier(n_neighbors=i) # 把数据集分成10组，每组都是训练集合测试集 scores = cross_val_score(knn,X,Y,cv = 10,scoring=\"accuracy\") # 如果是回归问题使用neg_mean_squared_error,就是均方差，但是是负的，所以要加- loss = -cross_val_score(knn,X,Y,cv = 10,scoring=\"neg_mean_squared_error\") k_scores.append(scores.mean()) k_losses.append(loss.mean()) 12345# 画图画出来import matplotlib.pyplot as pltplt.plot(range(1,31),k_scores)plt.xlabel(\"Value of K for KNN\")plt.ylabel(\"cross-validated Accuracy\") 1Text(0,0.5,&apos;cross-validated Accuracy&apos;) 123plt.plot(range(1,31),k_losses)plt.xlabel(\"Value of K for KNN\")plt.ylabel(\"cross-validated loss\") 1Text(0,0.5,&apos;cross-validated loss&apos;) 12345678910111213# 如果你有归一化的过程，那么怎么使用交叉验证呢，你需要的pipelinefrom sklearn.pipeline import make_pipelinefrom sklearn import preprocessingk_scores = []k_losses = []for i in range(1,31): knn = make_pipeline(preprocessing.StandardScaler(), KNeighborsClassifier(n_neighbors=i)) # 把数据集分成10组，每组都是训练集合测试集 scores = cross_val_score(knn,X,Y,cv = 10,scoring=\"accuracy\") # 如果是回归问题使用neg_mean_squared_error,就是均方差，但是是负的，所以要加- loss = -cross_val_score(knn,X,Y,cv = 10,scoring=\"neg_mean_squared_error\") k_scores.append(scores.mean()) k_losses.append(loss.mean()) 12345# 画图画出来import matplotlib.pyplot as pltplt.plot(range(1,31),k_scores)plt.xlabel(\"Value of K for KNN\")plt.ylabel(\"cross-validated Accuracy\") 1Text(0,0.5,&apos;cross-validated Accuracy&apos;)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"分类","slug":"机器学习/sklearn/分类","permalink":"http://yoursite.com/categories/机器学习/sklearn/分类/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"分类","slug":"分类","permalink":"http://yoursite.com/tags/分类/"},{"name":"交叉验证","slug":"交叉验证","permalink":"http://yoursite.com/tags/交叉验证/"},{"name":"cross_validation","slug":"cross-validation","permalink":"http://yoursite.com/tags/cross-validation/"},{"name":"鸢尾花","slug":"鸢尾花","permalink":"http://yoursite.com/tags/鸢尾花/"},{"name":"K近邻","slug":"K近邻","permalink":"http://yoursite.com/tags/K近邻/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"分类","slug":"机器学习/sklearn/分类","permalink":"http://yoursite.com/categories/机器学习/sklearn/分类/"}]},{"title":"sklearn的归一化05","slug":"test-05-sklearn的归一化05","date":"2018-03-08T12:19:32.000Z","updated":"2018-08-24T07:55:57.081Z","comments":true,"path":"2018/03/08/test-05-sklearn的归一化05/","link":"","permalink":"http://yoursite.com/2018/03/08/test-05-sklearn的归一化05/","excerpt":"","text":"要点 我们通过make_classification创建每个类都是正态分布的多类数据集；如果你要聚类数据集，请使用 make_blobs(blobs 斑点的意思)；如果创建多标签分类器使用make_multilabel_classification 通过3D 图可视化了数据集的分布 将数据进行归一化 函数 scale 为数组形状的数据集的标准化提供了一个快捷实现 将特征缩放至特定范围内, 可以分别使用 MinMaxScaler 范围 [0,1] 和 MaxAbsScaler 范围 [-1,1]实现 通过SVC 模型分类 SVC的优势 高纬度空间有效 数据维度n ,样本数量时m, n&gt;&gt;m 时有效 使用训练集的子集，高效利用内存 使用不同的核函数，我常用的是线性核 以及高斯核 比较使用归一化和没有使用归一化后的效果 代码12345678910111213141516# 标准化数据模块from sklearn import preprocessing import numpy as np# 将资料分割成train与test的模块from sklearn.model_selection import train_test_split# 生成适合做classification资料的模块from sklearn.datasets.samples_generator import make_classification # Support Vector Machine中的Support Vector Classifierfrom sklearn.svm import SVC # 可视化数据的模块import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D 1234567train_X,train_Y = make_classification(n_samples=300, n_features=3, n_redundant=0, n_informative=2, random_state=22, n_clusters_per_class=1, scale=100) 123456fig = plt.figure()# 创建一个三维的绘图工程ax = fig.add_subplot(111, projection='3d')ax.scatter(train_X[:,0],train_X[:,1],train_X[:,2],c=train_Y) 1&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x5299630&gt; 12# 分类的数据，如果只展示2维，所以可以把标签当成颜色分类出来plt.scatter(train_X[:,0],train_X[:,1],c=train_Y) 1&lt;matplotlib.collections.PathCollection at 0x13865278&gt; 12# minmax_scale 特征基于最大最小值进行缩放，范围[0,1] ,修改范围使用 minmax_scale(train_X,feature_range=(min, max))normalise_X = preprocessing.minmax_scale(train_X) 1234# 均值，对列求均值print(\"均值：\" ,normalise_X.mean(axis = 0))# 方差，对列求方差print(\"方差：\" ,normalise_X.std(axis = 0)) 12均值： [0.43318427 0.53885262 0.40536198]方差： [0.16968632 0.14949669 0.22150377] 123456# scale 最常见的归一化sclaed_X = preprocessing.scale(train_X)# 均值，对列求均值print(\"均值：\" ,sclaed_X.mean(axis = 0))# 方差，对列求方差print(\"方差：\" ,sclaed_X.std(axis = 0)) 12均值： [-4.14483263e-17 -2.82366723e-16 2.44249065e-17]方差： [1. 1. 1.] 1234567# 使用没有归一化后的数据X_train, X_test, Y_train, Y_test = train_test_split( train_X, train_Y, test_size=0.3)clf = SVC()clf.fit(X_train,Y_train)print(\"没有使用归一化后的得分：\",clf.score(X_test,Y_test)) 1没有使用归一化后的得分： 0.4666666666666667 1234567# 使用归一化后的数据X_train, X_test, Y_train, Y_test = train_test_split( normalise_X, train_Y, test_size=0.3)# 采用SVM 的分类器clf = SVC()clf.fit(X_train,Y_train)print(\"使用minmax_scale归一化后的得分：\",clf.score(X_test,Y_test)) 1使用minmax_scale归一化后的得分： 0.9333333333333333 1234567# 使用归一化后的数据X_train, X_test, Y_train, Y_test = train_test_split( sclaed_X, train_Y, test_size=0.3)# 采用SVM 的分类器clf = SVC()clf.fit(X_train,Y_train)print(\"使用scale归一化后的得分：\",clf.score(X_test,Y_test)) 1使用scale归一化后的得分： 0.9444444444444444","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"回归","slug":"机器学习/sklearn/回归","permalink":"http://yoursite.com/categories/机器学习/sklearn/回归/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"归一化","slug":"归一化","permalink":"http://yoursite.com/tags/归一化/"},{"name":"scale","slug":"scale","permalink":"http://yoursite.com/tags/scale/"},{"name":"minmax_scale","slug":"minmax-scale","permalink":"http://yoursite.com/tags/minmax-scale/"},{"name":"SVC","slug":"SVC","permalink":"http://yoursite.com/tags/SVC/"},{"name":"分类","slug":"分类","permalink":"http://yoursite.com/tags/分类/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"回归","slug":"机器学习/sklearn/回归","permalink":"http://yoursite.com/categories/机器学习/sklearn/回归/"}]},{"title":"sklearn的LinearRegression线性回归04","slug":"test_04_模型参数","date":"2018-03-06T12:19:32.000Z","updated":"2018-08-23T10:01:49.124Z","comments":true,"path":"2018/03/06/test_04_模型参数/","link":"","permalink":"http://yoursite.com/2018/03/06/test_04_模型参数/","excerpt":"","text":"参考英文API 我没有找到中文的API，如果哪位找到了，请告诉我 要点 linear_model中的 model 中有很多常用的属性，我以LinearRegression为例子，这个model中的属性有coef_ （斜率）以及 intercept_（截距） 还有get_params() （模型中所有的参数） ; 代码12from sklearn import datasetsfrom sklearn.linear_model import LinearRegression 12# 读入波士顿房价的数据boston_data = datasets.load_boston() 1234# 读入数据特征data_X = boston_data.data# 读入标签data_Y = boston_data.target 1234# 定义模型model = LinearRegression()# 训练模型model.fit(data_X,data_Y) LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False) 123# 线性回归模型常用参数 coef_ (斜率) intercept_(截距)# 模型的斜率model.coef_ array([-1.07170557e-01, 4.63952195e-02, 2.08602395e-02, 2.68856140e+00, -1.77957587e+01, 3.80475246e+00, 7.51061703e-04, -1.47575880e+00, 3.05655038e-01, -1.23293463e-02, -9.53463555e-01, 9.39251272e-03, -5.25466633e-01]) 12# 模型的截距model.intercept_ 36.49110328036133 12# 输出模型中所有参数model.get_params() {&apos;copy_X&apos;: True, &apos;fit_intercept&apos;: True, &apos;n_jobs&apos;: 1, &apos;normalize&apos;: False} 12# 模型预测model.predict(data_X[0:2,]) array([30.00821269, 25.0298606 ]) 1model.score(data_X,data_Y) 0.7406077428649427","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"回归","slug":"机器学习/sklearn/回归","permalink":"http://yoursite.com/categories/机器学习/sklearn/回归/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"LinearRegression","slug":"LinearRegression","permalink":"http://yoursite.com/tags/LinearRegression/"},{"name":"linear_model","slug":"linear-model","permalink":"http://yoursite.com/tags/linear-model/"},{"name":"线性回归","slug":"线性回归","permalink":"http://yoursite.com/tags/线性回归/"},{"name":"参数","slug":"参数","permalink":"http://yoursite.com/tags/参数/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"},{"name":"回归","slug":"机器学习/sklearn/回归","permalink":"http://yoursite.com/categories/机器学习/sklearn/回归/"}]},{"title":"sklearn的可视化自己的数据集03","slug":"test_03_可视化_自己创建数据集","date":"2018-03-01T12:19:32.000Z","updated":"2018-08-22T08:09:53.701Z","comments":true,"path":"2018/03/01/test_03_可视化_自己创建数据集/","link":"","permalink":"http://yoursite.com/2018/03/01/test_03_可视化_自己创建数据集/","excerpt":"","text":"引入包 sklearn的数据集包，这次自己创建数据集 sklearn线性回归包 matplotlib画图包 3D 画图包 要点我们自己通过make_regression 构造数据集 构造数据集，样本个数100个，每个特征3维，标签维度1，噪音1度 sklearn 数据集 代码1234from sklearn import datasetsfrom sklearn.linear_model import LinearRegressionimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3D 1X,Y = datasets.make_regression(n_samples=100,n_features=2,n_targets=1,noise=1) 123456789fig = plt.figure()# 创建一个三维的绘图工程ax = fig.add_subplot(111, projection='3d')# 用散点图scatter 把3维数据放进去ax.scatter(X[:,0],X[:,1],X[:,2],c=Y)ax.set_xlabel('X Label')ax.set_ylabel('Y Label')ax.set_zlabel('Z Label') 12 Text(0.0937963,0.0125663,&apos;Z Label&apos;) 1plt.show() 12345678910111213# 如果数据超过3维，如何可视化呢# 通过PCA 降维 或者 LDA 就可以了# LDA 的图形模型是一个三层贝叶斯模型# 以鸢尾花数据集为例，特征是4维的# 引入PCA 包 以及 LDA(隐 Dirichlet 分配)from sklearn.decomposition import PCAfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysisiris = datasets.load_iris()X = iris.datay = iris.targettarget_names = iris.target_names 1target_names array([&apos;setosa&apos;, &apos;versicolor&apos;, &apos;virginica&apos;], dtype=&apos;&lt;U10&apos;) 123# PCA将数据降低为2维pca = PCA(n_components=2)X_r = pca.fit(X).transform(X) 123# LDA 将数据降低为2维lda = LinearDiscriminantAnalysis(n_components=2)X_r2 = lda.fit(X, y).transform(X) 12print('explained variance ratio (first two components): %s' % str(pca.explained_variance_ratio_)) explained variance ratio (first two components): [0.92461621 0.05301557] 123456789plt.figure()colors = ['y', 'r', 'b']lw = 2for color, i, target_name in zip(colors, [0, 1, 2], target_names): plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, lw=lw, label=target_name)plt.legend(loc='best')plt.title('PCA of IRIS dataset') Text(0.5,1,&apos;PCA of IRIS dataset&apos;) 123456789plt.figure()for color, i, target_name in zip(colors, [0, 1, 2], target_names): plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=.8, color=color, label=target_name)plt.legend(loc='best')plt.title('LDA of IRIS dataset')plt.show()","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"数据集","slug":"数据集","permalink":"http://yoursite.com/tags/数据集/"},{"name":"可视化","slug":"可视化","permalink":"http://yoursite.com/tags/可视化/"},{"name":"3D","slug":"3D","permalink":"http://yoursite.com/tags/3D/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}]},{"title":"sklearn的鸢尾花瓣数据集02","slug":"test_01_花瓣_通用模型","date":"2018-02-05T12:19:32.000Z","updated":"2018-08-22T08:10:09.310Z","comments":true,"path":"2018/02/05/test_01_花瓣_通用模型/","link":"","permalink":"http://yoursite.com/2018/02/05/test_01_花瓣_通用模型/","excerpt":"","text":"鸢尾花瓣数据集引入包这里我们用到两个包，一个是sklearn 的自带的数据集合一个是sklearn 的数据集分割工具一个是k近邻分类器KNeighborsClassifierdas 格式非常的固定 只要是自带的数据集，都是load_XXX() 数据特征都是.data;标签都是.target 定义模型，一般一句话超级简单 训练模型，都是XX.fit(特征，标签) 预测结果xx.predict(特征) 评价模型.score(特征，标签) 大小[0,1]越接近1越好 评价指标有很多种 sklearn API 12345import numpy as np# 数据库可以用于TensorFlowfrom sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.neighbors import KNeighborsClassifier C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20. &quot;This module will be removed in 0.20.&quot;, DeprecationWarning) 123iris = datasets.load_iris()iris_X = iris.datairis_Y = iris.target 12# 读入花瓣数据，花瓣数据的特征是4维的，# 标签是3类的 0， 1 ， 2 1iris_X[:2,:] array([[5.1, 3.5, 1.4, 0.2], [4.9, 3. , 1.4, 0.2]]) 1iris_Y array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) 12# 将数据分成训练集 和测试集，采用train_test_split,数据会被打乱,比例就是后面的0.3X_train,X_test, Y_train,Y_test = train_test_split(iris_X,iris_Y,test_size=0.3) 1Y_train array([1, 1, 2, 1, 2, 0, 1, 0, 1, 0, 1, 2, 1, 1, 1, 0, 0, 2, 2, 0, 2, 0, 1, 1, 1, 1, 1, 0, 2, 2, 2, 1, 0, 1, 2, 2, 0, 1, 0, 2, 0, 1, 1, 1, 0, 0, 2, 2, 0, 1, 2, 1, 0, 0, 1, 2, 0, 0, 0, 0, 2, 1, 2, 0, 2, 0, 0, 2, 0, 1, 0, 1, 1, 1, 2, 2, 0, 1, 0, 2, 2, 2, 2, 0, 2, 2, 2, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 1, 0, 2, 2, 0, 2, 1, 2]) 123# 定义分类器knn = KNeighborsClassifier()knn.fit(X_train,Y_train) KNeighborsClassifier(algorithm=&apos;auto&apos;, leaf_size=30, metric=&apos;minkowski&apos;, metric_params=None, n_jobs=1, n_neighbors=5, p=2, weights=&apos;uniform&apos;) 1knn.predict(X_test) array([0, 2, 1, 2, 0, 0, 2, 1, 2, 0, 2, 0, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 0, 2, 0, 1, 1, 1, 0, 2, 0, 1, 2, 1, 1, 0, 2]) 1Y_test array([0, 2, 2, 2, 0, 0, 2, 1, 2, 0, 2, 0, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 0, 2, 0, 1, 1, 1, 0, 2, 0, 1, 2, 1, 1, 0, 2]) 1knn.score(X_test,Y_test) 0.9777777777777777 一句话说明K近邻就是找到K个和目标最近的训练集中的点(最常用的是欧式距离)，用少数服从多数来预测目标。 k 不是越小越好，也不是越大越好；k越小 过拟合，k越大欠拟合，所以后面要引入交叉验证来调整K 实例与每一个训练点的距离（这里的复杂度为0(n)比较大，所以要使用kd树等结构 参考资料 一文搞懂k近邻（k-NN）算法","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"分类","slug":"分类","permalink":"http://yoursite.com/tags/分类/"},{"name":"鸢尾","slug":"鸢尾","permalink":"http://yoursite.com/tags/鸢尾/"},{"name":"花瓣识别花","slug":"花瓣识别花","permalink":"http://yoursite.com/tags/花瓣识别花/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}]},{"title":"sklearn的波士顿房价数据集01","slug":"test_02_database_波士顿房价","date":"2018-02-01T12:19:32.000Z","updated":"2018-08-22T08:09:44.908Z","comments":true,"path":"2018/02/01/test_02_database_波士顿房价/","link":"","permalink":"http://yoursite.com/2018/02/01/test_02_database_波士顿房价/","excerpt":"","text":"引入包这里我们用到两个包 一个是sklearn 的自带的数据集合 一个是sklearn 的线性回归 12from sklearn import datasetsfrom sklearn.linear_model import LinearRegression 格式非常的固定 只要是自带的数据集，都是load_XXX() 数据特征都是.data;标签都是.target 定义模型，一般一句话超级简单 训练模型，都是XX.fit(特征，标签) 预测结果xx.predict(特征) 评价模型.score(特征，标签) 大小[0,1]越接近1越好 评价指标有很多种 sklearn API 12# 读入波士顿房价的数据boston_data = datasets.load_boston() 1234# 读入数据特征data_X = boston_data.data# 读入标签data_Y = boston_data.target 12# 定义模型model = LinearRegression() 12# 训练模型model.fit(data_X,data_Y) 输出： LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False) 1model.predict(data_X[0:4,]) 输出： array([30.00821269, 25.0298606 , 30.5702317 , 28.60814055]) 1data_Y[:4,] 输出： array([24. , 21.6, 34.7, 33.4]) 1model.score(data_X[0:4,],data_Y[:4])","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"sklearn","slug":"sklearn","permalink":"http://yoursite.com/tags/sklearn/"},{"name":"数据集","slug":"数据集","permalink":"http://yoursite.com/tags/数据集/"},{"name":"波士顿房价","slug":"波士顿房价","permalink":"http://yoursite.com/tags/波士顿房价/"}],"keywords":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"},{"name":"sklearn","slug":"机器学习/sklearn","permalink":"http://yoursite.com/categories/机器学习/sklearn/"}]},{"title":"搭建免费博客HEXO+GitHub","slug":"搭建免费博客HEXO+GitHub(1)","date":"2017-08-21T12:19:32.000Z","updated":"2018-08-24T09:45:39.713Z","comments":true,"path":"2017/08/21/搭建免费博客HEXO+GitHub(1)/","link":"","permalink":"http://yoursite.com/2017/08/21/搭建免费博客HEXO+GitHub(1)/","excerpt":"","text":"原料 Node.js ——简单的说就是运行在服务端的 JavaScript, 所以这个构建后端服务的 Nexo —— 一款基于Node.js的静态博客框架，这个是台湾人创建的 GitHub Pages —— GitHub全球最大的Gay站，我们用的是GitHub中的仓库，因为它是免费的.. 步骤创建Github仓库 创建Github仓库安装Git 安装Git 可以直接安装GitHub Desktop创建SSH秘钥 创建SSH秘钥 配置git的用户名和邮箱 右键打开gitBash,12git config --global user.name &quot;你的GitHub用户名&quot;git config --global user.email &quot;你的GitHub注册邮箱&quot; 看本地有秘钥没 1cd ~/. ssh 本地创建秘钥 1ssh-keygen -t rsa -C &quot;your_email@example.com&quot; 中间会提示你是否需要设置密码，可输可不输 上传到GitHub复制公钥到系统粘贴板中 1clip &lt; ~/.ssh/id_rsa.pub +测试 1ssh -T git@github.com 如果提示你 yes /no? 那就是yes 安装Node.js 安装Node.js 下载地址:官网 安装Hexo 安装Hexo 安装nexo 1npm install hexo-cli -g 安装部署工具 1npm install hexo-deployer-git --save 初始化 1hexo init 启动 12345hexo generatehexo server可以一句话hexo g -d 常用命令现在来介绍常用的Hexo 命令 1234567891011121314151617npm install hexo -g #安装Hexonpm update hexo -g #升级 hexo init #初始化博客命令简写hexo n &quot;我的博客&quot; == hexo new &quot;我的博客&quot; #新建文章hexo g == hexo generate #生成hexo s == hexo server #启动服务预览hexo d == hexo deploy #部署hexo server #Hexo会监视文件变动并自动更新，无须重启服务器hexo server -s #静态模式hexo server -p 5000 #更改端口hexo server -i 192.168.1.1 #自定义 IPhexo clean #清除缓存，若是网页正常情况下可以忽略这条命令刚刚的三个命令依次是新建一篇博客文章、生成网页、在本地预览的操作。 浏览器访问地址http://localhost:4000 上传到Github 上传到Github 配置根目录下 _config.yml 1234deploy:type: gitrepository: git@github.com:username/username.github.io.gitbranch: master 上传github 123hexo clean hexo g hexo d 最后一条命令是部署到github访问 http://xxxx.github.io 绑定域名 绑定域名 更换主题 更换主题 Themes 官网 如果你不喜欢Hexo默认的主题，可以更换不同的主题，主题传送门：Themes 我自己使用的是Next主题，可以在blog目录中的themes文件夹中查看你自己主题是什么。现在把默认主题更改成Next主题，在blog目录中（就是命令行的位置处于blog目录）打开命令行输入：1git clone https://github.com/iissnan/hexo-theme-next themes/next 发布文章 发布文章 命令行 1hexo n &quot;博客名字&quot; 直接做好markdown 文件，放在nexo的source_posts目录下 1source\\_posts OSS服务器 OSS服务器 为啥要用对象存储服务（Object Storage Service，简称OSS）？ 1.费用很低，甚至免费 2.图片加载快 我用的是阿里云的OSS","categories":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/categories/其他/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"},{"name":"搭建博客","slug":"搭建博客","permalink":"http://yoursite.com/tags/搭建博客/"},{"name":"免费","slug":"免费","permalink":"http://yoursite.com/tags/免费/"}],"keywords":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/categories/其他/"}]}]}